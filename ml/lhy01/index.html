<!doctype html><html lang=zh-tw dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[ML] 01. 機器學習基本概念簡介 | Rain Hu's Workspace</title><meta name=keywords content="ML,Linear Model,Piecewise Linear Curve,sigmoid,ReLU,Batch,Epoch"><meta name=description content="什麼是機器學習，機器學習任務，監督式學習的運作流程"><meta name=author content="Rain Hu"><link rel=canonical href=https://intervalrain.github.io/><meta name=google-site-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0cefe5a1d95e3d0f0cce057d37c60cd238d1a4af825090f831a18f21671f621d.css integrity="sha256-DO/lodlePQ8MzgV9N8YM0jjRpK+CUJD4MaGPIWcfYh0=" rel="preload stylesheet" as=style><link rel=icon href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=16x16 href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=32x32 href=https://intervalrain.github.io/images/rain.png><link rel=apple-touch-icon href=https://intervalrain.github.io/images/rain.png><link rel=mask-icon href=https://intervalrain.github.io/images/rain.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh-tw href=https://intervalrain.github.io/ml/lhy01/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css integrity=sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js integrity=sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script src=https://utteranc.es/client.js repo=intervalrain.github.io issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script><meta property="og:url" content="https://intervalrain.github.io/ml/lhy01/"><meta property="og:site_name" content="Rain Hu's Workspace"><meta property="og:title" content="[ML] 01. 機器學習基本概念簡介"><meta property="og:description" content="什麼是機器學習，機器學習任務，監督式學習的運作流程"><meta property="og:locale" content="zh-tw"><meta property="og:type" content="article"><meta property="article:section" content="ml"><meta property="article:published_time" content="2023-08-02T23:56:25+08:00"><meta property="article:modified_time" content="2023-08-02T23:56:25+08:00"><meta property="article:tag" content="ML"><meta name=twitter:card content="summary"><meta name=twitter:title content="[ML] 01. 機器學習基本概念簡介"><meta name=twitter:description content="什麼是機器學習，機器學習任務，監督式學習的運作流程"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"MLs","item":"https://intervalrain.github.io/ml/"},{"@type":"ListItem","position":2,"name":"[ML] 01. 機器學習基本概念簡介","item":"https://intervalrain.github.io/ml/lhy01/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[ML] 01. 機器學習基本概念簡介","name":"[ML] 01. 機器學習基本概念簡介","description":"什麼是機器學習，機器學習任務，監督式學習的運作流程","keywords":["ML","Linear Model","Piecewise Linear Curve","sigmoid","ReLU","Batch","Epoch"],"articleBody":"前言 什麼是機器學習 機器學習(Machine Learning)，就是利用機器的力量幫忙找出函式。 Input 可以是 vector matrix sequence Output 可以是 Regression Classification Structed Learning(令機器產生有結構的東西 eg. text, image) 示意圖 什麼是深度學習 深度學習(Deep Learning)，就是利用神經網路(neural network)的方式來產生函數。 機器如何學習 1. 基本原理(訓練三步驟) Step 1: 使用合適的 Model \\(y=f(\\text{\\red{data}})\\) Function with unknown parameters Model: \\(\\boxed{y=b+wx_1}\\) \\(w: \\text{weight}\\) \\(b: \\text{bias}\\) \\(x: \\text{feature}\\) Step 2: 定義 Loss function Define loss from training data 以 Model 的參數 \\(w,b\\) 來計算 Loss 物理意義：Loss 愈大代表參數愈不好，Loss 愈小代表參數愈好。 計算方法：求估計的值與實際的值(label)之間的差距 Loss function: \\(\\boxed{L=\\frac{1}{N}\\sum_ne_n}\\) MAE (mean absolute error): \\(e=|y-\\hat{y}|\\) MSE (mean square error): \\(e=(y-\\hat{y})^2\\) Cross-entropy: 計算機率分布之間的差距 Error Surface: 根據不同的參數，計算出 loss 所畫出來的等高線圖。 Step 3: Optimization 找到 loss 最小的參數組合 \\((w,b)\\) 方法：Gradient Descent \\(\\boxed{w’ = w - \\red{\\eta}\\frac{\\partial L}{\\partial w}|_{w=w^0,b=b^0}}\\) \\(\\boxed{b’ = b - \\red{\\eta}\\frac{\\partial L}{\\partial b}|_{w=w^0,b=b^0}}\\) \\(\\red{\\eta}\\): 學習率 learning rate, 決定 gradient descent 的一步有多大步 2. Linear Model \\(\\boxed{f\\leftarrow y=b+\\sum_{j=1}^{n}{w_jx_j}}\\) 不只考慮前一天的觀看人數 \\(x_1\\)，也考慮前二~七天 \\(x_2, x_3, … , x_7\\)。 當參數變多時，命中率可望有效提升。 3. Piecewise Linear Curves(Sigmoid) \\(\\text{Sigmoid Function:} \\boxed{y=\\red{c}\\frac{1}{1+e^{-(\\green{b}+\\blue{w}x_1)}}}=\\boxed{\\red{c}\\text{ sigmoid}(\\green{b}+\\blue{w}x_1)}\\) 將 \\(w_ix_i\\) 替換成 \\(c_i\\text{ sigmoid}(b_i+w_ix_i)\\) 特徵為1時，\\(\\boxed{y=b+\\sum_i{c_i\\text{ sigmoid}(b_i+ w_ix_1)}}\\) 特徵\u003e1時，\\(\\boxed{y=b+\\sum_i{c_i\\text{ sigmoid}(b_i+\\sum_j w_{ij}x_j)}}\\) 意義：一條曲線可以由多個鋸齒狀的線段(hard sigmoid)的總合，我們可以用 sigmoid 函數來逼近 hard sigmoid。事實上，sigmoid 的個數就是神經網路中一層 neuron 的 node 數，至於使用幾個 sigmoid 是 hyper parameter。 可將公式轉成矩陣計算+激勵函數的形式： 以線性代數方式表示：\\(\\boxed{y=b+c^T\\sigma(b_i+Wx)}\\) 將 \\(b\\)、\\(b_i\\)、\\(W\\)、\\(c^T\\) 等所有參數統稱為 \\(\\theta\\) 故 Loss 可表示成 \\(L(\\theta)\\) 重覆 gradient descent 的方法，更新(update) 參數。 梯度 gradient，\\(g=\\) \\(\\begin{bmatrix}\\frac{\\partial L}{\\partial \\theta_1}|_{\\theta=\\theta^0}\\\\\\frac{\\partial L}{\\partial \\theta_2}| _{\\theta=\\theta^0}\\\\\\vdots\\end{bmatrix}=\\nabla L(\\theta^0)\\) 更新(update)計算：\\(\\begin{bmatrix}\\theta_1^1\\\\\\theta_2^1\\\\\\vdots\\end{bmatrix}\\leftarrow\\begin{bmatrix}\\theta_1^0\\\\\\theta_2^0\\\\\\vdots\\end{bmatrix}-\\begin{bmatrix}\\eta \\frac{\\partial L}{\\partial \\theta_1}|_{\\theta=\\theta^0}\\\\\\eta\\frac{\\partial L}{\\partial\\theta_2}| _{\\theta=\\theta^0}\\\\\\vdots\\end{bmatrix}\\) 或寫成 \\(\\theta^1\\leftarrow \\theta^0-\\eta g\\) batch training 將樣本依批次(batch)進行更新，當所有的 batches 都跑過一遍，稱為一個 epoch 4. ReLU 用 hard sigmoid 的方式來表示。 其每一個 hard sigmoid 由兩個 Rectified Linear Unit(ReLU) 組成， 每一個 ReLU 寫成：\\(\\boxed{\\red{c}\\text{ max}(0,\\green{b}+\\blue{w}x_1)}\\) 故 Model 可以寫成：\\(\\boxed{y=b+\\sum_{\\red{2}i}\\text{max}(0,b_i+\\sum_j{w_{ij}x_j})}\\) 其中我們選用來逼近的函式，稱為 Activation function。 深度學習 Neural Network \\(\\boxed{y=b+c^T\\sigma(b_i+Wx)}\\) Multiple hidden layers -\u003e Deep learning ","wordCount":"235","inLanguage":"zh-tw","datePublished":"2023-08-02T23:56:25+08:00","dateModified":"2023-08-02T23:56:25+08:00","author":{"@type":"Person","name":"Rain Hu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://intervalrain.github.io/ml/lhy01/"},"publisher":{"@type":"Organization","name":"Rain Hu's Workspace","logo":{"@type":"ImageObject","url":"https://intervalrain.github.io/images/rain.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://intervalrain.github.io/ accesskey=h title="Rain Hu's Workspace (Alt + H)"><img src=https://intervalrain.github.io/images/rain.png alt aria-label=logo height=35>Rain Hu's Workspace</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://intervalrain.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://intervalrain.github.io/aboutme title="About me"><span>About me</span></a></li><li><a href=https://intervalrain.github.io/archives title=Archives><span>Archives</span></a></li><li><a href=https://intervalrain.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://intervalrain.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://intervalrain.github.io/csharp/csharp title=C#><span>C#</span></a></li><li><a href=https://intervalrain.github.io/csindex title=CS><span>CS</span></a></li><li><a href=https://intervalrain.github.io/leetcode title=LeetCode><span>LeetCode</span></a></li><li><a href=https://intervalrain.github.io/ai title=AI><span>AI</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://intervalrain.github.io/>首頁</a>&nbsp;»&nbsp;<a href=https://intervalrain.github.io/ml/>MLs</a></div><h1 class="post-title entry-hint-parent">[ML] 01. 機器學習基本概念簡介</h1><div class=post-description>什麼是機器學習，機器學習任務，監督式學習的運作流程</div><div class=post-meta><span title='2023-08-02 23:56:25 +0800 +0800'>August 2, 2023</span>&nbsp;·&nbsp;2 分鐘&nbsp;·&nbsp;Rain Hu&nbsp;|&nbsp;<a href=https://github.com/intervalrain/intervalrain.github.io/tree/main/content//ML/lhy01.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目錄</span></summary><div class=inner><ul><li><a href=#%e5%89%8d%e8%a8%80 aria-label=前言>前言</a></li><li><a href=#%e6%a9%9f%e5%99%a8%e5%a6%82%e4%bd%95%e5%ad%b8%e7%bf%92 aria-label=機器如何學習>機器如何學習</a><ul><li><a href=#1-%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86%e8%a8%93%e7%b7%b4%e4%b8%89%e6%ad%a5%e9%a9%9f aria-label="1. 基本原理(訓練三步驟)">1. 基本原理(訓練三步驟)</a></li><li><a href=#2-linear-model aria-label="2. Linear Model">2. Linear Model</a></li><li><a href=#3-piecewise-linear-curvessigmoid aria-label="3. Piecewise Linear Curves(Sigmoid)">3. Piecewise Linear Curves(Sigmoid)</a></li><li><a href=#4-relu aria-label="4. ReLU">4. ReLU</a></li></ul></li><li><a href=#%e6%b7%b1%e5%ba%a6%e5%ad%b8%e7%bf%92 aria-label=深度學習>深度學習</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h1 id=前言>前言<a hidden class=anchor aria-hidden=true href=#前言>#</a></h1><details><summary>什麼是機器學習</summary><ul><li>機器學習(Machine Learning)，就是利用機器的力量幫忙找出函式。<ul><li>Input 可以是<ul><li>vector</li><li>matrix</li><li>sequence</li></ul></li><li>Output 可以是<ul><li><strong>Regression</strong></li><li><strong>Classification</strong></li><li><strong>Structed Learning</strong>(令機器產生有結構的東西 eg. text, image)<details><summary>示意圖</summary><img alt=1_1 loading=lazy src=/ML/lhy/1_1.png></details></details><details><summary>什麼是深度學習</summary></li></ul></li></ul></li><li>深度學習(Deep Learning)，就是利用神經網路(neural network)的方式來產生函數。</details></li></ul><h1 id=機器如何學習>機器如何學習<a hidden class=anchor aria-hidden=true href=#機器如何學習>#</a></h1><h2 id=1-基本原理訓練三步驟>1. 基本原理(訓練三步驟)<a hidden class=anchor aria-hidden=true href=#1-基本原理訓練三步驟>#</a></h2><p><img alt=1_2 loading=lazy src=/ML/lhy/1_2.png></p><details><summary>Step 1: 使用合適的 Model</summary><ul><li>\(y=f(\text{\red{data}})\)<ul><li>Function with unknown parameters</li><li><strong>Model</strong>: \(\boxed{y=b+wx_1}\)<ul><li>\(w: \text{weight}\)</li><li>\(b: \text{bias}\)</li><li>\(x: \text{feature}\)</details></li></ul></li></ul></li></ul><details><summary>Step 2: 定義 Loss function</summary><ul><li>Define loss from training data<ul><li>以 Model 的參數 \(w,b\) 來計算 Loss</li><li>物理意義：Loss 愈大代表參數愈不好，Loss 愈小代表參數愈好。</li></ul></li><li>計算方法：求估計的值與實際的值(label)之間的差距<ul><li>Loss function: \(\boxed{L=\frac{1}{N}\sum_ne_n}\)</li><li>MAE (mean absolute error): \(e=|y-\hat{y}|\)</li><li>MSE (mean square error): \(e=(y-\hat{y})^2\)</li><li>Cross-entropy: 計算<strong>機率分布</strong>之間的差距</li></ul></li><li><em><strong>Error Surface</strong></em>: 根據不同的參數，計算出 loss 所畫出來的等高線圖。
<img alt=1_3 loading=lazy src=/ML/lhy/1_3.png></details><details><summary>Step 3: Optimization</summary></li><li>找到 loss 最小的參數組合 \((w,b)\)</li><li>方法：<strong>Gradient Descent</strong><ul><li>\(\boxed{w&rsquo; = w - \red{\eta}\frac{\partial L}{\partial w}|_{w=w^0,b=b^0}}\)</li><li>\(\boxed{b&rsquo; = b - \red{\eta}\frac{\partial L}{\partial b}|_{w=w^0,b=b^0}}\)</li><li>\(\red{\eta}\): <strong>學習率 learning rate</strong>, 決定 gradient descent 的一步有多大步
<img alt=1_4 loading=lazy src=/ML/lhy/1_4.png></details></li></ul></li></ul><h2 id=2-linear-model>2. Linear Model<a hidden class=anchor aria-hidden=true href=#2-linear-model>#</a></h2><ul><li>\(\boxed{f\leftarrow y=b+\sum_{j=1}^{n}{w_jx_j}}\)<ul><li>不只考慮前一天的觀看人數 \(x_1\)，也考慮前二~七天 \(x_2, x_3, &mldr; , x_7\)。</li><li>當參數變多時，命中率可望有效提升。</li></ul></li></ul><h2 id=3-piecewise-linear-curvessigmoid>3. Piecewise Linear Curves(Sigmoid)<a hidden class=anchor aria-hidden=true href=#3-piecewise-linear-curvessigmoid>#</a></h2><ul><li>\(\text{Sigmoid Function:} \boxed{y=\red{c}\frac{1}{1+e^{-(\green{b}+\blue{w}x_1)}}}=\boxed{\red{c}\text{ sigmoid}(\green{b}+\blue{w}x_1)}\)</li><li>將 \(w_ix_i\) 替換成 \(c_i\text{ sigmoid}(b_i+w_ix_i)\)<ul><li>特徵為1時，\(\boxed{y=b+\sum_i{c_i\text{ sigmoid}(b_i+ w_ix_1)}}\)</li><li>特徵>1時，\(\boxed{y=b+\sum_i{c_i\text{ sigmoid}(b_i+\sum_j w_{ij}x_j)}}\)</li></ul></li><li>意義：一條曲線可以由多個鋸齒狀的線段(hard sigmoid)的總合，我們可以用 sigmoid 函數來逼近 hard sigmoid。事實上，sigmoid 的個數就是神經網路中一層 neuron 的 node 數，至於使用幾個 sigmoid 是 hyper parameter。
<img alt=1_5 loading=lazy src=/ML/lhy/1_5.png><details><summary>可將公式轉成矩陣計算+激勵函數的形式：</summary><img alt=1_6 loading=lazy src=/ML/lhy/1_6.png>
<img alt=1_7 loading=lazy src=/ML/lhy/1_7.png>
<img alt=1_8 loading=lazy src=/ML/lhy/1_8.png></details></li><li>以線性代數方式表示：\(\boxed{y=b+c^T\sigma(b_i+Wx)}\)<ul><li>將 \(b\)、\(b_i\)、\(W\)、\(c^T\) 等所有參數統稱為 \(\theta\)</li><li>故 Loss 可表示成 \(L(\theta)\)</li><li>重覆 gradient descent 的方法，更新(update) 參數。</li><li>梯度 gradient，\(g=\)
\(\begin{bmatrix}\frac{\partial L}{\partial \theta_1}|_{\theta=\theta^0}\\\frac{\partial L}{\partial \theta_2}| _{\theta=\theta^0}\\\vdots\end{bmatrix}=\nabla L(\theta^0)\)</li><li>更新(update)計算：\(\begin{bmatrix}\theta_1^1\\\theta_2^1\\\vdots\end{bmatrix}\leftarrow\begin{bmatrix}\theta_1^0\\\theta_2^0\\\vdots\end{bmatrix}-\begin{bmatrix}\eta \frac{\partial L}{\partial \theta_1}|_{\theta=\theta^0}\\\eta\frac{\partial L}{\partial\theta_2}| _{\theta=\theta^0}\\\vdots\end{bmatrix}\)</li><li>或寫成 \(\theta^1\leftarrow \theta^0-\eta g\)<details><summary>batch training</summary><img alt=1_9 loading=lazy src=/ML/lhy/1_9.png></details></li><li>將樣本依批次(batch)進行更新，當所有的 batches 都跑過一遍，稱為一個 <strong>epoch</strong></li></ul></li></ul><h2 id=4-relu>4. ReLU<a hidden class=anchor aria-hidden=true href=#4-relu>#</a></h2><ul><li>用 hard sigmoid 的方式來表示。<ul><li>其每一個 hard sigmoid 由兩個 <strong>Rectified Linear Unit(ReLU)</strong> 組成，</li><li>每一個 ReLU 寫成：\(\boxed{\red{c}\text{ max}(0,\green{b}+\blue{w}x_1)}\)</li><li>故 Model 可以寫成：\(\boxed{y=b+\sum_{\red{2}i}\text{max}(0,b_i+\sum_j{w_{ij}x_j})}\)</li><li>其中我們選用來逼近的函式，稱為 <strong>Activation function</strong>。</li></ul></li></ul><h1 id=深度學習>深度學習<a hidden class=anchor aria-hidden=true href=#深度學習>#</a></h1><ul><li>Neural Network<ul><li>\(\boxed{y=b+c^T\sigma(b_i+Wx)}\)</li><li>Multiple hidden layers -> Deep learning
<img alt=1_10 loading=lazy src=/ML/lhy/1_10.png></li></ul></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://intervalrain.github.io/tags/ml/>ML</a></li></ul><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://intervalrain.github.io/>Rain Hu's Workspace</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="複製";function s(){t.innerHTML="已複製！",setTimeout(()=>{t.innerHTML="複製"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>