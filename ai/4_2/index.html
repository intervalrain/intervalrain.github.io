<!doctype html><html lang=zh-tw dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[AI] 多元分類問題 | Rain Hu's Workspace</title><meta name=keywords content="AI"><meta name=description content="Multi categorical classification problem"><meta name=author content="Rain Hu"><link rel=canonical href=https://intervalrain.github.io/><meta name=google-site-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.4c6c0beaf1dfe52cd0f712a5896ac127e66fd064cfc598e04750f496d470699e.css integrity="sha256-TGwL6vHf5SzQ9xKliWrBJ+Zv0GTPxZjgR1D0ltRwaZ4=" rel="preload stylesheet" as=style><link rel=icon href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=16x16 href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=32x32 href=https://intervalrain.github.io/images/rain.png><link rel=apple-touch-icon href=https://intervalrain.github.io/images/rain.png><link rel=mask-icon href=https://intervalrain.github.io/images/rain.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh-tw href=https://intervalrain.github.io/ai/4_2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css integrity=sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js integrity=sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script><script src=https://utteranc.es/client.js repo=intervalrain.github.io issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script><meta property="og:url" content="https://intervalrain.github.io/ai/4_2/"><meta property="og:site_name" content="Rain Hu's Workspace"><meta property="og:title" content="[AI] 多元分類問題"><meta property="og:description" content="Multi categorical classification problem"><meta property="og:locale" content="zh-tw"><meta property="og:type" content="article"><meta property="article:section" content="ai"><meta property="article:published_time" content="2025-01-06T22:58:45+08:00"><meta property="article:modified_time" content="2025-01-06T22:58:45+08:00"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="[AI] 多元分類問題"><meta name=twitter:description content="Multi categorical classification problem"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"[AI] 多元分類問題","item":"https://intervalrain.github.io/ai/4_2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[AI] 多元分類問題","name":"[AI] 多元分類問題","description":"Multi categorical classification problem","keywords":["AI"],"articleBody":"認識路透社(Reuters)資料集 透過 tensorflow 引入資料集\n參數 path: 數據的緩存位置（相對於 ~/.keras/dataset）。 num_words: 整數或 None。單詞按其出現頻率（在訓練集中）進行排名，並且僅保留 num_words 個最常見的單詞。任何較不常見的單詞在序列數據中都將顯示為 oov_char值。如果為 None，則保留所有單詞。默認為 None。 skip_top: 跳過前 N 個最常出現的單詞（這些單詞可能沒有信息量）。這些單詞在數據集中將顯示為 oov_char 值。0 表示不跳過任何單詞。默認為 0。 maxlen: int 或 None。最大序列長度。任何較長的序列都將被截斷。None 表示不截斷。默認為 None。 test_split: 介於 0. 與 1. 之間的浮點數。用作測試資料集的比例。0.2 表示 20% 的資料集用作測試資料。預設值為 0.2。 seed: 整數。用於可重複資料洗牌的種子。 start_char: 整數。序列的開頭將標記為此字元。0 通常是填充字元。預設值為 1。 oov_char: 整數。超出詞彙表的字元。由於 num_words 或 skip_top 限制而被刪除的詞彙將替換為此字元。 index_from: 整數。使用此索引及更高的索引來索引實際詞彙。 回傳值 Numpy 陣列的 tuple: (x_train, y_train), (x_test, y_test)。 相同於 IMDB 資料集，資料集包含了許多相異單字，這數字對訓練而言非常龐大，且對分類任務沒什麼幫助，所以我們只保留 10000 個最常出現的單字 from tensorflow.keras.datasets import reuters (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000) Reuters 路透社資料集是 1986 年由路透社發佈的一組簡短新聞和對應主題的資料集，被廣泛用於文章分類的研究。\nprint(train_data.shape) print(train_labels.shape) print(test_data.shape) print(test_labels.shape) \u003e (8982,) \u003e (8982,) \u003e (2246,) \u003e (2246,) 類似於 IMDB，資料的組成是由一連串的數字所組成，如：\n\\( \\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|} \\hline \\text{(保留)} \u0026 \\text{the} \u0026 \\text{of} \u0026 \\text{to} \u0026 \\text{…} \u0026 \\text{mln} \u0026 \\text{…} \u0026 \\text{much} \u0026 \\text{…} \u0026 \\text{w} \u0026 \\text{…} \\\\ \\hline 0 \u0026 1 \u0026 2 \u0026 3 \u0026 … \u0026 8 \u0026 … \u0026 386 \u0026 … \u0026 1969 \u0026 …\\\\ \\hline \\end{array} \\)\n每個數字代表一個單字，編號愈前面代表愈常用。 print(train_data[0]) \u003e [1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, ................] 我們可以用 imdb.get_word_index(path=\"imdb_word_index.json\") 來得到這個字典，並試著還原原始評論。 注意按引 0~2 為留保字，故索引值需位移 3。 dict = imdb.get_word_index(path=\"imdb_word_index.json\") index_to_word = {value: key for key, value in dict.items()} sentence_0 = ' '.join([index_to_word.get(idx, '?') for idx in train_data[0]]) print(sentence_0) \u003e ? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 ... labels 是由 0 到 45 之間的整數，用來表示特定的主題。 print(train_labels[:10]) \u003e array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0]) 準備資料 multi-hot 法\n\\( \\begin{array}{cc|c|c|c|c|c|c} \\text{data}\u0026\u00261\u00262\u00263\u00264\u00265\u00266\\\\\\hline [1,2,3]\u0026\\rightarrow\u00261\u00261\u00261\u00260\u00260\u00260\\\\ [2,4,5]\u0026\\rightarrow\u00260\u00261\u00260\u00261\u00261\u00260\\\\ [2,3,6]\u0026\\rightarrow\u00260\u00261\u00261\u00260\u00260\u00261\\\\ \\end{array} \\) import numpy as np def vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results x_train = vectorize_sequences(train_data) x_test = vectorize_sequences(test_data) 我們需要對 labels 作 ont-hot 處理，其對應的 loss function 為 categorical_crossentropy 若不使用 multi-hot 則要選用 sparse_categorical_crossentropy one-hot\n\\( \\begin{array}{cc|c|c|c|c|c} \\text{labels}\u0026\u00261\u00262\u00263\u00264\u00265\\\\\\hline 1\u0026\\rightarrow\u00261\u00260\u00260\u00260\u00260\\\\ 3\u0026\\rightarrow\u00260\u00260\u00261\u00260\u00260\\\\ 4\u0026\\rightarrow\u00260\u00260\u00260\u00261\u00260\\\\ \\end{array} \\) def to_one_hot(labels): shift = np.min(labels) dimension = np.max(labels) - shift + 1 results = np.zeros((len(labels), dimension)) for i, label in enumerate(labels): results[i, label - shift] = 1. return results y_train = to_one_hot(train_labels) y_test = to_one_hot(test_labels) Keras 則提供了 to_categorical 可以處理相同的工作 from tensorflow.keras.utils import to_categorical y_train = to_categorical(train_labels) y_test = to_categorical(test_labels) 建立神經網路 我們在 IMDB 資料集中使用了 16 維的空間，但對於一個有 46 類的分類問題，可能會因維度不足造成資訊遺失。我們嘗試提高層的單元數： 我設計以下的三層神經網路架構：\n\\( \\begin{array}{ccc} \\text{輸入(向量化文字)}\\\\ \\downarrow\\\\ \\boxed{\\text{密集(單元=64)}} \u0026 \\text{relu} \u0026 \\text{hidden layer}\\\\ \\downarrow\\\\ \\boxed{\\text{密集(單元=64)}} \u0026 \\text{relu} \u0026 \\text{hidden layer}\\\\ \\downarrow\\\\ \\boxed{\\text{密集(單元=46)}} \u0026 \\text{softmax} \u0026 \\text{output layer}\\\\ \\downarrow\\\\ \\text{輸出(預測值)}\\\\ \\end{array} \\) import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ layers.Dense(64, activation='relu'), layers.Dense(64, activation='relu'), layers.Dense(46, activation='softmax') ]) compile model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) 建立驗證集 x_val = x_train[:1000] partial_x_train = x_train[1000:] y_val = y_train[:1000] partial_y_train = y_train[1000:] 訓練模型 history = model.fit(partial_x_train, partial_y_train, epochs=20 batch_size=512, validation_data=(x_val, y_val)) 繪製訓練與驗證圖 import matplotlib.pyplot as plt loss = history.history['loss'] val_loss = history.history['val_loss'] epochs = range(1, len(loss) + 1) plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend() plt.show() plt.clf() acc = history.history['accuracy'] val_acc = history.history['val_accuracy'] plt.plot(epochs, acc, 'bo', label='Training accuracy') plt.plot(epochs, val_acc, 'b', label='Validation accuracy') plt.title('Training and validation accuracy') plt.xlabel('Epochs') plt.ylabel('Accuracy') plt.legend() plt.show() 一樣有發生 overfitting，大約在 epochs=9 左右 用測試集驗看看準確率 predictions = model.predict(x_test, batch_size=128) test_loss, test_acc = model.evaluate(x_test, y_test) print(f'test_acc: {test_acc}') \u003e test_acc: 0.7862867116928101 同樣試試看用 dropout + early stopping + sparse_crossentropy 的程式碼： from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.callbacks import EarlyStopping model = keras.Sequential([ layers.Dense(64, activation='relu'), layers.Dropout(0.5), layers.Dense(64, activation='relu'), layers.Dropout(0.5), layers.Dense(46, activation='softmax') ]) model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy']) early_stopping = EarlyStopping( monitor='val_loss', patience=4, restore_best_weights=True) x_val = x_train[:1000] partial_x_train = x_train[1000:] y_val = train_labels[:1000] partial_y_train = train_labels[1000:] history = model.fit(partial_x_train, partial_y_train, epochs=40, batch_size=512, validation_data=(x_val, y_val), callbacks=[early_stopping]) ","wordCount":"578","inLanguage":"zh-tw","datePublished":"2025-01-06T22:58:45+08:00","dateModified":"2025-01-06T22:58:45+08:00","author":{"@type":"Person","name":"Rain Hu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://intervalrain.github.io/ai/4_2/"},"publisher":{"@type":"Organization","name":"Rain Hu's Workspace","logo":{"@type":"ImageObject","url":"https://intervalrain.github.io/images/rain.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://intervalrain.github.io/ accesskey=h title="Rain Hu's Workspace (Alt + H)"><img src=https://intervalrain.github.io/images/rain.png alt aria-label=logo height=35>Rain Hu's Workspace</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://intervalrain.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://intervalrain.github.io/aboutme title="About me"><span>About me</span></a></li><li><a href=https://intervalrain.github.io/archives title=Archives><span>Archives</span></a></li><li><a href=https://intervalrain.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://intervalrain.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://intervalrain.github.io/csharp/csharp title=C#><span>C#</span></a></li><li><a href=https://intervalrain.github.io/csindex title=CS><span>CS</span></a></li><li><a href=https://intervalrain.github.io/leetcode title=LeetCode><span>LeetCode</span></a></li><li><a href=https://intervalrain.github.io/ai title=AI><span>AI</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://intervalrain.github.io/>首頁</a></div><h1 class="post-title entry-hint-parent">[AI] 多元分類問題</h1><div class=post-description>Multi categorical classification problem</div><div class=post-meta><span title='2025-01-06 22:58:45 +0800 +0800'>January 6, 2025</span>&nbsp;·&nbsp;3 分鐘&nbsp;·&nbsp;Rain Hu&nbsp;|&nbsp;<a href=https://github.com/intervalrain/intervalrain.github.io/tree/main/content//AI/4_2.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目錄</span></summary><div class=inner><ul><li><a href=#%e8%aa%8d%e8%ad%98%e8%b7%af%e9%80%8f%e7%a4%bereuters%e8%b3%87%e6%96%99%e9%9b%86 aria-label=認識路透社(Reuters)資料集>認識路透社(Reuters)資料集</a></li><li><a href=#%e6%ba%96%e5%82%99%e8%b3%87%e6%96%99 aria-label=準備資料>準備資料</a></li><li><a href=#%e5%bb%ba%e7%ab%8b%e7%a5%9e%e7%b6%93%e7%b6%b2%e8%b7%af aria-label=建立神經網路>建立神經網路</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=認識路透社reuters資料集>認識路透社(Reuters)資料集<a hidden class=anchor aria-hidden=true href=#認識路透社reuters資料集>#</a></h2><ul><li><p>透過 tensorflow 引入資料集</p><ul><li><strong>參數</strong><ul><li><code>path</code>: 數據的緩存位置（相對於 ~/.keras/dataset）。</li><li><code>num_words</code>: 整數或 None。單詞按其出現頻率（在訓練集中）進行排名，並且僅保留 num_words 個最常見的單詞。任何較不常見的單詞在序列數據中都將顯示為 oov_char值。如果為 None，則保留所有單詞。默認為 None。</li><li><code>skip_top</code>: 跳過前 N 個最常出現的單詞（這些單詞可能沒有信息量）。這些單詞在數據集中將顯示為 oov_char 值。0 表示不跳過任何單詞。默認為 0。</li><li><code>maxlen</code>: int 或 None。最大序列長度。任何較長的序列都將被截斷。None 表示不截斷。默認為 None。</li><li><code>test_split</code>: 介於 0. 與 1. 之間的浮點數。用作測試資料集的比例。0.2 表示 20% 的資料集用作測試資料。預設值為 0.2。</li><li><code>seed</code>: 整數。用於可重複資料洗牌的種子。</li><li><code>start_char</code>: 整數。序列的開頭將標記為此字元。0 通常是填充字元。預設值為 1。</li><li><code>oov_char</code>: 整數。超出詞彙表的字元。由於 num_words 或 skip_top 限制而被刪除的詞彙將替換為此字元。</li><li><code>index_from</code>: 整數。使用此索引及更高的索引來索引實際詞彙。</li></ul></li><li><strong>回傳值</strong><ul><li>Numpy 陣列的 tuple: <code>(x_train, y_train), (x_test, y_test)</code>。</li></ul></li><li>相同於 IMDB 資料集，資料集包含了許多相異單字，這數字對訓練而言非常龐大，且對分類任務沒什麼幫助，所以我們只保留 10000 個最常出現的單字</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.datasets <span style=color:#f92672>import</span> reuters
</span></span><span style=display:flex><span>(train_data, train_labels), (test_data, test_labels) <span style=color:#f92672>=</span> reuters<span style=color:#f92672>.</span>load_data(num_words<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>)
</span></span></code></pre></div></li><li><p>Reuters 路透社資料集是 1986 年由路透社發佈的一組簡短新聞和對應主題的資料集，被廣泛用於文章分類的研究。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(train_data<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(train_labels<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(test_data<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(test_labels<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;</span> (<span style=color:#ae81ff>8982</span>,)
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;</span> (<span style=color:#ae81ff>8982</span>,)
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;</span> (<span style=color:#ae81ff>2246</span>,)
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;</span> (<span style=color:#ae81ff>2246</span>,)
</span></span></code></pre></div></li><li><p>類似於 IMDB，資料的組成是由一連串的數字所組成，如：<br>\(
\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline \text{(保留)} & \text{the} & \text{of} & \text{to} & \text{&mldr;} & \text{mln} & \text{&mldr;} & \text{much} & \text{&mldr;} & \text{w} & \text{&mldr;} \\
\hline 0 & 1 & 2 & 3 & &mldr; & 8 & &mldr; & 386 & &mldr; & 1969 & &mldr;\\
\hline
\end{array}
\)</p><ul><li>每個數字代表一個單字，編號愈前面代表愈常用。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(train_data[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;</span> [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>43</span>, <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>447</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>25</span>, <span style=color:#ae81ff>207</span>, <span style=color:#ae81ff>270</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>3095</span>, <span style=color:#ae81ff>111</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>369</span>, <span style=color:#ae81ff>186</span>, <span style=color:#f92672>................</span>]
</span></span></code></pre></div><ul><li>我們可以用 <code>imdb.get_word_index(path="imdb_word_index.json")</code> 來得到這個字典，並試著還原原始評論。</li><li>注意按引 <code>0~2</code> 為留保字，故索引值需位移 3。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dict <span style=color:#f92672>=</span> imdb<span style=color:#f92672>.</span>get_word_index(path<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;imdb_word_index.json&#34;</span>)
</span></span><span style=display:flex><span>index_to_word <span style=color:#f92672>=</span> {value: key <span style=color:#66d9ef>for</span> key, value <span style=color:#f92672>in</span> dict<span style=color:#f92672>.</span>items()}
</span></span><span style=display:flex><span>sentence_0 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39; &#39;</span><span style=color:#f92672>.</span>join([index_to_word<span style=color:#f92672>.</span>get(idx, <span style=color:#e6db74>&#39;?&#39;</span>) <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> train_data[<span style=color:#ae81ff>0</span>]])
</span></span><span style=display:flex><span>print(sentence_0)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;</span> <span style=color:#960050;background-color:#1e0010>?</span> <span style=color:#960050;background-color:#1e0010>?</span> <span style=color:#960050;background-color:#1e0010>?</span> said <span style=color:#66d9ef>as</span> a result of its december acquisition of space co it expects earnings per share <span style=color:#f92672>in</span> <span style=color:#ae81ff>1987</span> of <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>15</span> to <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>30</span> <span style=color:#f92672>...</span>
</span></span></code></pre></div><ul><li>labels 是由 0 到 45 之間的整數，用來表示特定的主題。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(train_labels[:<span style=color:#ae81ff>10</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;</span> array([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>])
</span></span></code></pre></div></li></ul><h2 id=準備資料>準備資料<a hidden class=anchor aria-hidden=true href=#準備資料>#</a></h2><ul><li>multi-hot 法<br>\(
\begin{array}{cc|c|c|c|c|c|c}
\text{data}&&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6\\\hline
[1,2,3]&\rightarrow&amp;1&amp;1&amp;1&amp;0&amp;0&amp;0\\
[2,4,5]&\rightarrow&amp;0&amp;1&amp;0&amp;1&amp;1&amp;0\\
[2,3,6]&\rightarrow&amp;0&amp;1&amp;1&amp;0&amp;0&amp;1\\
\end{array}
\)</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>vectorize_sequences</span>(sequences, dimension<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>):
</span></span><span style=display:flex><span>    results <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((len(sequences), dimension))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, sequence <span style=color:#f92672>in</span> enumerate(sequences):
</span></span><span style=display:flex><span>        results[i, sequence] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> results
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>x_train <span style=color:#f92672>=</span> vectorize_sequences(train_data)
</span></span><span style=display:flex><span>x_test <span style=color:#f92672>=</span> vectorize_sequences(test_data)
</span></span></code></pre></div><ul><li>我們需要對 labels 作 ont-hot 處理，其對應的 loss function 為 <code>categorical_crossentropy</code></li><li>若不使用 multi-hot 則要選用 <code>sparse_categorical_crossentropy</code></li><li>one-hot<br>\(
\begin{array}{cc|c|c|c|c|c}
\text{labels}&&amp;1&amp;2&amp;3&amp;4&amp;5\\\hline
1&\rightarrow&amp;1&amp;0&amp;0&amp;0&amp;0\\
3&\rightarrow&amp;0&amp;0&amp;1&amp;0&amp;0\\
4&\rightarrow&amp;0&amp;0&amp;0&amp;1&amp;0\\
\end{array}
\)</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>to_one_hot</span>(labels):
</span></span><span style=display:flex><span>    shift <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>min(labels)
</span></span><span style=display:flex><span>    dimension <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>max(labels) <span style=color:#f92672>-</span> shift <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    results <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((len(labels), dimension))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, label <span style=color:#f92672>in</span> enumerate(labels):
</span></span><span style=display:flex><span>        results[i, label <span style=color:#f92672>-</span> shift] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> results
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>y_train <span style=color:#f92672>=</span> to_one_hot(train_labels)
</span></span><span style=display:flex><span>y_test <span style=color:#f92672>=</span> to_one_hot(test_labels)
</span></span></code></pre></div><ul><li>Keras 則提供了 <code>to_categorical</code> 可以處理相同的工作</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.utils <span style=color:#f92672>import</span> to_categorical
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y_train <span style=color:#f92672>=</span> to_categorical(train_labels)
</span></span><span style=display:flex><span>y_test <span style=color:#f92672>=</span> to_categorical(test_labels)
</span></span></code></pre></div><h2 id=建立神經網路>建立神經網路<a hidden class=anchor aria-hidden=true href=#建立神經網路>#</a></h2><ul><li>我們在 IMDB 資料集中使用了 16 維的空間，但對於一個有 46 類的分類問題，可能會因維度不足造成資訊遺失。我們嘗試提高層的單元數：</li><li>我設計以下的三層神經網路架構：<br>\(
\begin{array}{ccc}
\text{輸入(向量化文字)}\\
\downarrow\\
\boxed{\text{密集(單元=64)}} & \text{relu} & \text{hidden layer}\\
\downarrow\\
\boxed{\text{密集(單元=64)}} & \text{relu} & \text{hidden layer}\\
\downarrow\\
\boxed{\text{密集(單元=46)}} & \text{softmax} & \text{output layer}\\
\downarrow\\
\text{輸出(預測值)}\\
\end{array}
\)<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> tensorflow <span style=color:#66d9ef>as</span> tf
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow <span style=color:#f92672>import</span> keras
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras <span style=color:#f92672>import</span> layers
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>64</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>64</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>46</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;softmax&#39;</span>)
</span></span><span style=display:flex><span>])
</span></span></code></pre></div><ul><li>compile</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model<span style=color:#f92672>.</span>compile(optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;rmsprop&#39;</span>,
</span></span><span style=display:flex><span>              loss<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;categorical_crossentropy&#39;</span>,
</span></span><span style=display:flex><span>              metrics<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;accuracy&#39;</span>])
</span></span></code></pre></div><ul><li>建立驗證集</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x_val <span style=color:#f92672>=</span> x_train[:<span style=color:#ae81ff>1000</span>]
</span></span><span style=display:flex><span>partial_x_train <span style=color:#f92672>=</span> x_train[<span style=color:#ae81ff>1000</span>:]
</span></span><span style=display:flex><span>y_val <span style=color:#f92672>=</span> y_train[:<span style=color:#ae81ff>1000</span>]
</span></span><span style=display:flex><span>partial_y_train <span style=color:#f92672>=</span> y_train[<span style=color:#ae81ff>1000</span>:]
</span></span></code></pre></div><ul><li>訓練模型</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>history <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(partial_x_train,
</span></span><span style=display:flex><span>                    partial_y_train,
</span></span><span style=display:flex><span>                    epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>
</span></span><span style=display:flex><span>                    batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>,
</span></span><span style=display:flex><span>                    validation_data<span style=color:#f92672>=</span>(x_val, y_val))
</span></span></code></pre></div><ul><li>繪製訓練與驗證圖</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loss <span style=color:#f92672>=</span> history<span style=color:#f92672>.</span>history[<span style=color:#e6db74>&#39;loss&#39;</span>]
</span></span><span style=display:flex><span>val_loss <span style=color:#f92672>=</span> history<span style=color:#f92672>.</span>history[<span style=color:#e6db74>&#39;val_loss&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>epochs <span style=color:#f92672>=</span> range(<span style=color:#ae81ff>1</span>, len(loss) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(epochs, loss, <span style=color:#e6db74>&#39;bo&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Training loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(epochs, val_loss, <span style=color:#e6db74>&#39;b&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Validation loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Training and validation loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Epochs&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><img alt=validation1 loading=lazy src=/ai/AI/4_2/validation_plot1.png><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt<span style=color:#f92672>.</span>clf()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>acc <span style=color:#f92672>=</span> history<span style=color:#f92672>.</span>history[<span style=color:#e6db74>&#39;accuracy&#39;</span>]
</span></span><span style=display:flex><span>val_acc <span style=color:#f92672>=</span> history<span style=color:#f92672>.</span>history[<span style=color:#e6db74>&#39;val_accuracy&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(epochs, acc, <span style=color:#e6db74>&#39;bo&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Training accuracy&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(epochs, val_acc, <span style=color:#e6db74>&#39;b&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Validation accuracy&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Training and validation accuracy&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Epochs&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Accuracy&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><img alt=validation2 loading=lazy src=/ai/AI/4_2/validation_plot2.png><ul><li>一樣有發生 overfitting，大約在 epochs=9 左右</li><li>用測試集驗看看準確率</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>predictions <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(x_test, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>)
</span></span><span style=display:flex><span>test_loss, test_acc <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>evaluate(x_test, y_test)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;test_acc: </span><span style=color:#e6db74>{</span>test_acc<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;</span> test_acc: <span style=color:#ae81ff>0.7862867116928101</span>
</span></span></code></pre></div><ul><li>同樣試試看用 dropout + early stopping + sparse_crossentropy 的程式碼：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow <span style=color:#f92672>import</span> keras
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras <span style=color:#f92672>import</span> layers
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.callbacks <span style=color:#f92672>import</span> EarlyStopping
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>64</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0.5</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>64</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0.5</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>46</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;softmax&#39;</span>)
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>compile(optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;adam&#34;</span>,
</span></span><span style=display:flex><span>            loss<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;sparse_categorical_crossentropy&#34;</span>,
</span></span><span style=display:flex><span>            metrics<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;accuracy&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>early_stopping <span style=color:#f92672>=</span> EarlyStopping(
</span></span><span style=display:flex><span>    monitor<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;val_loss&#39;</span>, 
</span></span><span style=display:flex><span>    patience<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>    restore_best_weights<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x_val <span style=color:#f92672>=</span> x_train[:<span style=color:#ae81ff>1000</span>]
</span></span><span style=display:flex><span>partial_x_train <span style=color:#f92672>=</span> x_train[<span style=color:#ae81ff>1000</span>:]
</span></span><span style=display:flex><span>y_val <span style=color:#f92672>=</span> train_labels[:<span style=color:#ae81ff>1000</span>]
</span></span><span style=display:flex><span>partial_y_train <span style=color:#f92672>=</span> train_labels[<span style=color:#ae81ff>1000</span>:]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>history <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(partial_x_train,
</span></span><span style=display:flex><span>                    partial_y_train,
</span></span><span style=display:flex><span>                    epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>,
</span></span><span style=display:flex><span>                    batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>,
</span></span><span style=display:flex><span>                    validation_data<span style=color:#f92672>=</span>(x_val, y_val),
</span></span><span style=display:flex><span>                    callbacks<span style=color:#f92672>=</span>[early_stopping])
</span></span></code></pre></div></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://intervalrain.github.io/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://intervalrain.github.io/ai/4_1/><span class=title>« 上一頁</span><br><span>[AI] 二元分類問題</span>
</a><a class=next href=https://intervalrain.github.io/ai/4_3/><span class=title>下一頁 »</span><br><span>[AI] 迴歸問題</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://intervalrain.github.io/>Rain Hu's Workspace</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="複製";function s(){t.innerHTML="已複製！",setTimeout(()=>{t.innerHTML="複製"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>