<!doctype html><html lang=zh-tw dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[AI] 3-5. 邏輯斯迴歸(logistic regression) | Rain Hu's Workspace</title>
<meta name=keywords content="AI"><meta name=description content="The introduction to logistic regression"><meta name=author content="Rain Hu"><link rel=canonical href=https://intervalrain.github.io/><meta name=google-site-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.662816b9df27c772d2b97c5f5f6bf4f2c5531051a330015f0ad4135736d0e56a.css integrity="sha256-ZigWud8nx3LSuXxfX2v08sVTEFGjMAFfCtQTVzbQ5Wo=" rel="preload stylesheet" as=style><link rel=icon href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=16x16 href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=32x32 href=https://intervalrain.github.io/images/rain.png><link rel=apple-touch-icon href=https://intervalrain.github.io/images/rain.png><link rel=mask-icon href=https://intervalrain.github.io/images/rain.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh-tw href=https://intervalrain.github.io/ai/3_5/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css integrity=sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js integrity=sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script src=https://utteranc.es/client.js repo=intervalrain.github.io issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script><meta property="og:url" content="https://intervalrain.github.io/ai/3_5/"><meta property="og:site_name" content="Rain Hu's Workspace"><meta property="og:title" content="[AI] 3-5. 邏輯斯迴歸(logistic regression)"><meta property="og:description" content="The introduction to logistic regression"><meta property="og:locale" content="zh-tw"><meta property="og:type" content="article"><meta property="article:section" content="ai"><meta property="article:published_time" content="2024-12-19T15:55:12+08:00"><meta property="article:modified_time" content="2024-12-19T15:55:12+08:00"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="[AI] 3-5. 邏輯斯迴歸(logistic regression)"><meta name=twitter:description content="The introduction to logistic regression"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"[AI] 3-5. 邏輯斯迴歸(logistic regression)","item":"https://intervalrain.github.io/ai/3_5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[AI] 3-5. 邏輯斯迴歸(logistic regression)","name":"[AI] 3-5. 邏輯斯迴歸(logistic regression)","description":"The introduction to logistic regression","keywords":["AI"],"articleBody":"貝式定理 假設有一個抽獎箱內有紅球與藍球，球上有標示 A 與 B 類別。 $$ \\begin{array}{|c|c|} \\hline A\u0026B\\\\\\hline \\blue{\\text{●}}\\blue{\\text{●}}\\blue{\\text{●}}\\red{\\text{●}}\u0026\\blue{\\text{●}}\\blue{\\text{●}}\\red{\\text{●}}\\red{\\text{●}}\\red{\\text{●}}\\red{\\text{●}}\\\\\\hline \\end{array} $$\n我們抽到藍球，它是來自於 A 的機率為何，即求 \\(P(A|\\blue{\\text{●}})\\)？ 根據貝式定理： $$ P(A|x)=\\frac{P(x|A)P(A)}{P(x|A)P(A)+P(x|B)P(B)} $$\n先驗機率 \\(P(A)=\\frac{\\text{A的球數}}{\\text{總球數}}=\\frac{4}{10}\\) \\(P(B)=\\frac{\\text{B的球數}}{\\text{總球數}}=\\frac{6}{10}\\) 條件機率 \\(P(\\blue{\\text{●}}|A)=\\frac{\\text{A中的}\\blue{\\text{●}}}{\\text{A的總球數}}=\\frac{3}{4}\\) \\(P(\\blue{\\text{●}}|B)=\\frac{\\text{B中的}\\blue{\\text{●}}}{\\text{B的總球數}}=\\frac{2}{6}\\) 套入公式可得 $$ P(A|\\blue{\\text{●}})=\\frac{P(\\blue{\\text{●}}|A)P(A)}{P(\\blue{\\text{●}}|A)P(A)+P(\\blue{\\text{●}}|B)P(B)}=\\frac{3/4\\times4/10}{3/4\\times4/10+2/6\\times6/10}=\\frac{3}{5} $$ 假設今天猜中類別才能得獎，已經知道是藍球的情況下，來自 A 的機率是 0.6，來自 B 的機率是 0.4，所以我們理論上會選擇 A，因為機率較大。換言之，在機器學習中，我們判斷一個二元分類的問題，我們會將分類判給機率 \u003e 0.5 的那個類別。 高斯分布 一維的高斯分分機率密度函數(probability density function, pdf)為： $$ f_{\\mu,\\sigma}(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\bigg\\lbrace-\\frac{(x-\\mu)^2}{2\\sigma^2}\\bigg\\rbrace $$ 其中 \\(\\mu\\) 為平均數(Mean)，決定分布的中心位置。 \\(\\sigma\\) 為標準差(Standard Deviation)，決定分布的寬度。 擴展到 n 維向量的多維高斯分布機率密度函數為： $$ f_{\\mu,\\Sigma}(x)=\\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}}exp\\bigg\\lbrace-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\bigg\\rbrace $$ 其中 x 是一階張量 $$ x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} $$\n\\(\\mu\\) 代表 x 在每個維度的均值 $$ \\mu = \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_n \\end{bmatrix} $$\n\\(\\Sigma\\) 是協方差矩陣，\\(\\Sigma\\in\\mathbb{R}^{n \\times n}\\)，表示數據分布的相關性與變異性： $$ \\Sigma = \\begin{bmatrix} \\sigma_{11} \u0026 \\sigma_{12} \u0026 \\cdots \u0026 \\sigma_{1n} \\\\ \\sigma_{21} \u0026 \\sigma_{22} \u0026 \\cdots \u0026 \\sigma_{2n} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\sigma_{n1} \u0026 \\sigma_{n2} \u0026 \\cdots \u0026 \\sigma_{nn} \\end{bmatrix} $$\n\\(|\\Sigma|\\)是協方差矩陣的行列式，表示協方差矩陣的尺度，用於歸一化分布。\n\\(\\Sigma^{-1}\\)是協方差矩陣的逆矩陣，用於計算標準化的二次型距離。\n\\((x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\)是馬氏距離（Mahalanobis Distance），表示點 \\(x\\) 與均值 \\(\\mu\\) 的加權距離。\n二元分類 我們嘗試使用高斯分布模型對一個簡單的問題進行描述：\n假設我們想用身高、體重來猜測一個人的性別： class HeightWeightGenerator: def __init__(self): # 設定男性身高體重的均值和協方差 self.male_mean = np.array([172, 70]) # [height, weight] self.male_cov = np.array([[60, 20], # height與weight的協方差矩陣 [20, 35]]) # 設定女性身高體重的均值和協方差 self.female_mean = np.array([162, 55]) self.female_cov = np.array([[35, 15], [15, 20]]) def generate_samples(self, n_samples=1000): # 生成性別 (0: 女性, 1: 男性) genders = np.random.binomial(n=1, p=0.5, size=n_samples) # 初始化數據陣列 data = np.zeros((n_samples, 2)) # 生成男性和女性的身高體重數據 male_indices = genders == 1 female_indices = genders == 0 # 使用多維高斯分布生成數據 data[male_indices] = np.random.multivariate_normal( self.male_mean, self.male_cov, size=np.sum(male_indices)) data[female_indices] = np.random.multivariate_normal( self.female_mean, self.female_cov, size=np.sum(female_indices)) return data, genders 假設 \\(x = [\\text{身高}, \\text{體重}]^T \\in \\mathbb{R}^2\\)：每個數據點的特徵向量。 類別 \\(y \\in {0,要 1}\\)：性別標籤，0 表示女，1 表示男。 基本假設 高斯分布假設\n\\(P(x|y=0) \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)\\)：女生特徵的高斯分布。 \\(P(x|y=1) \\sim \\mathcal{N}(\\mu_1, \\Sigma_1)\\)：男生特徵的高斯分布。 目標\n利用貝氏定理計算 \\(P(y|x)\\)： $$ P(y|x) = \\frac{P(x|y)P(y)}{P(x)} $$ 其中： \\(P(x|y)\\)：由高斯分布表示。 \\(P(y)\\)：類別的先驗概率（例如， \\(P(y=1)\\) 和 \\(P(y=0)\\) 可以從訓練數據中估計）。 \\(P(x)\\)：由所有類別的加權概率總和給出。 輸出\n使用 \\(P(y=1|x)\\) 作為預測為男的概率，並通過交叉熵損失來進行模型訓練。 代入高斯分布機率密度函數\n條件概率 \\(P(y|x)\\)\n由於 \\(P(x)\\) 是常數，實際上只需要比較 \\(P(x|y)P(y)\\) 即可： $$ P(y=1|x) = \\frac{P(x|y=1)P(y=1)}{P(x|y=1)P(y=1) + P(x|y=0)P(y=0)} $$ 將 \\(P(x|y)\\) 展開為高斯分布： $$ P(x|y=k) = \\frac{1}{(2\\pi)^{n/2} |\\Sigma_k|^{1/2}} \\exp\\left(-\\frac{1}{2}(x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)\\right), \\quad k \\in {0, 1} $$\n預測概率 定義 \\(h(x)\\) 為模型的預測概率： $$ h(x) = P(y=1|x) = f(\\mu,\\Sigma) $$\n換言之我們的模型是一個 \\(f(\\mu,\\Sigma)\\)，以 \\(\\mu\\) 與 \\(\\Sigma\\) 為參數的模型。 損失函數 使用交叉熵損失(BCE, binary crossentropy)： $$ L = -\\frac{1}{m} \\sum_{i=1}^m \\left[ y_i \\log(h(x_i)) + (1 - y_i) \\log(1 - h(x_i)) \\right], $$ 其中：\n\\(h(x_i)\\)：由高斯分布計算出的 \\(P(y=1|x_i)\\)。 \\(y_i\\)：訓練數據的真實標籤。 class GaussianClassifier: def __init__(self, shared_sigma=False): self.shared_sigma = shared_sigma def fit(self, X, y, lr=0.01, epochs=100): # 初始化參數 self.mu_0 = np.mean(X[y==0], axis=0) # class 0 的均值 self.mu_1 = np.mean(X[y==1], axis=0) # class 1 的均值 if self.shared_sigma: # 使用共用的協方差矩陣 self.sigma = np.cov(X.T) else: # 分別計算每個類別的協方差矩陣 self.sigma_0 = np.cov(X[y==0].T) self.sigma_1 = np.cov(X[y==1].T) self.losses = [] # 梯度下降 for _ in tqdm(range(epochs)): # 計算每個類別的概率 if self.shared_sigma: p_0 = multivariate_normal.pdf(X, self.mu_0, self.sigma) p_1 = multivariate_normal.pdf(X, self.mu_1, self.sigma) else: p_0 = multivariate_normal.pdf(X, self.mu_0, self.sigma_0) p_1 = multivariate_normal.pdf(X, self.mu_1, self.sigma_1) # 計算後驗概率 p = p_1 / (p_0 + p_1) # 計算BCE損失 loss = -np.mean(y * np.log(p + 1e-15) + (1-y) * np.log(1 - p + 1e-15)) self.losses.append(loss) # 計算梯度並更新參數 for i in range(len(X)): diff_0 = X[i] - self.mu_0 diff_1 = X[i] - self.mu_1 if self.shared_sigma: inv_sigma = np.linalg.inv(self.sigma) if y[i] == 0: self.mu_0 += lr * inv_sigma.dot(diff_0) self.sigma += lr * (np.outer(diff_0, diff_0).dot(inv_sigma) - inv_sigma) else: self.mu_1 += lr * inv_sigma.dot(diff_1) self.sigma += lr * (np.outer(diff_1, diff_1).dot(inv_sigma) - inv_sigma) # 確保協方差矩陣是正定的 self.sigma = (self.sigma + self.sigma.T) / 2 eigvals = np.linalg.eigvals(self.sigma) if np.any(eigvals \u003c 0): self.sigma += np.eye(2) * (abs(min(eigvals)) + 1e-6) else: if y[i] == 0: inv_sigma_0 = np.linalg.inv(self.sigma_0) self.mu_0 += lr * inv_sigma_0.dot(diff_0) self.sigma_0 += lr * (np.outer(diff_0, diff_0).dot(inv_sigma_0) - inv_sigma_0) self.sigma_0 = (self.sigma_0 + self.sigma_0.T) / 2 else: inv_sigma_1 = np.linalg.inv(self.sigma_1) self.mu_1 += lr * inv_sigma_1.dot(diff_1) self.sigma_1 += lr * (np.outer(diff_1, diff_1).dot(inv_sigma_1) - inv_sigma_1) self.sigma_1 = (self.sigma_1 + self.sigma_1.T) / 2 def predict(self, X): if self.shared_sigma: p_0 = multivariate_normal.pdf(X, self.mu_0, self.sigma) p_1 = multivariate_normal.pdf(X, self.mu_1, self.sigma) else: p_0 = multivariate_normal.pdf(X, self.mu_0, self.sigma_0) p_1 = multivariate_normal.pdf(X, self.mu_1, self.sigma_1) return (p_1 \u003e p_0).astype(int) 我用了兩個策略，一個是各別計算協方差矩陣(左)，一個是共用協方差矩陣(右)。 優缺點： 分離Σ的優勢：\n更好地捕捉每個類別的特徵 可以處理類別有不同形狀的情況 共用Σ的優勢：(boundary 會是線性的)\n參數更少，不容易過擬合 計算更高效 決策邊界更簡單 **摘要：**數據量較小或類別分布相似，建議使用共用Σ的模型；如果數據量大且類別分布差異明顯，可以考慮使用分離Σ的模型。 Sigmoid ? 若所有維度都是非相關的，我們可以使用 Naive Bayes Classifier 則後設計率為 $$ P(A|x)=\\frac{P(x|A)P(A)}{P(x|A)P(A)+P(x|B)P(B)}=\\frac{1}{1+\\frac{P(x|B)P(B)}{P(x|A)P(A)}} $$ 令 \\(z=\\ln\\frac{P(x|A)P(A)}{P(x|B)P(B)}\\)，則 $$ P(A|x)=\\frac{1}{1+exp(-z)}=\\sigma(z) $$ 哈，是 sigmoid 函式！ $$ z=\\ln\\frac{P(x|A)P(A)}{P(x|B)P(B)}=\\ln\\frac{P(x|A)}{P(x|B)}+\\ln\\frac{P(A)}{P(B)} $$ 後面的這一項，可以從訓練資料得到 \\(\\frac{P(A)}{P(B)}=\\frac{N_A}{N_B}\\) $$ P(x|A)=\\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma_A|^{1/2}}exp\\bigg\\lbrace{-\\frac{1}{2}(x-\\mu_A)^T(\\Sigma_A)^{-1}(x-\\mu_A)}\\bigg\\rbrace\\\\ P(x|B)=\\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma_B|^{1/2}}exp\\bigg\\lbrace{-\\frac{1}{2}(x-\\mu_B)^T(\\Sigma_B)^{-1}(x-\\mu_B)}\\bigg\\rbrace $$ $$ z=\\ln\\frac{N_A}{N_B}+\\frac{\\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma_A|^{1/2}}exp\\bigg\\lbrace{-\\frac{1}{2}(x-\\mu_A)^T(\\Sigma_A)^{-1}(x-\\mu_A)}\\bigg\\rbrace}{\\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma_B|^{1/2}}exp\\bigg\\lbrace{-\\frac{1}{2}(x-\\mu_B)^T(\\Sigma_B)^{-1}(x-\\mu_B)}\\bigg\\rbrace} $$ $$ z=\\ln\\frac{N_A}{N_B}+\\ln\\frac{|\\Sigma_B|^{1/2}}{|\\Sigma_A|^{1/2}}exp\\bigg\\lbrace{-\\frac{1}{2}[(x-\\mu_A)^T(\\Sigma_A)^{-1}(x-\\mu_A)-(x-\\mu_B)^T(\\Sigma_B)^{-1}(x-\\mu_B)]}\\bigg\\rbrace $$ $$ z=(\\mu_A-\\mu_B)^T\\Sigma^{-1}\\red{x}-\\frac{1}{2}(\\mu_A)^T(\\Sigma_A)^{-1}\\mu_A+\\frac{1}{2}(\\mu_B)^T(\\Sigma_B)^{-1}\\mu_B+\\ln\\frac{N_A}{N_B} $$ 發現酷東西了： $$ z=w^Tx+b $$ 將 z 代入原式 $$ P(A|x)=\\sigma(z)=\\sigma(w\\cdot x+b) $$ 換言之，我們透過求 \\(N_A, N_B, \\mu_A, \\mu_B, \\Sigma \\) 可以得到 w, b 那我們是不是可以假設 \\(f(y)=\\sigma(wx+b)\\)，直接求 \\(w^* \\)與 \\( b^*\\) Crossentropy? 百努力分布(Bernoulli Distribution)寫成，其中 k = 0 或 1 $$ P(x=k)=p^k\\times(1-p)^{(1-k)} $$ 假設我們的 \\(y\\) 滿足百努力分布，則模型的輸出 \\(\\hat{y}\\) 可視為對 p 的估計，我們可以將損失函數寫成 $$ L(w,b)=f_{w,b}(x_1)f_{w,b}(x_2)(1-f_{w,b}(x_3))\\cdots f_{w,b}(x_n) $$ 同取 ln $$ \\ln L(w,b)=\\sum_{i=1}^n [y_i\\ln f_{w,b}(x_i)+(1-y_i)\\ln(1-f_{w,b}(x_i))] $$ 為了使其最小化，我們取負號，得到交叉熵損失函數： $$ J(w,b)=-\\frac{1}{n}\\sum_{i=1}^n [y_i\\ln f_{w,b}(x_i)+(1-y_i)\\ln(1-f_{w,b}(x_i))] $$ 其中： \\(y_i\\) 是第 i 個樣本的真實標籤（0或1） \\(f_{w,b}(x_i)\\) 是模型對第 i 個樣本的預測值（介於0和1之間） \\(\\frac{1}{n}\\) 是為了取平均 二元分類 v.s. 線性迴歸 $$ \\begin{array}{c|c|c} \u0026\\text{Logistic Regression}\u0026\\text{Linear Regression}\\\\\\hline \\text{function}\u0026f_{w,b}(x)=\\sigma(\\sum_i w_ix_i+b)\u0026f_{w,b}(x)=\\sum_i w_ix_i+b\\\\\\hline \\text{loss function}\u0026L(f)=\\sum_iC(f(x_i),\\hat{y}_i)\u0026L(f)=\\frac{1}{2}\\sum_i(f(x_i)-\\hat{y}_i)^2\\\\\\hline \\text{update}\u0026w_i=w_i-\\eta\\sum_i (f(x_i)-\\hat{y}_i)x_i\u0026w_i=w_i-\\eta\\sum_i (f(x_i)-\\hat{y}_i)x_i\\\\ \\end{array} $$\n可以發現 Linear Regression 與 Logistic Regression 基本上是差不多的作法，差別是 Logistic 使用了 sigmoid function，在 loss function 的部分使用了二元交叉熵。 那為何 Logistic Regression 不使用 MSE 呢？ 用微分會發現 $$ \\frac{\\partial L(f)}{\\partial w_i}=2(f_{w,b}(x)-\\hat{y}f_{w,b}(x)(1-f_{w,b}(x)))x_i $$ \\(\\hat{y}_i=0或1\\)，在 \\(f(x_i)=0或1\\)時，\\(\\frac{\\partial L}{\\partial w_i}皆=0\\)，換言之在接近 target，與相當遠離 target 的地方，梯度都為 0，也就是說使用 MSE 會有梯度遺失的問題。 Discriminative v.s. Generative 我們先前採用高斯模型，並透過找尋最佳的 \\(\\mu\\) 與 \\(\\Sigma\\) 來找到最佳的高斯分布密度方程式的方法稱為 Generative，那既然我們發現其實 $$ z=(\\mu_A-\\mu_B)^T\\Sigma^{-1}\\red{x}-\\frac{1}{2}(\\mu_A)^T(\\Sigma_A)^{-1}\\mu_A+\\frac{1}{2}(\\mu_B)^T(\\Sigma_B)^{-1}\\mu_B+\\ln\\frac{N_A}{N_B} $$ 符合了 \\(z=w^Tx+b\\) 的 pattern，那能否直接代入 wx+b 來求最佳的 \\(w\\) 與 \\(b\\) 呢？答案是可以的，這種方法就稱為 Discriminative 的方法。 那這兩種方法求出來的方程式會一樣嗎？答案是不一樣。主要原因在於這兩種方法的目標函數(objective function)不同： 判別式方法(Discriminative)： 直接學習後驗機率 P(y|x) 目標函數是最大化條件似然(conditional likelihood) 損失函數通常是交叉熵(cross entropy)或負對數似然 只關注如何根據輸入特徵x預測類別y 生成式方法(Generative)： 學習聯合機率分布 P(x,y) = P(x|y)P(y) 目標函數是最大化聯合似然(joint likelihood) 需要同時建模特徵的分布P(x|y)和類別的先驗機率P(y) 不僅要預測y，還要能夠\"生成\"符合該類別的特徵x 一般來說： 當訓練數據充足時，判別式模型表現較好，因為它直接優化分類效果 當訓練數據較少時，生成式模型可能表現更好，因為它對數據分布有更強的假設 需要注意的是，這兩種方法的參數更新規則和收斂性質也會不同，因此即使使用相同的學習率和初始值，最終得到的結果也會有所差異。 Logistic Regression 的限制 並不能表示所有的情況，例如 xor gate。 因為我們無法找到一個線性模型來有效的描述 xor gate。 在這種情況下，我們需要嘗試做 Feature Transformation，將 xor gate 的 input \u0026 output 做特殊的轉換，使線性模型可以描述 事實上這種作法就是在 output layer 前，加一個 hidden layer 作 feature transformation。一個\\(z=\\sigma(wx_i+b)\\) 被我們稱為一個 neural node，由一群 neural node 組成的模型，即稱為 neural network。 ","wordCount":"881","inLanguage":"zh-tw","datePublished":"2024-12-19T15:55:12+08:00","dateModified":"2024-12-19T15:55:12+08:00","author":{"@type":"Person","name":"Rain Hu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://intervalrain.github.io/ai/3_5/"},"publisher":{"@type":"Organization","name":"Rain Hu's Workspace","logo":{"@type":"ImageObject","url":"https://intervalrain.github.io/images/rain.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://intervalrain.github.io/ accesskey=h title="Rain Hu's Workspace (Alt + H)"><img src=https://intervalrain.github.io/images/rain.png alt aria-label=logo height=35>Rain Hu's Workspace</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://intervalrain.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://intervalrain.github.io/aboutme title="About me"><span>About me</span></a></li><li><a href=https://intervalrain.github.io/archives title=Archives><span>Archives</span></a></li><li><a href=https://intervalrain.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://intervalrain.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://intervalrain.github.io/csharp/csharp title=C#><span>C#</span></a></li><li><a href=https://intervalrain.github.io/csindex title=CS><span>CS</span></a></li><li><a href=https://intervalrain.github.io/leetcode title=LeetCode><span>LeetCode</span></a></li><li><a href=https://intervalrain.github.io/ai title=AI><span>AI</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://intervalrain.github.io/>首頁</a></div><h1 class="post-title entry-hint-parent">[AI] 3-5. 邏輯斯迴歸(logistic regression)</h1><div class=post-description>The introduction to logistic regression</div><div class=post-meta><span title='2024-12-19 15:55:12 +0800 +0800'>December 19, 2024</span>&nbsp;·&nbsp;5 分鐘&nbsp;·&nbsp;Rain Hu&nbsp;|&nbsp;<a href=https://github.com/intervalrain/intervalrain.github.io/tree/main/content//AI/3_5.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目錄</span></summary><div class=inner><ul><li><a href=#%e8%b2%9d%e5%bc%8f%e5%ae%9a%e7%90%86 aria-label=貝式定理>貝式定理</a></li><li><a href=#%e9%ab%98%e6%96%af%e5%88%86%e5%b8%83 aria-label=高斯分布>高斯分布</a></li><li><a href=#%e4%ba%8c%e5%85%83%e5%88%86%e9%a1%9e aria-label=二元分類>二元分類</a><ul><li><a href=#%e5%9f%ba%e6%9c%ac%e5%81%87%e8%a8%ad aria-label=基本假設>基本假設</a></li><li><a href=#sigmoid- aria-label="Sigmoid ?">Sigmoid ?</a></li><li><a href=#crossentropy aria-label=Crossentropy?>Crossentropy?</a></li></ul></li><li><a href=#%e4%ba%8c%e5%85%83%e5%88%86%e9%a1%9e-vs-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8 aria-label="二元分類 v.s. 線性迴歸">二元分類 v.s. 線性迴歸</a></li><li><a href=#discriminative-vs-generative aria-label="Discriminative v.s. Generative">Discriminative v.s. Generative</a></li><li><a href=#logistic-regression-%e7%9a%84%e9%99%90%e5%88%b6 aria-label="Logistic Regression 的限制">Logistic Regression 的限制</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=貝式定理>貝式定理<a hidden class=anchor aria-hidden=true href=#貝式定理>#</a></h2><ul><li><p>假設有一個抽獎箱內有紅球與藍球，球上有標示 A 與 B 類別。
$$
\begin{array}{|c|c|}
\hline
A&amp;B\\\hline
\blue{\text{●}}\blue{\text{●}}\blue{\text{●}}\red{\text{●}}&\blue{\text{●}}\blue{\text{●}}\red{\text{●}}\red{\text{●}}\red{\text{●}}\red{\text{●}}\\\hline
\end{array}
$$</p><ul><li>我們抽到藍球，它是來自於 A 的機率為何，即求 \(P(A|\blue{\text{●}})\)？</li></ul></li><li><p>根據貝式定理：
$$
P(A|x)=\frac{P(x|A)P(A)}{P(x|A)P(A)+P(x|B)P(B)}
$$</p><ol><li>先驗機率<ul><li>\(P(A)=\frac{\text{A的球數}}{\text{總球數}}=\frac{4}{10}\)</li><li>\(P(B)=\frac{\text{B的球數}}{\text{總球數}}=\frac{6}{10}\)</li></ul></li><li>條件機率<ul><li>\(P(\blue{\text{●}}|A)=\frac{\text{A中的}\blue{\text{●}}}{\text{A的總球數}}=\frac{3}{4}\)</li><li>\(P(\blue{\text{●}}|B)=\frac{\text{B中的}\blue{\text{●}}}{\text{B的總球數}}=\frac{2}{6}\)
套入公式可得
$$
P(A|\blue{\text{●}})=\frac{P(\blue{\text{●}}|A)P(A)}{P(\blue{\text{●}}|A)P(A)+P(\blue{\text{●}}|B)P(B)}=\frac{3/4\times4/10}{3/4\times4/10+2/6\times6/10}=\frac{3}{5}
$$</li></ul></li></ol><ul><li>假設今天猜中類別才能得獎，已經知道是藍球的情況下，來自 A 的機率是 0.6，來自 B 的機率是 0.4，所以我們理論上會選擇 A，因為機率較大。換言之，在機器學習中，我們判斷一個二元分類的問題，我們會將分類判給機率 > 0.5 的那個類別。</li></ul></li></ul><h2 id=高斯分布>高斯分布<a hidden class=anchor aria-hidden=true href=#高斯分布>#</a></h2><ul><li>一維的高斯分分機率密度函數(probability density function, pdf)為：
$$
f_{\mu,\sigma}(x)=\frac{1}{\sqrt{2\pi\sigma^2}}exp\bigg\lbrace-\frac{(x-\mu)^2}{2\sigma^2}\bigg\rbrace
$$<ul><li>其中<ul><li>\(\mu\) 為平均數(Mean)，決定分布的中心位置。</li><li>\(\sigma\) 為標準差(Standard Deviation)，決定分布的寬度。</li></ul></li></ul></li><li>擴展到 n 維向量的多維高斯分布機率密度函數為：
$$
f_{\mu,\Sigma}(x)=\frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp\bigg\lbrace-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\bigg\rbrace
$$<ul><li><p>其中 x 是一階張量
$$
x = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
$$</p></li><li><p>\(\mu\) 代表 x 在每個維度的均值
$$
\mu = \begin{bmatrix} \mu_1 \\ \mu_2 \\ \vdots \\ \mu_n \end{bmatrix}
$$</p></li><li><p>\(\Sigma\) 是協方差矩陣，\(\Sigma\in\mathbb{R}^{n \times n}\)，表示數據分布的相關性與變異性：
$$
\Sigma = \begin{bmatrix}
\sigma_{11} & \sigma_{12} & \cdots & \sigma_{1n} \\
\sigma_{21} & \sigma_{22} & \cdots & \sigma_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{n1} & \sigma_{n2} & \cdots & \sigma_{nn}
\end{bmatrix}
$$</p></li><li><p>\(|\Sigma|\)是協方差矩陣的行列式，表示協方差矩陣的尺度，用於歸一化分布。</p></li><li><p>\(\Sigma^{-1}\)是協方差矩陣的逆矩陣，用於計算標準化的二次型距離。</p></li><li><p>\((x - \mu)^T \Sigma^{-1} (x - \mu)\)是馬氏距離（Mahalanobis Distance），表示點 \(x\) 與均值 \(\mu\) 的加權距離。</p></li></ul></li></ul><h2 id=二元分類>二元分類<a hidden class=anchor aria-hidden=true href=#二元分類>#</a></h2><ul><li><p>我們嘗試使用高斯分布模型對一個簡單的問題進行描述：</p><ul><li>假設我們想用身高、體重來猜測一個人的性別：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>HeightWeightGenerator</span>:
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 設定男性身高體重的均值和協方差</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>male_mean <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>172</span>, <span style=color:#ae81ff>70</span>])  <span style=color:#75715e># [height, weight]</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>male_cov <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>60</span>, <span style=color:#ae81ff>20</span>],   <span style=color:#75715e># height與weight的協方差矩陣</span>
</span></span><span style=display:flex><span>                             [<span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>35</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 設定女性身高體重的均值和協方差</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>female_mean <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>162</span>, <span style=color:#ae81ff>55</span>])
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>female_cov <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>35</span>, <span style=color:#ae81ff>15</span>],
</span></span><span style=display:flex><span>                              [<span style=color:#ae81ff>15</span>, <span style=color:#ae81ff>20</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_samples</span>(self, n_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 生成性別 (0: 女性, 1: 男性)</span>
</span></span><span style=display:flex><span>    genders <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>binomial(n<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, size<span style=color:#f92672>=</span>n_samples)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 初始化數據陣列</span>
</span></span><span style=display:flex><span>    data <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((n_samples, <span style=color:#ae81ff>2</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 生成男性和女性的身高體重數據</span>
</span></span><span style=display:flex><span>    male_indices <span style=color:#f92672>=</span> genders <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    female_indices <span style=color:#f92672>=</span> genders <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 使用多維高斯分布生成數據</span>
</span></span><span style=display:flex><span>    data[male_indices] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>multivariate_normal(
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>male_mean, self<span style=color:#f92672>.</span>male_cov, size<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>sum(male_indices))
</span></span><span style=display:flex><span>    data[female_indices] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>multivariate_normal(
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>female_mean, self<span style=color:#f92672>.</span>female_cov, size<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>sum(female_indices))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> data, genders
</span></span></code></pre></div><p><img alt=distribution loading=lazy src=/ai/AI/3_5/distribution.png></p><ul><li><strong>假設</strong><ul><li>\(x = [\text{身高}, \text{體重}]^T \in \mathbb{R}^2\)：每個數據點的特徵向量。</li><li>類別 \(y \in {0,要 1}\)：性別標籤，0 表示女，1 表示男。</li></ul></li></ul><h3 id=基本假設>基本假設<a hidden class=anchor aria-hidden=true href=#基本假設>#</a></h3><ul><li><p><strong>高斯分布假設</strong></p><ul><li>\(P(x|y=0) \sim \mathcal{N}(\mu_0, \Sigma_0)\)：女生特徵的高斯分布。</li><li>\(P(x|y=1) \sim \mathcal{N}(\mu_1, \Sigma_1)\)：男生特徵的高斯分布。</li></ul></li><li><p><strong>目標</strong></p><ul><li>利用貝氏定理計算 \(P(y|x)\)：
$$
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
$$</li><li>其中：<ul><li>\(P(x|y)\)：由高斯分布表示。</li><li>\(P(y)\)：類別的先驗概率（例如， \(P(y=1)\) 和 \(P(y=0)\) 可以從訓練數據中估計）。</li><li>\(P(x)\)：由所有類別的加權概率總和給出。</li></ul></li></ul></li><li><p><strong>輸出</strong></p><ul><li>使用 \(P(y=1|x)\) 作為預測為男的概率，並通過交叉熵損失來進行模型訓練。</li></ul></li><li><p><strong>代入高斯分布機率密度函數</strong></p><ol><li><strong>條件概率 \(P(y|x)\)</strong><br>由於 \(P(x)\) 是常數，實際上只需要比較 \(P(x|y)P(y)\) 即可：
$$
P(y=1|x) = \frac{P(x|y=1)P(y=1)}{P(x|y=1)P(y=1) + P(x|y=0)P(y=0)}
$$</li></ol><p>將 \(P(x|y)\) 展開為高斯分布：
$$
P(x|y=k) = \frac{1}{(2\pi)^{n/2} |\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)\right), \quad k \in {0, 1}
$$</p><ol start=2><li><p><strong>預測概率</strong>
定義 \(h(x)\) 為模型的預測概率：
$$
h(x) = P(y=1|x) = f(\mu,\Sigma)
$$</p><ul><li>換言之我們的模型是一個 \(f(\mu,\Sigma)\)，以 \(\mu\) 與 \(\Sigma\) 為參數的模型。</li></ul></li><li><p><strong>損失函數</strong>
使用交叉熵損失(BCE, binary crossentropy)：
$$
L = -\frac{1}{m} \sum_{i=1}^m \left[ y_i \log(h(x_i)) + (1 - y_i) \log(1 - h(x_i)) \right],
$$
其中：</p><ul><li>\(h(x_i)\)：由高斯分布計算出的 \(P(y=1|x_i)\)。</li><li>\(y_i\)：訓練數據的真實標籤。</li></ul></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>GaussianClassifier</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, shared_sigma<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>shared_sigma <span style=color:#f92672>=</span> shared_sigma
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, X, y, lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 初始化參數</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>mu_0 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean(X[y<span style=color:#f92672>==</span><span style=color:#ae81ff>0</span>], axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)  <span style=color:#75715e># class 0 的均值</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>mu_1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean(X[y<span style=color:#f92672>==</span><span style=color:#ae81ff>1</span>], axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)  <span style=color:#75715e># class 1 的均值</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>shared_sigma:
</span></span><span style=display:flex><span>            <span style=color:#75715e># 使用共用的協方差矩陣</span>
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>sigma <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>cov(X<span style=color:#f92672>.</span>T)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            <span style=color:#75715e># 分別計算每個類別的協方差矩陣</span>
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>sigma_0 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>cov(X[y<span style=color:#f92672>==</span><span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>T)
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>sigma_1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>cov(X[y<span style=color:#f92672>==</span><span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>T)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>losses <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 梯度下降</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> tqdm(range(epochs)):
</span></span><span style=display:flex><span>            <span style=color:#75715e># 計算每個類別的概率</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>shared_sigma:
</span></span><span style=display:flex><span>                p_0 <span style=color:#f92672>=</span> multivariate_normal<span style=color:#f92672>.</span>pdf(X, self<span style=color:#f92672>.</span>mu_0, self<span style=color:#f92672>.</span>sigma)
</span></span><span style=display:flex><span>                p_1 <span style=color:#f92672>=</span> multivariate_normal<span style=color:#f92672>.</span>pdf(X, self<span style=color:#f92672>.</span>mu_1, self<span style=color:#f92672>.</span>sigma)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                p_0 <span style=color:#f92672>=</span> multivariate_normal<span style=color:#f92672>.</span>pdf(X, self<span style=color:#f92672>.</span>mu_0, self<span style=color:#f92672>.</span>sigma_0)
</span></span><span style=display:flex><span>                p_1 <span style=color:#f92672>=</span> multivariate_normal<span style=color:#f92672>.</span>pdf(X, self<span style=color:#f92672>.</span>mu_1, self<span style=color:#f92672>.</span>sigma_1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 計算後驗概率</span>
</span></span><span style=display:flex><span>            p <span style=color:#f92672>=</span> p_1 <span style=color:#f92672>/</span> (p_0 <span style=color:#f92672>+</span> p_1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 計算BCE損失</span>
</span></span><span style=display:flex><span>            loss <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>np<span style=color:#f92672>.</span>mean(y <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>log(p <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-15</span>) <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span><span style=color:#f92672>-</span>y) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>log(<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> p <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-15</span>))
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>losses<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 計算梯度並更新參數</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(X)):
</span></span><span style=display:flex><span>                diff_0 <span style=color:#f92672>=</span> X[i] <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>mu_0
</span></span><span style=display:flex><span>                diff_1 <span style=color:#f92672>=</span> X[i] <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>mu_1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>shared_sigma:
</span></span><span style=display:flex><span>                    inv_sigma <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(self<span style=color:#f92672>.</span>sigma)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>if</span> y[i] <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                        self<span style=color:#f92672>.</span>mu_0 <span style=color:#f92672>+=</span> lr <span style=color:#f92672>*</span> inv_sigma<span style=color:#f92672>.</span>dot(diff_0)
</span></span><span style=display:flex><span>                        self<span style=color:#f92672>.</span>sigma <span style=color:#f92672>+=</span> lr <span style=color:#f92672>*</span> (np<span style=color:#f92672>.</span>outer(diff_0, diff_0)<span style=color:#f92672>.</span>dot(inv_sigma) <span style=color:#f92672>-</span> inv_sigma)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                        self<span style=color:#f92672>.</span>mu_1 <span style=color:#f92672>+=</span> lr <span style=color:#f92672>*</span> inv_sigma<span style=color:#f92672>.</span>dot(diff_1)
</span></span><span style=display:flex><span>                        self<span style=color:#f92672>.</span>sigma <span style=color:#f92672>+=</span> lr <span style=color:#f92672>*</span> (np<span style=color:#f92672>.</span>outer(diff_1, diff_1)<span style=color:#f92672>.</span>dot(inv_sigma) <span style=color:#f92672>-</span> inv_sigma)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                    <span style=color:#75715e># 確保協方差矩陣是正定的</span>
</span></span><span style=display:flex><span>                    self<span style=color:#f92672>.</span>sigma <span style=color:#f92672>=</span> (self<span style=color:#f92672>.</span>sigma <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>sigma<span style=color:#f92672>.</span>T) <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>                    eigvals <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>eigvals(self<span style=color:#f92672>.</span>sigma)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>if</span> np<span style=color:#f92672>.</span>any(eigvals <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0</span>):
</span></span><span style=display:flex><span>                        self<span style=color:#f92672>.</span>sigma <span style=color:#f92672>+=</span> np<span style=color:#f92672>.</span>eye(<span style=color:#ae81ff>2</span>) <span style=color:#f92672>*</span> (abs(min(eigvals)) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-6</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>if</span> y[i] <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                        inv_sigma_0 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(self<span style=color:#f92672>.</span>sigma_0)
</span></span><span style=display:flex><span>                        self<span style=color:#f92672>.</span>mu_0 <span style=color:#f92672>+=</span> lr <span style=color:#f92672>*</span> inv_sigma_0<span style=color:#f92672>.</span>dot(diff_0)
</span></span><span style=display:flex><span>                        self<span style=color:#f92672>.</span>sigma_0 <span style=color:#f92672>+=</span> lr <span style=color:#f92672>*</span> (np<span style=color:#f92672>.</span>outer(diff_0, diff_0)<span style=color:#f92672>.</span>dot(inv_sigma_0) <span style=color:#f92672>-</span> inv_sigma_0)
</span></span><span style=display:flex><span>                        self<span style=color:#f92672>.</span>sigma_0 <span style=color:#f92672>=</span> (self<span style=color:#f92672>.</span>sigma_0 <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>sigma_0<span style=color:#f92672>.</span>T) <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                        inv_sigma_1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(self<span style=color:#f92672>.</span>sigma_1)
</span></span><span style=display:flex><span>                        self<span style=color:#f92672>.</span>mu_1 <span style=color:#f92672>+=</span> lr <span style=color:#f92672>*</span> inv_sigma_1<span style=color:#f92672>.</span>dot(diff_1)
</span></span><span style=display:flex><span>                        self<span style=color:#f92672>.</span>sigma_1 <span style=color:#f92672>+=</span> lr <span style=color:#f92672>*</span> (np<span style=color:#f92672>.</span>outer(diff_1, diff_1)<span style=color:#f92672>.</span>dot(inv_sigma_1) <span style=color:#f92672>-</span> inv_sigma_1)
</span></span><span style=display:flex><span>                        self<span style=color:#f92672>.</span>sigma_1 <span style=color:#f92672>=</span> (self<span style=color:#f92672>.</span>sigma_1 <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>sigma_1<span style=color:#f92672>.</span>T) <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(self, X):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>shared_sigma:
</span></span><span style=display:flex><span>            p_0 <span style=color:#f92672>=</span> multivariate_normal<span style=color:#f92672>.</span>pdf(X, self<span style=color:#f92672>.</span>mu_0, self<span style=color:#f92672>.</span>sigma)
</span></span><span style=display:flex><span>            p_1 <span style=color:#f92672>=</span> multivariate_normal<span style=color:#f92672>.</span>pdf(X, self<span style=color:#f92672>.</span>mu_1, self<span style=color:#f92672>.</span>sigma)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            p_0 <span style=color:#f92672>=</span> multivariate_normal<span style=color:#f92672>.</span>pdf(X, self<span style=color:#f92672>.</span>mu_0, self<span style=color:#f92672>.</span>sigma_0)
</span></span><span style=display:flex><span>            p_1 <span style=color:#f92672>=</span> multivariate_normal<span style=color:#f92672>.</span>pdf(X, self<span style=color:#f92672>.</span>mu_1, self<span style=color:#f92672>.</span>sigma_1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> (p_1 <span style=color:#f92672>&gt;</span> p_0)<span style=color:#f92672>.</span>astype(int)
</span></span></code></pre></div><ul><li>我用了兩個策略，一個是各別計算協方差矩陣(左)，一個是共用協方差矩陣(右)。
<img alt=cov loading=lazy src=/ai/AI/3_5/covariance.png></li><li>優缺點：<ul><li><p>分離Σ的優勢：</p><ul><li>更好地捕捉每個類別的特徵</li><li>可以處理類別有不同形狀的情況</li></ul></li><li><p>共用Σ的優勢：(boundary 會是線性的)</p><ul><li>參數更少，不容易過擬合</li><li>計算更高效</li><li>決策邊界更簡單</li></ul></li></ul></li><li>**摘要：**數據量較小或類別分布相似，建議使用共用Σ的模型；如果數據量大且類別分布差異明顯，可以考慮使用分離Σ的模型。</li></ul></li></ul><h3 id=sigmoid->Sigmoid ?<a hidden class=anchor aria-hidden=true href=#sigmoid->#</a></h3><ul><li>若所有維度都是非相關的，我們可以使用 Naive Bayes Classifier</li><li>則後設計率為
$$
P(A|x)=\frac{P(x|A)P(A)}{P(x|A)P(A)+P(x|B)P(B)}=\frac{1}{1+\frac{P(x|B)P(B)}{P(x|A)P(A)}}
$$</li><li>令 \(z=\ln\frac{P(x|A)P(A)}{P(x|B)P(B)}\)，則
$$
P(A|x)=\frac{1}{1+exp(-z)}=\sigma(z)
$$</li><li>哈，是 sigmoid 函式！
$$
z=\ln\frac{P(x|A)P(A)}{P(x|B)P(B)}=\ln\frac{P(x|A)}{P(x|B)}+\ln\frac{P(A)}{P(B)}
$$</li><li>後面的這一項，可以從訓練資料得到 \(\frac{P(A)}{P(B)}=\frac{N_A}{N_B}\)
$$
P(x|A)=\frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma_A|^{1/2}}exp\bigg\lbrace{-\frac{1}{2}(x-\mu_A)^T(\Sigma_A)^{-1}(x-\mu_A)}\bigg\rbrace\\
P(x|B)=\frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma_B|^{1/2}}exp\bigg\lbrace{-\frac{1}{2}(x-\mu_B)^T(\Sigma_B)^{-1}(x-\mu_B)}\bigg\rbrace
$$
$$
z=\ln\frac{N_A}{N_B}+\frac{\frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma_A|^{1/2}}exp\bigg\lbrace{-\frac{1}{2}(x-\mu_A)^T(\Sigma_A)^{-1}(x-\mu_A)}\bigg\rbrace}{\frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma_B|^{1/2}}exp\bigg\lbrace{-\frac{1}{2}(x-\mu_B)^T(\Sigma_B)^{-1}(x-\mu_B)}\bigg\rbrace}
$$
$$
z=\ln\frac{N_A}{N_B}+\ln\frac{|\Sigma_B|^{1/2}}{|\Sigma_A|^{1/2}}exp\bigg\lbrace{-\frac{1}{2}[(x-\mu_A)^T(\Sigma_A)^{-1}(x-\mu_A)-(x-\mu_B)^T(\Sigma_B)^{-1}(x-\mu_B)]}\bigg\rbrace
$$
$$
z=(\mu_A-\mu_B)^T\Sigma^{-1}\red{x}-\frac{1}{2}(\mu_A)^T(\Sigma_A)^{-1}\mu_A+\frac{1}{2}(\mu_B)^T(\Sigma_B)^{-1}\mu_B+\ln\frac{N_A}{N_B}
$$</li><li>發現酷東西了：
$$
z=w^Tx+b
$$</li><li>將 z 代入原式
$$
P(A|x)=\sigma(z)=\sigma(w\cdot x+b)
$$</li><li>換言之，我們透過求 \(N_A, N_B, \mu_A, \mu_B, \Sigma \) 可以得到 w, b</li><li>那我們是不是可以假設 \(f(y)=\sigma(wx+b)\)，直接求 \(w^* \)與 \( b^*\)</li></ul><h3 id=crossentropy>Crossentropy?<a hidden class=anchor aria-hidden=true href=#crossentropy>#</a></h3><ul><li>百努力分布(Bernoulli Distribution)寫成，其中 k = 0 或 1
$$
P(x=k)=p^k\times(1-p)^{(1-k)}
$$</li><li>假設我們的 \(y\) 滿足百努力分布，則模型的輸出 \(\hat{y}\) 可視為對 p 的估計，我們可以將損失函數寫成
$$
L(w,b)=f_{w,b}(x_1)f_{w,b}(x_2)(1-f_{w,b}(x_3))\cdots f_{w,b}(x_n)
$$</li><li>同取 ln
$$
\ln L(w,b)=\sum_{i=1}^n [y_i\ln f_{w,b}(x_i)+(1-y_i)\ln(1-f_{w,b}(x_i))]
$$</li><li>為了使其最小化，我們取負號，得到交叉熵損失函數：
$$
J(w,b)=-\frac{1}{n}\sum_{i=1}^n [y_i\ln f_{w,b}(x_i)+(1-y_i)\ln(1-f_{w,b}(x_i))]
$$</li><li>其中：<ul><li>\(y_i\) 是第 i 個樣本的真實標籤（0或1）</li><li>\(f_{w,b}(x_i)\) 是模型對第 i 個樣本的預測值（介於0和1之間）</li><li>\(\frac{1}{n}\) 是為了取平均</li></ul></li></ul></li></ul><h2 id=二元分類-vs-線性迴歸>二元分類 v.s. 線性迴歸<a hidden class=anchor aria-hidden=true href=#二元分類-vs-線性迴歸>#</a></h2><p>$$
\begin{array}{c|c|c}
&\text{Logistic Regression}&\text{Linear Regression}\\\hline
\text{function}&amp;f_{w,b}(x)=\sigma(\sum_i w_ix_i+b)&amp;f_{w,b}(x)=\sum_i w_ix_i+b\\\hline
\text{loss function}&amp;L(f)=\sum_iC(f(x_i),\hat{y}_i)&amp;L(f)=\frac{1}{2}\sum_i(f(x_i)-\hat{y}_i)^2\\\hline
\text{update}&amp;w_i=w_i-\eta\sum_i (f(x_i)-\hat{y}_i)x_i&amp;w_i=w_i-\eta\sum_i (f(x_i)-\hat{y}_i)x_i\\
\end{array}
$$</p><ul><li>可以發現 Linear Regression 與 Logistic Regression 基本上是差不多的作法，差別是 Logistic 使用了 sigmoid function，在 loss function 的部分使用了二元交叉熵。</li><li>那為何 Logistic Regression 不使用 MSE 呢？ 用微分會發現
$$
\frac{\partial L(f)}{\partial w_i}=2(f_{w,b}(x)-\hat{y}f_{w,b}(x)(1-f_{w,b}(x)))x_i
$$</li><li>\(\hat{y}_i=0或1\)，在 \(f(x_i)=0或1\)時，\(\frac{\partial L}{\partial w_i}皆=0\)，換言之在接近 target，與相當遠離 target 的地方，梯度都為 0，也就是說使用 MSE 會有梯度遺失的問題。
<img alt="mse v.s. ce" loading=lazy src=https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iO6LSBu3kRYJeOFO7_Rd2A.gif></li></ul><h2 id=discriminative-vs-generative>Discriminative v.s. Generative<a hidden class=anchor aria-hidden=true href=#discriminative-vs-generative>#</a></h2><ul><li>我們先前採用高斯模型，並透過找尋最佳的 \(\mu\) 與 \(\Sigma\) 來找到最佳的高斯分布密度方程式的方法稱為 <strong>Generative</strong>，那既然我們發現其實
$$
z=(\mu_A-\mu_B)^T\Sigma^{-1}\red{x}-\frac{1}{2}(\mu_A)^T(\Sigma_A)^{-1}\mu_A+\frac{1}{2}(\mu_B)^T(\Sigma_B)^{-1}\mu_B+\ln\frac{N_A}{N_B}
$$
符合了 \(z=w^Tx+b\) 的 pattern，那能否直接代入 <code>wx+b</code> 來求最佳的 \(w\) 與 \(b\) 呢？答案是可以的，這種方法就稱為 Discriminative 的方法。</li><li>那這兩種方法求出來的方程式會一樣嗎？答案是不一樣。主要原因在於這兩種方法的目標函數(objective function)不同：<ul><li>判別式方法(Discriminative)：<ul><li>直接學習後驗機率 P(y|x)</li><li>目標函數是最大化條件似然(conditional likelihood)</li><li>損失函數通常是交叉熵(cross entropy)或負對數似然</li><li>只關注如何根據輸入特徵x預測類別y</li></ul></li><li>生成式方法(Generative)：<ul><li>學習聯合機率分布 P(x,y) = P(x|y)P(y)</li><li>目標函數是最大化聯合似然(joint likelihood)</li><li>需要同時建模特徵的分布P(x|y)和類別的先驗機率P(y)</li><li>不僅要預測y，還要能夠"生成"符合該類別的特徵x</li></ul></li></ul></li><li>一般來說：<ul><li>當訓練數據充足時，判別式模型表現較好，因為它直接優化分類效果</li><li>當訓練數據較少時，生成式模型可能表現更好，因為它對數據分布有更強的假設</li><li>需要注意的是，這兩種方法的參數更新規則和收斂性質也會不同，因此即使使用相同的學習率和初始值，最終得到的結果也會有所差異。</li></ul></li></ul><h2 id=logistic-regression-的限制>Logistic Regression 的限制<a hidden class=anchor aria-hidden=true href=#logistic-regression-的限制>#</a></h2><ul><li>並不能表示所有的情況，例如 xor gate。
<img alt=xorgate loading=lazy src=/ai/AI/3_5/xor.png></li><li>因為我們無法找到一個線性模型來有效的描述 xor gate。</li><li>在這種情況下，我們需要嘗試做 <strong>Feature Transformation</strong>，將 xor gate 的 input & output 做特殊的轉換，使線性模型可以描述
<img alt=feature_transformation loading=lazy src=/ai/AI/3_5/feature_transformation.png></li><li>事實上這種作法就是在 output layer 前，加一個 <strong>hidden layer</strong> 作 feature transformation。一個\(z=\sigma(wx_i+b)\) 被我們稱為一個 neural node，由一群 neural node 組成的模型，即稱為 <strong>neural network</strong>。</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://intervalrain.github.io/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://intervalrain.github.io/ai/3_4/><span class=title>« 上一頁</span><br><span>[AI] 3-4. 線性迴歸</span>
</a><a class=next href=https://intervalrain.github.io/ai/3_6/><span class=title>下一頁 »</span><br><span>[AI] 3-6. 實作線性分類器</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://intervalrain.github.io/>Rain Hu's Workspace</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="複製";function s(){t.innerHTML="已複製！",setTimeout(()=>{t.innerHTML="複製"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>