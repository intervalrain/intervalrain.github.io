<!doctype html><html lang=zh-tw dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[AI] 二元分類問題 | Rain Hu's Workspace</title><meta name=keywords content="AI"><meta name=description content="Binary categorical classification problem"><meta name=author content="Rain Hu"><link rel=canonical href=https://intervalrain.github.io/><meta name=google-site-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0cefe5a1d95e3d0f0cce057d37c60cd238d1a4af825090f831a18f21671f621d.css integrity="sha256-DO/lodlePQ8MzgV9N8YM0jjRpK+CUJD4MaGPIWcfYh0=" rel="preload stylesheet" as=style><link rel=icon href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=16x16 href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=32x32 href=https://intervalrain.github.io/images/rain.png><link rel=apple-touch-icon href=https://intervalrain.github.io/images/rain.png><link rel=mask-icon href=https://intervalrain.github.io/images/rain.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh-tw href=https://intervalrain.github.io/ai/4_1/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css integrity=sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js integrity=sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script><script src=https://utteranc.es/client.js repo=intervalrain.github.io issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script><meta property="og:url" content="https://intervalrain.github.io/ai/4_1/"><meta property="og:site_name" content="Rain Hu's Workspace"><meta property="og:title" content="[AI] 二元分類問題"><meta property="og:description" content="Binary categorical classification problem"><meta property="og:locale" content="zh-tw"><meta property="og:type" content="article"><meta property="article:section" content="ai"><meta property="article:published_time" content="2025-01-02T12:50:29+08:00"><meta property="article:modified_time" content="2025-01-02T12:50:29+08:00"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="[AI] 二元分類問題"><meta name=twitter:description content="Binary categorical classification problem"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"[AI] 二元分類問題","item":"https://intervalrain.github.io/ai/4_1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[AI] 二元分類問題","name":"[AI] 二元分類問題","description":"Binary categorical classification problem","keywords":["AI"],"articleBody":"1. 認識 IMDB 資料集 透過 tensorflow 引入資料集 參數 path: 資料的快取位置（相對於 ~/.keras/dataset）。 num_words: 整數或 None。單詞按其出現頻率（在訓練集中）進行排序，並且僅保留 num_words 個最常出现的单词。任何較不常出现的单词在序列資料中都將顯示為 oov_char 值。如果為 None，則保留所有單詞。預設值為 None。 skip_top: 跳過出現頻率最高的前 N 個單詞（這些單詞可能沒有參考價值）。這些單詞在資料+ 集中將顯示為 oov_char 值。當為 0 時，則不跳過任何單詞。預設值為 0。 maxlen: 整數或 None。最大序列長度。任何較長的序列都將被截斷。None 表示不進行截斷。預設值為 None。 seed: 整數。用於可重現資料洗牌的種子。 start_char: 整數。序列的開頭將標記為此字元。0 通常是填充字元。預設值為 1。 oov_char: 整數。詞彙表外字元。由於 num_words 或 skip_top 限制而被刪除的詞彙將替換為此字元。 index_from: 整數。使用此索引和更高索引為實際詞彙編制索引。 回傳值 Numpy 陣列的 tuple: (x_train, y_train), (x_test, y_test)。 其中資料集包含了 88585 個相異單字，有很大的單字甚至只有出現一次，這數字對訓練而言非常龐大，且對分類任務沒什麼幫助，所以我們只保留 10000 個最常出現的單字 from tensorflow.keras.datasets import imdb (train_images, train_labels), (test_images, test_labels) = imdb.load_data(num_words=10000) IMDb (Internet Movie Database) 是一個與影視相關的網路資料庫，IMDB 資料集就是從 IMDb 網站收集而來的資料。資料包含了 50000 筆評論，其中包含了 50% 的正面評論與 50% 的負面評論。 print(train_data.shape) print(train_labels.shape) print(test_data.shape) print(test_labels.shape) \u003e (25000,) \u003e (25000,) \u003e (25000,) \u003e (25000,) 資料的組成是由一連串的數字所組成，如：\n\\( \\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|} \\hline \\text{(保留)} \u0026 \\text{the} \u0026 \\text{and} \u0026 \\text{a} \u0026 \\text{…} \u0026 \\text{in} \u0026 \\text{…} \u0026 \\text{wonderful} \u0026 \\text{…} \u0026 \\text{morning} \u0026 \\text{…} \\\\ \\hline 0 \u0026 1 \u0026 2 \u0026 3 \u0026 … \u0026 8 \u0026 … \u0026 386 \u0026 … \u0026 1969 \u0026 …\\\\ \\hline \\end{array} \\) 每個數字代表一個單字，編號愈前面代表愈常用。 print(train_data[0]) \u003e [1, 14, 22, 16, 43, 530, 973, 1622, ..., 32] 我們可以用 imdb.get_word_index(path=\"imdb_word_index.json\") 來得到這個字典，並試著還原原始評論。 注意按引 0~2 為留保字，故索引值需位移 3。 dict = imdb.get_word_index(path=\"imdb_word_index.json\") index_to_word = {value: key for key, value in dict.items()} sentence_0 = ' '.join([index_to_word.get(idx, '?') for idx in train_data[0]]) print(sentence_0) \u003e ? this film was just brilliant casting location scenery story direction labels 是由 1, 0 組成的陣列，代表正評(1)或負評(0) print(train_labels[:10]) \u003e array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0]) 2. 準備資料 由於目前的測試資料的長度都不一樣長，我們需要預先做整型，方法有二： 填補資料中每個串列，使長度相同。 做 mutli-hot(k-hot) 轉換。\n\\( \\begin{array}{|c|} \\hline \\text{column}\\\\ \\hline \\text{A}\\\\ \\hline \\text{B}\\\\ \\hline \\text{C}\\\\ \\hline \\text{A}\\\\ \\hline \\text{A}\\\\\\hline \\end{array} \\rightarrow \\begin{array}{|c|c|c|} \\hline \\text{A} \u0026 \\text{B} \u0026 \\text{C}\\\\ \\hline 1 \u0026 0 \u0026 0\\\\ \\hline 0 \u0026 1 \u0026 0\\\\ \\hline 0 \u0026 0 \u0026 1\\\\ \\hline 1 \u0026 0 \u0026 0\\\\ \\hline 1 \u0026 0 \u0026 0\\\\\\hline \\end{array} \\) 將測試資料轉成 multi-hot 型式： import numpy as np def vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results x_train = vectorize_sequences(train_data) x_test = vectorize_sequences(test_data) 將 labels 也轉成向量資料 y_train = np.asarray(train_labels).astype('float32') y_test = np.asarray(test_labels).astype('float32') 3. 建立神經網路 我們的輸入資料是向量、標籤為 0 與 1，我們可以使用一個密集層堆疊架構搭配 relu 函數。\n我設計以下的三層神經網路架構：\n\\( \\begin{array}{ccc} \\text{輸入(向量化文字)}\\\\ \\downarrow\\\\ \\boxed{\\text{密集(單元=16)}} \u0026 \\text{relu} \u0026 \\text{hidden layer}\\\\ \\downarrow\\\\ \\boxed{\\text{密集(單元=16)}} \u0026 \\text{relu} \u0026 \\text{hidden layer}\\\\ \\downarrow\\\\ \\boxed{\\text{密集(單元=1)}} \u0026 \\text{softmax} \u0026 \\text{output layer}\\\\ \\downarrow\\\\ \\text{輸出(預測值)}\\\\ \\end{array} \\)\nfrom tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ layers.Dense(16, activation='relu'), layers.Dense(16, activation='relu'), layers.Dense(1, activation='sigmoid') ]) compile model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) 建立驗證集 x_val = x_train[:10000] partial_x_train = x_train[10000:] y_val = y_train[:10000] partial_y_train = y_train[10000:] 訓練模型 history = model.fit(partial_x_train, partial_y_train, epochs=20 batch_size=512, validation_data=(x_val, y_val)) 繪製訓練與驗證圖 import matplotlib.pyplot as plt history_dict = history.history loss_values = history_dict['loss'] val_loss_values = history_dict['val_loss'] epochs = range(1, len(loss_values) + 1) plt.plot(epochs, loss_values, 'bo', label='Training loss') plt.plot(epochs, val_loss_values, 'b', label='Validation loss') plt.title('Training and validation loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend() plt.show() 繪製訓練和驗證準確度 plt.clf() acc = history_dict['accuracy'] val_acc = history_dict['val_accuracy'] plt.plot(epochs, acc, 'bo', label='Training accuracy') plt.plot(epochs, val_acc, 'b', label='Validation accuracy') plt.title('Training and validation accuracy') plt.xlabel('Epochs') plt.ylabel('Accuracy') plt.legend() 從結果可見，訓練集的損失隨訓練週期增加而減少，準確度隨週期增加而增加，但是對驗證集而言，卻沒有得到一樣的成效，這就是 過度配適(overfitting) 的現象。 我們可以使用其它技術來避免 overfitting 的發生，如 early stop、drop out from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.callbacks import EarlyStopping model = keras.Sequential([ layers.Dense(16, activation='relu'), layers.Dropout(0.5), layers.Dense(16, activation='relu'), layers.Dropout(0,5), layers.Dense(1, activation='sigmoid') ]) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) early_stopping = EarlyStopping( monitor='val_loss', patience=4, # 連續4個epoch沒改善就停止 restore_best_weights=True # 回復最佳權重 ) x_val = x_train[:10000] partial_x_train = x_train[10000:] y_val = y_train[:10000] partial_y_train = y_train[10000:] history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val), callbacks=[early_stopping]) 拿來測試驗證集 predictions = model.predict(x_test, batch_size=128) test_loss, test_acc = model.evaluate(x_test, y_test) print(f'test_acc: {test_acc}') \u003e test_acc: 0.8849200010299683 ","wordCount":"549","inLanguage":"zh-tw","datePublished":"2025-01-02T12:50:29+08:00","dateModified":"2025-01-02T12:50:29+08:00","author":{"@type":"Person","name":"Rain Hu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://intervalrain.github.io/ai/4_1/"},"publisher":{"@type":"Organization","name":"Rain Hu's Workspace","logo":{"@type":"ImageObject","url":"https://intervalrain.github.io/images/rain.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://intervalrain.github.io/ accesskey=h title="Rain Hu's Workspace (Alt + H)"><img src=https://intervalrain.github.io/images/rain.png alt aria-label=logo height=35>Rain Hu's Workspace</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://intervalrain.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://intervalrain.github.io/aboutme title="About me"><span>About me</span></a></li><li><a href=https://intervalrain.github.io/archives title=Archives><span>Archives</span></a></li><li><a href=https://intervalrain.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://intervalrain.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://intervalrain.github.io/csharp/csharp title=C#><span>C#</span></a></li><li><a href=https://intervalrain.github.io/csindex title=CS><span>CS</span></a></li><li><a href=https://intervalrain.github.io/leetcode title=LeetCode><span>LeetCode</span></a></li><li><a href=https://intervalrain.github.io/ai title=AI><span>AI</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://intervalrain.github.io/>首頁</a></div><h1 class="post-title entry-hint-parent">[AI] 二元分類問題</h1><div class=post-description>Binary categorical classification problem</div><div class=post-meta><span title='2025-01-02 12:50:29 +0800 +0800'>January 2, 2025</span>&nbsp;·&nbsp;3 分鐘&nbsp;·&nbsp;Rain Hu&nbsp;|&nbsp;<a href=https://github.com/intervalrain/intervalrain.github.io/tree/main/content//AI/4_1.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目錄</span></summary><div class=inner><ul><li><a href=#1-%e8%aa%8d%e8%ad%98-imdb-%e8%b3%87%e6%96%99%e9%9b%86 aria-label="1. 認識 IMDB 資料集">1. 認識 IMDB 資料集</a></li><li><a href=#2-%e6%ba%96%e5%82%99%e8%b3%87%e6%96%99 aria-label="2. 準備資料">2. 準備資料</a></li><li><a href=#3-%e5%bb%ba%e7%ab%8b%e7%a5%9e%e7%b6%93%e7%b6%b2%e8%b7%af aria-label="3. 建立神經網路">3. 建立神經網路</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=1-認識-imdb-資料集>1. 認識 IMDB 資料集<a hidden class=anchor aria-hidden=true href=#1-認識-imdb-資料集>#</a></h2><ul><li>透過 tensorflow 引入資料集<ul><li><strong>參數</strong><ul><li><code>path</code>: 資料的快取位置（相對於 ~/.keras/dataset）。</li><li><code>num_words</code>: 整數或 None。單詞按其出現頻率（在訓練集中）進行排序，並且僅保留 num_words 個最常出现的单词。任何較不常出现的单词在序列資料中都將顯示為 oov_char 值。如果為 None，則保留所有單詞。預設值為 None。</li><li><code>skip_top</code>: 跳過出現頻率最高的前 N 個單詞（這些單詞可能沒有參考價值）。這些單詞在資料+ 集中將顯示為 oov_char 值。當為 0 時，則不跳過任何單詞。預設值為 0。</li><li><code>maxlen</code>: 整數或 None。最大序列長度。任何較長的序列都將被截斷。None 表示不進行截斷。預設值為 None。</li><li><code>seed</code>: 整數。用於可重現資料洗牌的種子。</li><li><code>start_char</code>: 整數。序列的開頭將標記為此字元。0 通常是填充字元。預設值為 1。</li><li><code>oov_char</code>: 整數。詞彙表外字元。由於 num_words 或 skip_top 限制而被刪除的詞彙將替換為此字元。</li><li><code>index_from</code>: 整數。使用此索引和更高索引為實際詞彙編制索引。</li></ul></li><li><strong>回傳值</strong><ul><li>Numpy 陣列的 tuple: <code>(x_train, y_train), (x_test, y_test)</code>。</li></ul></li><li>其中資料集包含了 88585 個相異單字，有很大的單字甚至只有出現一次，這數字對訓練而言非常龐大，且對分類任務沒什麼幫助，所以我們只保留 10000 個最常出現的單字</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.datasets <span style=color:#f92672>import</span> imdb
</span></span><span style=display:flex><span>(train_images, train_labels), (test_images, test_labels) <span style=color:#f92672>=</span> imdb<span style=color:#f92672>.</span>load_data(num_words<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>)
</span></span></code></pre></div></li><li>IMDb (Internet Movie Database) 是一個與影視相關的網路資料庫，IMDB 資料集就是從 IMDb 網站收集而來的資料。資料包含了 50000 筆評論，其中包含了 50% 的正面評論與 50% 的負面評論。<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(train_data<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(train_labels<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(test_data<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(test_labels<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;</span> (<span style=color:#ae81ff>25000</span>,)
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;</span> (<span style=color:#ae81ff>25000</span>,)
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;</span> (<span style=color:#ae81ff>25000</span>,)
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;</span> (<span style=color:#ae81ff>25000</span>,)
</span></span></code></pre></div></li><li>資料的組成是由一連串的數字所組成，如：<br>\(
\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline \text{(保留)} & \text{the} & \text{and} & \text{a} & \text{&mldr;} & \text{in} & \text{&mldr;} & \text{wonderful} & \text{&mldr;} & \text{morning} & \text{&mldr;} \\
\hline 0 & 1 & 2 & 3 & &mldr; & 8 & &mldr; & 386 & &mldr; & 1969 & &mldr;\\
\hline
\end{array}
\)<ul><li>每個數字代表一個單字，編號愈前面代表愈常用。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(train_data[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;</span> [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>14</span>, <span style=color:#ae81ff>22</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>43</span>, <span style=color:#ae81ff>530</span>, <span style=color:#ae81ff>973</span>, <span style=color:#ae81ff>1622</span>, <span style=color:#f92672>...</span>, <span style=color:#ae81ff>32</span>]
</span></span></code></pre></div><ul><li>我們可以用 <code>imdb.get_word_index(path="imdb_word_index.json")</code> 來得到這個字典，並試著還原原始評論。</li><li>注意按引 <code>0~2</code> 為留保字，故索引值需位移 3。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dict <span style=color:#f92672>=</span> imdb<span style=color:#f92672>.</span>get_word_index(path<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;imdb_word_index.json&#34;</span>)
</span></span><span style=display:flex><span>index_to_word <span style=color:#f92672>=</span> {value: key <span style=color:#66d9ef>for</span> key, value <span style=color:#f92672>in</span> dict<span style=color:#f92672>.</span>items()}
</span></span><span style=display:flex><span>sentence_0 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39; &#39;</span><span style=color:#f92672>.</span>join([index_to_word<span style=color:#f92672>.</span>get(idx, <span style=color:#e6db74>&#39;?&#39;</span>) <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> train_data[<span style=color:#ae81ff>0</span>]])
</span></span><span style=display:flex><span>print(sentence_0)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;</span> <span style=color:#960050;background-color:#1e0010>?</span> this film was just brilliant casting location scenery story direction 
</span></span></code></pre></div><ul><li>labels 是由 1, 0 組成的陣列，代表正評(1)或負評(0)</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(train_labels[:<span style=color:#ae81ff>10</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;</span> array([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>])
</span></span></code></pre></div></li></ul><h2 id=2-準備資料>2. 準備資料<a hidden class=anchor aria-hidden=true href=#2-準備資料>#</a></h2><ul><li>由於目前的測試資料的長度都不一樣長，我們需要預先做整型，方法有二：<ol><li>填補資料中每個串列，使長度相同。</li><li>做 mutli-hot(k-hot) 轉換。<br>\(
\begin{array}{|c|}
\hline \text{column}\\
\hline \text{A}\\
\hline \text{B}\\
\hline \text{C}\\
\hline \text{A}\\
\hline \text{A}\\\hline
\end{array}
\rightarrow
\begin{array}{|c|c|c|}
\hline \text{A} & \text{B} & \text{C}\\
\hline 1 & 0 & 0\\
\hline 0 & 1 & 0\\
\hline 0 & 0 & 1\\
\hline 1 & 0 & 0\\
\hline 1 & 0 & 0\\\hline
\end{array}
\)</li></ol><ul><li>將測試資料轉成 multi-hot 型式：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>vectorize_sequences</span>(sequences, dimension<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>):
</span></span><span style=display:flex><span>    results <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((len(sequences), dimension))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, sequence <span style=color:#f92672>in</span> enumerate(sequences):
</span></span><span style=display:flex><span>        results[i, sequence] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> results
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x_train <span style=color:#f92672>=</span> vectorize_sequences(train_data)
</span></span><span style=display:flex><span>x_test <span style=color:#f92672>=</span> vectorize_sequences(test_data)
</span></span></code></pre></div><ul><li>將 labels 也轉成向量資料</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>y_train <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>asarray(train_labels)<span style=color:#f92672>.</span>astype(<span style=color:#e6db74>&#39;float32&#39;</span>)
</span></span><span style=display:flex><span>y_test <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>asarray(test_labels)<span style=color:#f92672>.</span>astype(<span style=color:#e6db74>&#39;float32&#39;</span>)
</span></span></code></pre></div></li></ul><h2 id=3-建立神經網路>3. 建立神經網路<a hidden class=anchor aria-hidden=true href=#3-建立神經網路>#</a></h2><ul><li><p>我們的輸入資料是向量、標籤為 0 與 1，我們可以使用一個密集層堆疊架構搭配 relu 函數。</p></li><li><p>我設計以下的三層神經網路架構：<br>\(
\begin{array}{ccc}
\text{輸入(向量化文字)}\\
\downarrow\\
\boxed{\text{密集(單元=16)}} & \text{relu} & \text{hidden layer}\\
\downarrow\\
\boxed{\text{密集(單元=16)}} & \text{relu} & \text{hidden layer}\\
\downarrow\\
\boxed{\text{密集(單元=1)}} & \text{softmax} & \text{output layer}\\
\downarrow\\
\text{輸出(預測值)}\\
\end{array}
\)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow <span style=color:#f92672>import</span> keras
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras <span style=color:#f92672>import</span> layers
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>16</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>16</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;sigmoid&#39;</span>)
</span></span><span style=display:flex><span>])
</span></span></code></pre></div><ul><li>compile</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model<span style=color:#f92672>.</span>compile(optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;rmsprop&#39;</span>,
</span></span><span style=display:flex><span>              loss<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;binary_crossentropy&#39;</span>,
</span></span><span style=display:flex><span>              metrics<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;accuracy&#39;</span>])
</span></span></code></pre></div><ul><li>建立驗證集</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x_val <span style=color:#f92672>=</span> x_train[:<span style=color:#ae81ff>10000</span>]
</span></span><span style=display:flex><span>partial_x_train <span style=color:#f92672>=</span> x_train[<span style=color:#ae81ff>10000</span>:]
</span></span><span style=display:flex><span>y_val <span style=color:#f92672>=</span> y_train[:<span style=color:#ae81ff>10000</span>]
</span></span><span style=display:flex><span>partial_y_train <span style=color:#f92672>=</span> y_train[<span style=color:#ae81ff>10000</span>:]
</span></span></code></pre></div><ul><li>訓練模型</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>history <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(partial_x_train,
</span></span><span style=display:flex><span>                    partial_y_train,
</span></span><span style=display:flex><span>                    epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>
</span></span><span style=display:flex><span>                    batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>,
</span></span><span style=display:flex><span>                    validation_data<span style=color:#f92672>=</span>(x_val, y_val))
</span></span></code></pre></div><ul><li>繪製訓練與驗證圖</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>history_dict <span style=color:#f92672>=</span> history<span style=color:#f92672>.</span>history
</span></span><span style=display:flex><span>loss_values <span style=color:#f92672>=</span> history_dict[<span style=color:#e6db74>&#39;loss&#39;</span>]
</span></span><span style=display:flex><span>val_loss_values <span style=color:#f92672>=</span> history_dict[<span style=color:#e6db74>&#39;val_loss&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>epochs <span style=color:#f92672>=</span> range(<span style=color:#ae81ff>1</span>, len(loss_values) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(epochs, loss_values, <span style=color:#e6db74>&#39;bo&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Training loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(epochs, val_loss_values, <span style=color:#e6db74>&#39;b&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Validation loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Training and validation loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Epochs&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p><img alt=validation1 loading=lazy src=/ai/AI/4_1/validation_plot1.png></p><ul><li>繪製訓練和驗證準確度</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt<span style=color:#f92672>.</span>clf()
</span></span><span style=display:flex><span>acc <span style=color:#f92672>=</span> history_dict[<span style=color:#e6db74>&#39;accuracy&#39;</span>]
</span></span><span style=display:flex><span>val_acc <span style=color:#f92672>=</span> history_dict[<span style=color:#e6db74>&#39;val_accuracy&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(epochs, acc, <span style=color:#e6db74>&#39;bo&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Training accuracy&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(epochs, val_acc, <span style=color:#e6db74>&#39;b&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Validation accuracy&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Training and validation accuracy&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Epochs&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Accuracy&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span></code></pre></div><p><img alt=validation2 loading=lazy src=/ai/AI/4_1/validation_plot2.png></p><ul><li>從結果可見，訓練集的損失隨訓練週期增加而減少，準確度隨週期增加而增加，但是對驗證集而言，卻沒有得到一樣的成效，這就是 <strong>過度配適(overfitting)</strong> 的現象。</li><li>我們可以使用其它技術來避免 overfitting 的發生，如 early stop、drop out</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow <span style=color:#f92672>import</span> keras
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras <span style=color:#f92672>import</span> layers
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.callbacks <span style=color:#f92672>import</span> EarlyStopping
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>16</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0.5</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>16</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>5</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;sigmoid&#39;</span>)
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>compile(optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;rmsprop&#39;</span>,
</span></span><span style=display:flex><span>            loss<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;binary_crossentropy&#39;</span>,
</span></span><span style=display:flex><span>            metrics<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;accuracy&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>early_stopping <span style=color:#f92672>=</span> EarlyStopping(
</span></span><span style=display:flex><span>    monitor<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;val_loss&#39;</span>,
</span></span><span style=display:flex><span>    patience<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>,  <span style=color:#75715e># 連續4個epoch沒改善就停止</span>
</span></span><span style=display:flex><span>    restore_best_weights<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>  <span style=color:#75715e># 回復最佳權重</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x_val <span style=color:#f92672>=</span> x_train[:<span style=color:#ae81ff>10000</span>]
</span></span><span style=display:flex><span>partial_x_train <span style=color:#f92672>=</span> x_train[<span style=color:#ae81ff>10000</span>:]
</span></span><span style=display:flex><span>y_val <span style=color:#f92672>=</span> y_train[:<span style=color:#ae81ff>10000</span>]
</span></span><span style=display:flex><span>partial_y_train <span style=color:#f92672>=</span> y_train[<span style=color:#ae81ff>10000</span>:]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>history <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(partial_x_train,
</span></span><span style=display:flex><span>                    partial_y_train,
</span></span><span style=display:flex><span>                    epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>,
</span></span><span style=display:flex><span>                    batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>,
</span></span><span style=display:flex><span>                    validation_data<span style=color:#f92672>=</span>(x_val, y_val),
</span></span><span style=display:flex><span>                    callbacks<span style=color:#f92672>=</span>[early_stopping])
</span></span></code></pre></div><p><img alt=validation_plot_with_early_stopping_1 loading=lazy src=/ai/AI/4_1/validation_plot_es1.png>
<img alt=validation_plot_with_early_stopping_1 loading=lazy src=/ai/AI/4_1/validation_plot_es2.png></p><ul><li>拿來測試驗證集</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>predictions <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(x_test, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>)
</span></span><span style=display:flex><span>test_loss, test_acc <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>evaluate(x_test, y_test)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;test_acc: </span><span style=color:#e6db74>{</span>test_acc<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;</span> test_acc: <span style=color:#ae81ff>0.8849200010299683</span>
</span></span></code></pre></div></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://intervalrain.github.io/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://intervalrain.github.io/ai/3_8/><span class=title>« 上一頁</span><br><span>[AI] 3-8. 客製化 Training</span>
</a><a class=next href=https://intervalrain.github.io/ai/4_2/><span class=title>下一頁 »</span><br><span>[AI] 多元分類問題</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://intervalrain.github.io/>Rain Hu's Workspace</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="複製";function s(){t.innerHTML="已複製！",setTimeout(()=>{t.innerHTML="複製"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>