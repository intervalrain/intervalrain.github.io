<!doctype html><html lang=zh-tw dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[AI] 2-3. 優化器 Optimizer | Rain Hu's Workspace</title><meta name=keywords content="AI"><meta name=description content="The concept of optimizer in machine learning"><meta name=author content="Rain Hu"><link rel=canonical href=https://intervalrain.github.io/><meta name=google-site-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0cefe5a1d95e3d0f0cce057d37c60cd238d1a4af825090f831a18f21671f621d.css integrity="sha256-DO/lodlePQ8MzgV9N8YM0jjRpK+CUJD4MaGPIWcfYh0=" rel="preload stylesheet" as=style><link rel=icon href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=16x16 href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=32x32 href=https://intervalrain.github.io/images/rain.png><link rel=apple-touch-icon href=https://intervalrain.github.io/images/rain.png><link rel=mask-icon href=https://intervalrain.github.io/images/rain.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh-tw href=https://intervalrain.github.io/ai/2_3/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css integrity=sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js integrity=sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script src=https://utteranc.es/client.js repo=intervalrain.github.io issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script><meta property="og:url" content="https://intervalrain.github.io/ai/2_3/"><meta property="og:site_name" content="Rain Hu's Workspace"><meta property="og:title" content="[AI] 2-3. 優化器 Optimizer"><meta property="og:description" content="The concept of optimizer in machine learning"><meta property="og:locale" content="zh-tw"><meta property="og:type" content="article"><meta property="article:section" content="ai"><meta property="article:published_time" content="2024-12-16T15:48:11+08:00"><meta property="article:modified_time" content="2024-12-16T15:48:11+08:00"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="[AI] 2-3. 優化器 Optimizer"><meta name=twitter:description content="The concept of optimizer in machine learning"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"[AI] 2-3. 優化器 Optimizer","item":"https://intervalrain.github.io/ai/2_3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[AI] 2-3. 優化器 Optimizer","name":"[AI] 2-3. 優化器 Optimizer","description":"The concept of optimizer in machine learning","keywords":["AI"],"articleBody":" 在神經網路中，每個神經層都通過以下的方式進行資料轉換\noutput = relu(dot(W, input) + b) W 和 b 為該層的屬性張量，統稱為該層的權重(weights) 或可訓練參數(trainable parameters)，分別為 內核(kernel) 屬性和 偏值(bias) 屬性。 我們的目標是要透過訓練(training) 來進行權重的微調，進而得到最小的損失值。 參數調大或調小，可能會對應到「損失值變大」或「損失值變小」兩種可能(如果兩者皆變小，稱為 saddle point，兩者皆變大，可能為 local minium)。\n但如果透過兩個方向的正向傳播來檢查微調的方向，效率很低，於是科學家引入了梯度下降法。 梯度下降法(gradient descent) 張量運算的導數稱為梯度(gradient)。 純量函數的導數相當於在函數曲線上找某一點的斜率。 張量函數上求梯度，相當於在函數描繪的多維曲面上求曲率。 y_pred = dot(W, x) loss_value = loss(y_pred, y_true) 假設只考慮 W 為變數，我們可以將 loss_value 視為 W 的函數。\nloss_value = loss(dot(W, x), y_true) = f(W) 假設 W 的當前值為 W0，那麼函數 f 在 W0 這點的導數就是\n$$ f’(W)=\\frac{d\\text{(loss\\_value)}}{d\\text{W}}|\\text{W}_0 $$ 我們將上式寫成\ng = grad(loss_value, W0) 代表了在 W0 附近，loss_value = f(W) 的梯度最陡方向之張量，我們可以透過往往「斜率反方向」移動來降低 f(W)。\nif g \u003e 0: W' = W0 - dW elif: g \u003c 0: W' = W0 + dW 我們將 g 直接代入式子，並引入 \\(\\eta\\)(eta) 的概念，\\(\\eta\\)可以想成是每次移動的步伐大小。\nW' = W0 - g * eta 藉由不斷迭代，直到最後平緩，代表移到動 local minimum。但這個 local minimum 不一定是 global minimum，所以後面會加入其它方法來避免被 trap 在 local minimum。 以下是李宏毅老師的圖解，我覺得很生動，可以幫助理解。 圖解 隨機梯度下降 從上述的推導，其實我們可以設想，如何快速取得最小的損失值，方法是找出所有導數為 0 的點，再對這些點進行檢查，便可求出 global minimum。\n然後對實際的神經網路而言，參數不會只有 2~3 個，可能會有上千萬個，故要求這樣的方程式解並非容易的事。\n於是我們可以透過 小批次隨機梯度下降(mini-batch stochastic gradient descent, mini-batch SGD) 來進行\n取出一批次的訓練樣本 x 和對應的目標 y_true(也是標籤(label)) 以 x 為輸出資料，運行神經網路獲得預測值 y_pred。此步驟稱為正向傳播(forward pass) 計算神經網路的批次損失值。 計算損失值相對於神經網路權重的梯度。 將參數稍微向梯度的反方向移動，從而降低一些批次損失值。此步驟稱為反向傳播(back propagation) $$ W’ = W - \\eta \\times \\frac{dL}{dW} $$ 其中 \\(\\eta\\) 為學習率(learning rate)，為純量因子，可用來調整梯度下降的速度。 學習率太大太小都可能產生問題 學習率太大可能會略過真正的最小值 學習率太小可能會困在區域最小值 批次的量(batch size)\n一次將所有的資料全用上，稱為整批 SGD(batch gradient descent)，好處是每次參數值更新會更準確，但是也會提升時間複雜度。 為了在「準確度」與「複雜度」取一個 tradeoff，通常會使用合理大小的小批次資料進行計算。 SGD 的變體\n在計算下一次權重的更新量，可以考慮先前的取重更新量來做調整，而非僅僅根據梯度的當前值。常見的變體包含： momemtum past_velocity = 0. momemtum = 0.1 # 固定的動量因數 while loss \u003e 0.01: w, loss, gradient = get_current_parameters() velocity = past_velocity * momemtum - learning_rate * gradient w = w + momemtum * velocity - learning_rate * gradient past_velocity = velocity update_parameter(w) Adagrad RMSProp 反向傳播演算法(Backpropagation) 前面介紹的函數是簡單函式，可以很簡單地算出導數(梯度)，但在實際情況下，我們需要能夠處理複雜函數的梯度。\n連鎖律(Chain Rule) 反向傳播是借助簡單運算(eg. 加法、relu或是張量積)的導數，進而得出這算簡單運算的複雜組合的梯度。\n舉例而言，如下圖，我們使用了兩個密集層做轉換，\n\\( \\boxed{ \\begin{array}{ccccccc} \u0026\u0026 \\text{輸入資料 X} \u0026 \\\\ \u0026\u0026 \\downarrow \u0026 \\\\ \\boxed{\\text{權重’}} \u0026 \\rightarrow \u0026 \\boxed{\\text{層(資料轉換)}} \u0026 \\red{\\text{relu(W1,b1)}}\\\\ \\uparrow \u0026\u0026 \\downarrow \u0026 \\\\ \\boxed{\\text{權重’}} \u0026 \\rightarrow \u0026 \\boxed{\\text{層(資料轉換)}} \u0026 \\red{\\text{softmax(W1,b2)}}\\\\ \u0026\u0026 \\downarrow \u0026 \\\\ \\uparrow \u0026\u0026 \\boxed{\\text{預測 Y’}}\\rightarrow \u0026 \\boxed{\\text{損失函數}} \u0026 \\leftarrow \u0026 \\boxed{\\text{標準答案 Y}} \\\\ \u0026\u0026\u0026 \\downarrow \u0026 \\\\ \\boxed{\\text{優化器}} \u0026\u0026 \\leftarrow \u0026 \\boxed{\\text{損失分數}} \\end{array} } \\)\n我們可以把函式表運成：\ny1 = relu(dot(W1, X)+b1) y2 = softmax(dot(W2, y1)+b2) loss_value = loss(y_true, y2) 或\nloss_value = loss(y_true, softmax(dot(W2, relu(dot(W1, X)+b1))+b2)) 我們可以透過連鎖律求得連鎖函數的導數：假設有 \\(f\\), \\(g\\) 兩個函數，它們的複合函數 \\(fg\\) 有著 \\(fg(x)=f(g(x))\\)的特性。\ndef fg(x): x1 = g(x) y = f(x1) return y 根據連鎖律 $$ y=f(x_1,x_2,x_3…x_n),\\frac{d\\text{y}}{d\\text{x}}=\\red{\\frac{d\\text{y}}{d\\text{x}_1}\\times\\frac{d\\text{x}_1}{d\\text{x}_2}\\times\\frac{d\\text{x}_2}{d\\text{x}_3}\\times…\\times\\frac{d\\text{x}_n}{d\\text{x}}} $$ 換言之，我們只需要求出 \\(\\frac{d\\text{y}}{d\\text{x}_1}\\times\\frac{d\\text{x}_1}{d\\text{x}_2}\\times\\frac{d\\text{x}_2}{d\\text{x}_3}\\times…\\times\\frac{d\\text{x}_n}{d\\text{x}}\\) 就可以知道找出複合函數 \\(y\\) 的導數了。\n詳述 前向傳播\n對於一個神經元，輸入值 \\(\\text{x}\\) 通過權重 \\(\\text{w}\\) 和偏置 \\(\\text{b}\\) 進行線性組合，然後通過激活函數 \\(\\text{relu}\\)： $$\\text{z}=\\text{wx+b}$$ $$\\text{y}=\\text{relu(z)}$$\n損失函數\n假設使用均方誤差(MSE)作為損失函數： $$L = \\frac{1}{2}(\\text{y} - \\text{y}_\\text{true})^2$$\n反向傳播\n使用連鎖律計算損失函數對權重的偏導數： $$\\frac{\\partial L}{\\partial \\text{w}} = \\frac{\\partial L}{\\partial \\text{y}} \\cdot \\frac{\\partial \\text{y}}{\\partial \\text{z}} \\cdot \\frac{\\partial \\text{z}}{\\partial \\text{w}}$$ 分解每一項：\n$$\\frac{\\partial L}{\\partial \\text{y}} = (\\text{y} - \\text{y}_\\text{true})$$ $$\\frac{\\partial \\text{y}}{\\partial \\text{z}} = \\text{relu}’(\\text{z})$$ $$\\frac{\\partial \\text{z}}{\\partial \\text{w}} = \\text{x}$$\n因此： $$\\frac{\\partial L}{\\partial \\text{w}} = (\\text{y} - \\text{y}_\\text{true}) \\cdot \\text{relu}’(\\text{z}) \\cdot \\text{x}$$\n權重更新\n使用梯度下降更新權重： $$\\text{w’} = \\text{w} - \\eta \\frac{\\partial L}{\\partial \\text{w}}$$\n思考正向傳播時，我們計算的順序是 $$ \\text{z}\\rightarrow\\text{y}\\rightarrow L $$ 而在推導梯度時，我們計算的順序是，剛好是反向過來計算的，故這個過程稱作「反向傳播」 $$\\frac{\\partial L}{\\partial \\text{y}}\\rightarrow\\frac{\\partial \\text{y}}{\\partial \\text{z}}\\rightarrow\\frac{\\partial \\text{z}}{\\partial \\text{w}}$$ 如今我們在現代框架中，已經可以透過自動微分來實作神經網路，如 TensorFlow 的 Gradient Tape。故我們可以只專注在「正向傳播」的過程。\nTensorFlow 的 Gradient Tape 透過 TensorFlow 的 Gradient Tape 函式，可以快速的取得微分值：\n其中 tf.Variable 是一個變數物件，它可以是任意階的張量\n範例： import tensorflow as tf x = tf.Variable(0.) with tf.GradientTape() as tape: y = 2 * x + 3 grad = tape.gradient(y, x) grad \u003e\u003e\u003e \u003ctf.Tensor: shape=(), dtype=float32, numpy=2.0\u003e 實作一個簡單的模型 Dense Layer 作用是將 input 與 W 做點積，與 b 相加，再輸入激活函數 f 相當於 y = f(wx+b) import tensorflow as tf class NaiveDense: def __init__(self, input_size, output_size, activation): self.activation = activation w_shape = (input_size, output_size) w_initial_value = tf.random.uniform(w_shape, minval = 0, maxval = 0.1) self.W = tf.Variable(w_initial_value) b_shape = (output_size, ) b_initial_value = tf.zeros(b_shape) self.b = tf.Variable(b_initial_value) def __call__(self, inputs): return self.activation(tf.matmul(inputs, self.W) + self.b) @property def weights(self): return [self.W, self.b] Sequential Sequential 用來定義層與層之間連接的方式 在次簡單的用「依序」的方式進行正向傳播 class NaiveSequential: def __init__(self, layers): self.layers = layers def __call__(self, inputs): x = inputs for layer in self.layers: x = layer(x) return x @property def weights(self): weights = [] for layer in self.layers: weights += layer.weights return weights Build Model 通過上面兩個實作，便可以簡單建立一個 Keras 雙層模型： model = NaiveSequential([ NaiveDense(input_size=28*28, output_size=512, activation=tf.nn.relu), NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax) ]) Batch Generator 我們需要用「小批次」的方式來迭代，能更有效率的訓練資料： import math class BatchGeneartor: def __init__(self, images, labels, batch_size=128): assert len(images) == len(labels) self.index = 0 self.images = images self.labels = labels self.batch_size = batch_size self.num_batches = math.ceil(len(images) / batch_size) def next(self): images = self.images[self.index : self.index + self.batch_size] labels = self.labels[self.index : self.index + self.batch_size] self.index += self.batch_size return images, labels Update Weights 接下來我要透過 \\(\\text{w’}=\\text{w}-\\eta\\frac{dL}{d\\text{W}}\\) 來更新 W： def update_weights(gradients, weights, learning_rate=1e-3): for g, w in zip(gradients, weights): w.assign_sub(g * learning_rate) 事實上我們不會這樣更新權重，我們可以借用 keras 的 optimizers from tensorflow.keras import optimizers optimizer = optimizers.SGD(learning_rate=1e-3) def update_weights(gradients, weights): optimizer.apply_gradients(zip(gradients, weights)) Training 有了以上的工具，我們就可以進行訓練了 def one_training_step(model, images_batch, labels_batch): with tf.GradientTape() as tape: # y' = f(wx+b) predictions = model(images_batch) # loss[] = y' - y_true per_sample_losses = (tf.keras.losses.sparse_categorical_crossentropy (labels_batch, predictions)) # avg_loss = loss average_loss = tf.reduce_mean(per_sample_losses)[] / batch_size # g = grad(L, w) gradients = tape.gradient(average_loss, model.weights) # w' = w-ηg update_weights(gradients, model.weights) return average_loss fit def fit(model, images, labels, epochs, batch_size=128): for epoch_counter in range(epochs): print(f\"Epoch {epoch_counter}\") batch_generator = BatchGeneartor(images, labels) for batch_counter in range(batch_generator.num_batches): images_batch, labels_batch = batch_generator.next() loss = one_training_step(model, images_batch, labels_batch) if batch_counter % 100 == 0: print(f\"loss at batch {batch_counter}: {loss:.2f}\") 執行 from tensorflow.keras.datasets import mnist (train_images, train_labels),(test_images, test_labels) = mnist.load_data() train_images = train_images.reshape((60000, 28*28)) train_images = train_images.astype(\"float32\") / 255 test_images = test_images.reshape((10000, 28*28)) test_images = test_images.astype(\"float32\") / 255 fit(model, train_images, train_labels, epochs=10, batch_size=128) Evaluation 最後我們再投入 test_images 來評估我們 training 的結果 import numpy as np predictions = model(test_images) predictions = predictions.numpy() predicted_labels = np.argmax(predictions, axis=1) matches = predicted_labels == test_labels print(f\"accuracy: {matches.mean():.2f}\") ","wordCount":"839","inLanguage":"zh-tw","datePublished":"2024-12-16T15:48:11+08:00","dateModified":"2024-12-16T15:48:11+08:00","author":{"@type":"Person","name":"Rain Hu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://intervalrain.github.io/ai/2_3/"},"publisher":{"@type":"Organization","name":"Rain Hu's Workspace","logo":{"@type":"ImageObject","url":"https://intervalrain.github.io/images/rain.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://intervalrain.github.io/ accesskey=h title="Rain Hu's Workspace (Alt + H)"><img src=https://intervalrain.github.io/images/rain.png alt aria-label=logo height=35>Rain Hu's Workspace</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://intervalrain.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://intervalrain.github.io/aboutme title="About me"><span>About me</span></a></li><li><a href=https://intervalrain.github.io/archives title=Archives><span>Archives</span></a></li><li><a href=https://intervalrain.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://intervalrain.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://intervalrain.github.io/csharp/csharp title=C#><span>C#</span></a></li><li><a href=https://intervalrain.github.io/csindex title=CS><span>CS</span></a></li><li><a href=https://intervalrain.github.io/leetcode title=LeetCode><span>LeetCode</span></a></li><li><a href=https://intervalrain.github.io/ai title=AI><span>AI</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://intervalrain.github.io/>首頁</a></div><h1 class="post-title entry-hint-parent">[AI] 2-3. 優化器 Optimizer</h1><div class=post-description>The concept of optimizer in machine learning</div><div class=post-meta><span title='2024-12-16 15:48:11 +0800 +0800'>December 16, 2024</span>&nbsp;·&nbsp;4 分鐘&nbsp;·&nbsp;Rain Hu&nbsp;|&nbsp;<a href=https://github.com/intervalrain/intervalrain.github.io/tree/main/content//AI/2_3.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目錄</span></summary><div class=inner><ul><li><a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95gradient-descent aria-label="梯度下降法(gradient descent)">梯度下降法(gradient descent)</a></li><li><a href=#%e9%9a%a8%e6%a9%9f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d aria-label=隨機梯度下降>隨機梯度下降</a></li><li><a href=#%e5%8f%8d%e5%90%91%e5%82%b3%e6%92%ad%e6%bc%94%e7%ae%97%e6%b3%95backpropagation aria-label=反向傳播演算法(Backpropagation)>反向傳播演算法(Backpropagation)</a><ul><li><a href=#%e9%80%a3%e9%8e%96%e5%be%8bchain-rule aria-label="連鎖律(Chain Rule)">連鎖律(Chain Rule)</a></li></ul></li><li><a href=#tensorflow-%e7%9a%84-gradient-tape aria-label="TensorFlow 的 Gradient Tape">TensorFlow 的 Gradient Tape</a></li><li><a href=#%e5%af%a6%e4%bd%9c%e4%b8%80%e5%80%8b%e7%b0%a1%e5%96%ae%e7%9a%84%e6%a8%a1%e5%9e%8b aria-label=實作一個簡單的模型>實作一個簡單的模型</a><ul><li><a href=#dense-layer aria-label="Dense Layer">Dense Layer</a></li><li><a href=#sequential aria-label=Sequential>Sequential</a></li><li><a href=#build-model aria-label="Build Model">Build Model</a></li><li><a href=#batch-generator aria-label="Batch Generator">Batch Generator</a></li><li><a href=#update-weights aria-label="Update Weights">Update Weights</a></li><li><a href=#training aria-label=Training>Training</a></li><li><a href=#evaluation aria-label=Evaluation>Evaluation</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><ul><li>在神經網路中，每個神經層都通過以下的方式進行資料轉換<br><code>output = relu(dot(W, input) + b)</code></li><li><code>W</code> 和 <code>b</code> 為該層的屬性張量，統稱為該層的權重(weights) 或可訓練參數(trainable parameters)，分別為 <strong>內核(kernel)</strong> 屬性和 <strong>偏值(bias)</strong> 屬性。</li><li>我們的目標是要透過訓練(training) 來進行權重的微調，進而得到最小的損失值。</li><li>參數調大或調小，可能會對應到「損失值變大」或「損失值變小」兩種可能(如果兩者皆變小，稱為 saddle point，兩者皆變大，可能為 local minium)。<br>但如果透過兩個方向的正向傳播來檢查微調的方向，效率很低，於是科學家引入了梯度下降法。</li></ul><h3 id=梯度下降法gradient-descent>梯度下降法(gradient descent)<a hidden class=anchor aria-hidden=true href=#梯度下降法gradient-descent>#</a></h3><ul><li>張量運算的導數稱為<strong>梯度(gradient)</strong>。<ul><li>純量函數的導數相當於在函數曲線上找某一點的<strong>斜率</strong>。</li><li>張量函數上求梯度，相當於在函數描繪的多維曲面上求<strong>曲率</strong>。</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>y_pred <span style=color:#f92672>=</span> dot(W, x)
</span></span><span style=display:flex><span>loss_value <span style=color:#f92672>=</span> loss(y_pred, y_true)
</span></span></code></pre></div><p>假設只考慮 W 為變數，我們可以將 loss_value 視為 W 的函數。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loss_value <span style=color:#f92672>=</span> loss(dot(W, x), y_true) <span style=color:#f92672>=</span> f(W)
</span></span></code></pre></div><p>假設 W 的當前值為 W0，那麼函數 f 在 W0 這點的導數就是</p><p>$$
f&rsquo;(W)=\frac{d\text{(loss\_value)}}{d\text{W}}|\text{W}_0
$$
我們將上式寫成</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>g <span style=color:#f92672>=</span> grad(loss_value, W0)
</span></span></code></pre></div><p>代表了在 W0 附近，loss_value = f(W) 的梯度最陡方向之張量，我們可以透過往往「斜率反方向」移動來降低 f(W)。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>if</span> g <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>    W<span style=color:#e6db74>&#39; = W0 - dW</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>elif</span>: g <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>    W<span style=color:#e6db74>&#39; = W0 + dW</span>
</span></span></code></pre></div><p>我們將 g 直接代入式子，並引入 \(\eta\)(eta) 的概念，\(\eta\)可以想成是每次移動的步伐大小。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W<span style=color:#e6db74>&#39; = W0 - g * eta</span>
</span></span></code></pre></div><ul><li>藉由不斷迭代，直到最後平緩，代表移到動 local minimum。但這個 local minimum 不一定是 global minimum，所以後面會加入其它方法來避免被 trap 在 local minimum。</li><li>以下是李宏毅老師的圖解，我覺得很生動，可以幫助理解。<details><summary>圖解</summary><img alt=gradient_descent loading=lazy src=/ai/AI/2_3/gradient-descent.png></details></li></ul><h3 id=隨機梯度下降>隨機梯度下降<a hidden class=anchor aria-hidden=true href=#隨機梯度下降>#</a></h3><ul><li><p>從上述的推導，其實我們可以設想，如何快速取得最小的損失值，方法是找出所有導數為 0 的點，再對這些點進行檢查，便可求出 global minimum。</p></li><li><p>然後對實際的神經網路而言，參數不會只有 2~3 個，可能會有上千萬個，故要求這樣的方程式解並非容易的事。</p></li><li><p>於是我們可以透過 <strong>小批次隨機梯度下降(mini-batch stochastic gradient descent, mini-batch SGD)</strong> 來進行</p><ol><li>取出一批次的訓練樣本 x 和對應的目標 y_true(也是標籤(label))</li><li>以 x 為輸出資料，運行神經網路獲得預測值 y_pred。此步驟稱為<strong>正向傳播(forward pass)</strong></li><li>計算神經網路的批次損失值。</li><li>計算損失值相對於神經網路權重的梯度。</li><li>將參數稍微向梯度的反方向移動，從而降低一些批次損失值。此步驟稱為<strong>反向傳播(back propagation)</strong>
$$
W&rsquo; = W - \eta \times \frac{dL}{dW}
$$
其中 \(\eta\) 為<strong>學習率(learning rate)</strong>，為純量因子，可用來調整梯度下降的速度。<ul><li>學習率太大太小都可能產生問題<ul><li>學習率太大可能會略過真正的最小值</li><li>學習率太小可能會困在區域最小值</li></ul></li></ul></li></ol></li><li><p>批次的量(batch size)</p><ul><li>一次將所有的資料全用上，稱為整批 SGD(batch gradient descent)，好處是每次參數值更新會更準確，但是也會提升時間複雜度。</li><li>為了在「準確度」與「複雜度」取一個 tradeoff，通常會使用合理大小的小批次資料進行計算。</li></ul></li><li><p>SGD 的變體</p><ul><li>在計算下一次權重的更新量，可以考慮先前的取重更新量來做調整，而非僅僅根據梯度的當前值。常見的變體包含：<ul><li>momemtum</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>past_velocity <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.</span>
</span></span><span style=display:flex><span>momemtum <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>  <span style=color:#75715e># 固定的動量因數</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>while</span> loss <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0.01</span>:
</span></span><span style=display:flex><span>    w, loss, gradient <span style=color:#f92672>=</span> get_current_parameters()
</span></span><span style=display:flex><span>    velocity <span style=color:#f92672>=</span> past_velocity <span style=color:#f92672>*</span> momemtum <span style=color:#f92672>-</span> learning_rate <span style=color:#f92672>*</span> gradient
</span></span><span style=display:flex><span>    w <span style=color:#f92672>=</span> w <span style=color:#f92672>+</span> momemtum <span style=color:#f92672>*</span> velocity <span style=color:#f92672>-</span> learning_rate <span style=color:#f92672>*</span> gradient
</span></span><span style=display:flex><span>    past_velocity <span style=color:#f92672>=</span> velocity
</span></span><span style=display:flex><span>    update_parameter(w)
</span></span></code></pre></div><ul><li>Adagrad</li><li>RMSProp</li></ul></li></ul></li></ul><h3 id=反向傳播演算法backpropagation>反向傳播演算法(Backpropagation)<a hidden class=anchor aria-hidden=true href=#反向傳播演算法backpropagation>#</a></h3><p>前面介紹的函數是簡單函式，可以很簡單地算出導數(梯度)，但在實際情況下，我們需要能夠處理複雜函數的梯度。</p><h4 id=連鎖律chain-rule>連鎖律(Chain Rule)<a hidden class=anchor aria-hidden=true href=#連鎖律chain-rule>#</a></h4><p>反向傳播是借助簡單運算(eg. 加法、relu或是張量積)的導數，進而得出這算簡單運算的複雜組合的梯度。<br>舉例而言，如下圖，我們使用了兩個密集層做轉換，</p><p>\(
\boxed{
\begin{array}{ccccccc}
&& \text{輸入資料 X} & \\
&& \downarrow & \\
\boxed{\text{權重&rsquo;}} & \rightarrow & \boxed{\text{層(資料轉換)}} & \red{\text{relu(W1,b1)}}\\
\uparrow && \downarrow & \\
\boxed{\text{權重&rsquo;}} & \rightarrow & \boxed{\text{層(資料轉換)}} & \red{\text{softmax(W1,b2)}}\\
&& \downarrow & \\
\uparrow && \boxed{\text{預測 Y&rsquo;}}\rightarrow & \boxed{\text{損失函數}} & \leftarrow & \boxed{\text{標準答案 Y}} \\
&&& \downarrow & \\
\boxed{\text{優化器}} && \leftarrow & \boxed{\text{損失分數}}
\end{array}
}
\)</p><p>我們可以把函式表運成：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>y1 <span style=color:#f92672>=</span> relu(dot(W1, X)<span style=color:#f92672>+</span>b1)
</span></span><span style=display:flex><span>y2 <span style=color:#f92672>=</span> softmax(dot(W2, y1)<span style=color:#f92672>+</span>b2)
</span></span><span style=display:flex><span>loss_value <span style=color:#f92672>=</span> loss(y_true, y2)
</span></span></code></pre></div><p>或</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loss_value <span style=color:#f92672>=</span> loss(y_true, softmax(dot(W2, relu(dot(W1, X)<span style=color:#f92672>+</span>b1))<span style=color:#f92672>+</span>b2))
</span></span></code></pre></div><p>我們可以透過連鎖律求得連鎖函數的導數：假設有 \(f\), \(g\) 兩個函數，它們的複合函數 \(fg\) 有著 \(fg(x)=f(g(x))\)的特性。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fg</span>(x):
</span></span><span style=display:flex><span>    x1 <span style=color:#f92672>=</span> g(x)
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> f(x1)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> y
</span></span></code></pre></div><p>根據連鎖律
$$
y=f(x_1,x_2,x_3&mldr;x_n),\frac{d\text{y}}{d\text{x}}=\red{\frac{d\text{y}}{d\text{x}_1}\times\frac{d\text{x}_1}{d\text{x}_2}\times\frac{d\text{x}_2}{d\text{x}_3}\times&mldr;\times\frac{d\text{x}_n}{d\text{x}}}
$$
換言之，我們只需要求出 \(\frac{d\text{y}}{d\text{x}_1}\times\frac{d\text{x}_1}{d\text{x}_2}\times\frac{d\text{x}_2}{d\text{x}_3}\times&mldr;\times\frac{d\text{x}_n}{d\text{x}}\) 就可以知道找出複合函數 \(y\) 的導數了。</p><p><details><summary>詳述</summary><strong>前向傳播</strong><br>對於一個神經元，輸入值 \(\text{x}\) 通過權重 \(\text{w}\) 和偏置 \(\text{b}\) 進行線性組合，然後通過激活函數 \(\text{relu}\)：
$$\text{z}=\text{wx+b}$$
$$\text{y}=\text{relu(z)}$$</p><p><strong>損失函數</strong><br>假設使用均方誤差(MSE)作為損失函數：
$$L = \frac{1}{2}(\text{y} - \text{y}_\text{true})^2$$</p><p><strong>反向傳播</strong><br>使用連鎖律計算損失函數對權重的偏導數：
$$\frac{\partial L}{\partial \text{w}} = \frac{\partial L}{\partial \text{y}} \cdot \frac{\partial \text{y}}{\partial \text{z}} \cdot \frac{\partial \text{z}}{\partial \text{w}}$$
分解每一項：</p><p>$$\frac{\partial L}{\partial \text{y}} = (\text{y} - \text{y}_\text{true})$$
$$\frac{\partial \text{y}}{\partial \text{z}} = \text{relu}&rsquo;(\text{z})$$
$$\frac{\partial \text{z}}{\partial \text{w}} = \text{x}$$</p><p>因此：
$$\frac{\partial L}{\partial \text{w}} = (\text{y} - \text{y}_\text{true}) \cdot \text{relu}&rsquo;(\text{z}) \cdot \text{x}$$</p><p><strong>權重更新</strong><br>使用梯度下降更新權重：
$$\text{w&rsquo;} = \text{w} - \eta \frac{\partial L}{\partial \text{w}}$$</p><ul><li>思考正向傳播時，我們計算的順序是
$$
\text{z}\rightarrow\text{y}\rightarrow L
$$</li><li>而在推導梯度時，我們計算的順序是，剛好是反向過來計算的，故這個過程稱作「反向傳播」
$$\frac{\partial L}{\partial \text{y}}\rightarrow\frac{\partial \text{y}}{\partial \text{z}}\rightarrow\frac{\partial \text{z}}{\partial \text{w}}$$</li></ul></details><p>如今我們在現代框架中，已經可以透過<strong>自動微分</strong>來實作神經網路，如 TensorFlow 的 Gradient Tape。故我們可以只專注在「正向傳播」的過程。</p><h3 id=tensorflow-的-gradient-tape>TensorFlow 的 Gradient Tape<a hidden class=anchor aria-hidden=true href=#tensorflow-的-gradient-tape>#</a></h3><p>透過 TensorFlow 的 Gradient Tape 函式，可以快速的取得微分值：</p><ul><li>其中 <code>tf.Variable</code> 是一個變數物件，它可以是任意階的張量<br>範例：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> tensorflow <span style=color:#66d9ef>as</span> tf
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>Variable(<span style=color:#ae81ff>0.</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>GradientTape() <span style=color:#66d9ef>as</span> tape:
</span></span><span style=display:flex><span>  y <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> x <span style=color:#f92672>+</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>grad <span style=color:#f92672>=</span> tape<span style=color:#f92672>.</span>gradient(y, x)
</span></span><span style=display:flex><span>grad
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#f92672>&lt;</span>tf<span style=color:#f92672>.</span>Tensor: shape<span style=color:#f92672>=</span>(), dtype<span style=color:#f92672>=</span>float32, numpy<span style=color:#f92672>=</span><span style=color:#ae81ff>2.0</span><span style=color:#f92672>&gt;</span>
</span></span></code></pre></div><h3 id=實作一個簡單的模型>實作一個簡單的模型<a hidden class=anchor aria-hidden=true href=#實作一個簡單的模型>#</a></h3><h4 id=dense-layer>Dense Layer<a hidden class=anchor aria-hidden=true href=#dense-layer>#</a></h4><ul><li>作用是將 input 與 W 做點積，與 b 相加，再輸入激活函數 f</li><li>相當於 <code>y = f(wx+b)</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> tensorflow <span style=color:#66d9ef>as</span> tf
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>NaiveDense</span>:
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, input_size, output_size, activation):
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>activation <span style=color:#f92672>=</span> activation
</span></span><span style=display:flex><span>    w_shape <span style=color:#f92672>=</span> (input_size, output_size)
</span></span><span style=display:flex><span>    w_initial_value <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>uniform(w_shape, minval <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>, maxval <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>W <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>Variable(w_initial_value)
</span></span><span style=display:flex><span>    b_shape <span style=color:#f92672>=</span> (output_size, )
</span></span><span style=display:flex><span>    b_initial_value <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>zeros(b_shape)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>b <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>Variable(b_initial_value)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__call__</span>(self, inputs):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>activation(tf<span style=color:#f92672>.</span>matmul(inputs, self<span style=color:#f92672>.</span>W) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>@property</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>weights</span>(self):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> [self<span style=color:#f92672>.</span>W, self<span style=color:#f92672>.</span>b]
</span></span></code></pre></div><h4 id=sequential>Sequential<a hidden class=anchor aria-hidden=true href=#sequential>#</a></h4><ul><li>Sequential 用來定義層與層之間連接的方式</li><li>在次簡單的用「依序」的方式進行正向傳播</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>NaiveSequential</span>:
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, layers):
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>layers <span style=color:#f92672>=</span> layers
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__call__</span>(self, inputs):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> inputs
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>layers:
</span></span><span style=display:flex><span>      x <span style=color:#f92672>=</span> layer(x)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>@property</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>weights</span>(self):
</span></span><span style=display:flex><span>    weights <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>layers:
</span></span><span style=display:flex><span>      weights <span style=color:#f92672>+=</span> layer<span style=color:#f92672>.</span>weights
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> weights
</span></span></code></pre></div><h4 id=build-model>Build Model<a hidden class=anchor aria-hidden=true href=#build-model>#</a></h4><ul><li>通過上面兩個實作，便可以簡單建立一個 Keras 雙層模型：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#f92672>=</span> NaiveSequential([
</span></span><span style=display:flex><span>    NaiveDense(input_size<span style=color:#f92672>=</span><span style=color:#ae81ff>28</span><span style=color:#f92672>*</span><span style=color:#ae81ff>28</span>, output_size<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>, activation<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>relu),
</span></span><span style=display:flex><span>    NaiveDense(input_size<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>, output_size<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, activation<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>softmax)
</span></span><span style=display:flex><span>])
</span></span></code></pre></div><h4 id=batch-generator>Batch Generator<a hidden class=anchor aria-hidden=true href=#batch-generator>#</a></h4><ul><li>我們需要用「小批次」的方式來迭代，能更有效率的訓練資料：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>BatchGeneartor</span>:
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, images, labels, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> len(images) <span style=color:#f92672>==</span> len(labels)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>index <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>images <span style=color:#f92672>=</span> images
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>labels <span style=color:#f92672>=</span> labels
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>batch_size <span style=color:#f92672>=</span> batch_size
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>num_batches <span style=color:#f92672>=</span> math<span style=color:#f92672>.</span>ceil(len(images) <span style=color:#f92672>/</span> batch_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>next</span>(self):
</span></span><span style=display:flex><span>    images <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>images[self<span style=color:#f92672>.</span>index : self<span style=color:#f92672>.</span>index <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>batch_size]
</span></span><span style=display:flex><span>    labels <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>labels[self<span style=color:#f92672>.</span>index : self<span style=color:#f92672>.</span>index <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>batch_size]
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>index <span style=color:#f92672>+=</span> self<span style=color:#f92672>.</span>batch_size
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> images, labels
</span></span></code></pre></div><h4 id=update-weights>Update Weights<a hidden class=anchor aria-hidden=true href=#update-weights>#</a></h4><ul><li>接下來我要透過 \(\text{w&rsquo;}=\text{w}-\eta\frac{dL}{d\text{W}}\) 來更新 W：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update_weights</span>(gradients, weights, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-3</span>):
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>for</span> g, w <span style=color:#f92672>in</span> zip(gradients, weights):
</span></span><span style=display:flex><span>    w<span style=color:#f92672>.</span>assign_sub(g <span style=color:#f92672>*</span> learning_rate)
</span></span></code></pre></div><ul><li>事實上我們不會這樣更新權重，我們可以借用 keras 的 optimizers</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras <span style=color:#f92672>import</span> optimizers
</span></span><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> optimizers<span style=color:#f92672>.</span>SGD(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-3</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update_weights</span>(gradients, weights):
</span></span><span style=display:flex><span>  optimizer<span style=color:#f92672>.</span>apply_gradients(zip(gradients, weights))
</span></span></code></pre></div><h4 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h4><ul><li>有了以上的工具，我們就可以進行訓練了</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>one_training_step</span>(model, images_batch, labels_batch):
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>GradientTape() <span style=color:#66d9ef>as</span> tape:
</span></span><span style=display:flex><span>    <span style=color:#75715e># y&#39; = f(wx+b)</span>
</span></span><span style=display:flex><span>    predictions <span style=color:#f92672>=</span> model(images_batch)    
</span></span><span style=display:flex><span>    <span style=color:#75715e># loss[] = y&#39; - y_true</span>
</span></span><span style=display:flex><span>    per_sample_losses <span style=color:#f92672>=</span> (tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>losses<span style=color:#f92672>.</span>sparse_categorical_crossentropy   (labels_batch, predictions))    
</span></span><span style=display:flex><span>    <span style=color:#75715e># avg_loss = loss</span>
</span></span><span style=display:flex><span>    average_loss <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>reduce_mean(per_sample_losses)[] <span style=color:#f92672>/</span> batch_size
</span></span><span style=display:flex><span>    <span style=color:#75715e># g = grad(L, w)</span>
</span></span><span style=display:flex><span>    gradients <span style=color:#f92672>=</span> tape<span style=color:#f92672>.</span>gradient(average_loss, model<span style=color:#f92672>.</span>weights)  
</span></span><span style=display:flex><span>    <span style=color:#75715e># w&#39; = w-ηg</span>
</span></span><span style=display:flex><span>    update_weights(gradients, model<span style=color:#f92672>.</span>weights)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> average_loss
</span></span></code></pre></div><ul><li>fit</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(model, images, labels, epochs, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>for</span> epoch_counter <span style=color:#f92672>in</span> range(epochs):
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Epoch </span><span style=color:#e6db74>{</span>epoch_counter<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    batch_generator <span style=color:#f92672>=</span> BatchGeneartor(images, labels)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> batch_counter <span style=color:#f92672>in</span> range(batch_generator<span style=color:#f92672>.</span>num_batches):
</span></span><span style=display:flex><span>      images_batch, labels_batch <span style=color:#f92672>=</span> batch_generator<span style=color:#f92672>.</span>next()
</span></span><span style=display:flex><span>      loss <span style=color:#f92672>=</span> one_training_step(model, images_batch, labels_batch)
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> batch_counter <span style=color:#f92672>%</span> <span style=color:#ae81ff>100</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;loss at batch </span><span style=color:#e6db74>{</span>batch_counter<span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span>loss<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><ul><li>執行</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.datasets <span style=color:#f92672>import</span> mnist
</span></span><span style=display:flex><span>(train_images, train_labels),(test_images, test_labels) <span style=color:#f92672>=</span> mnist<span style=color:#f92672>.</span>load_data()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_images <span style=color:#f92672>=</span> train_images<span style=color:#f92672>.</span>reshape((<span style=color:#ae81ff>60000</span>, <span style=color:#ae81ff>28</span><span style=color:#f92672>*</span><span style=color:#ae81ff>28</span>))
</span></span><span style=display:flex><span>train_images <span style=color:#f92672>=</span> train_images<span style=color:#f92672>.</span>astype(<span style=color:#e6db74>&#34;float32&#34;</span>) <span style=color:#f92672>/</span> <span style=color:#ae81ff>255</span>
</span></span><span style=display:flex><span>test_images <span style=color:#f92672>=</span> test_images<span style=color:#f92672>.</span>reshape((<span style=color:#ae81ff>10000</span>, <span style=color:#ae81ff>28</span><span style=color:#f92672>*</span><span style=color:#ae81ff>28</span>))
</span></span><span style=display:flex><span>test_images <span style=color:#f92672>=</span> test_images<span style=color:#f92672>.</span>astype(<span style=color:#e6db74>&#34;float32&#34;</span>) <span style=color:#f92672>/</span> <span style=color:#ae81ff>255</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fit(model, train_images, train_labels, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>)
</span></span></code></pre></div><h4 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h4><ul><li>最後我們再投入 test_images 來評估我們 training 的結果</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>predictions <span style=color:#f92672>=</span> model(test_images)
</span></span><span style=display:flex><span>predictions <span style=color:#f92672>=</span> predictions<span style=color:#f92672>.</span>numpy()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>predicted_labels <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argmax(predictions, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>matches <span style=color:#f92672>=</span> predicted_labels <span style=color:#f92672>==</span> test_labels
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;accuracy: </span><span style=color:#e6db74>{</span>matches<span style=color:#f92672>.</span>mean()<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://intervalrain.github.io/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://intervalrain.github.io/ai/2_2/><span class=title>« 上一頁</span><br><span>[AI] 2-2. 張量 Tensor</span>
</a><a class=next href=https://intervalrain.github.io/ai/3_1/><span class=title>下一頁 »</span><br><span>[AI] 3-1. TensorFlow 介紹</span></a></nav><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://intervalrain.github.io/>Rain Hu's Workspace</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="複製";function s(){t.innerHTML="已複製！",setTimeout(()=>{t.innerHTML="複製"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>