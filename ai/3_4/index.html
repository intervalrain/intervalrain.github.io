<!doctype html><html lang=zh-tw dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[AI] 3-4. 線性迴歸 | Rain Hu's Workspace</title>
<meta name=keywords content="AI"><meta name=description content="The introduction to linear regression"><meta name=author content="Rain Hu"><link rel=canonical href=https://intervalrain.github.io/><meta name=google-site-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.662816b9df27c772d2b97c5f5f6bf4f2c5531051a330015f0ad4135736d0e56a.css integrity="sha256-ZigWud8nx3LSuXxfX2v08sVTEFGjMAFfCtQTVzbQ5Wo=" rel="preload stylesheet" as=style><link rel=icon href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=16x16 href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=32x32 href=https://intervalrain.github.io/images/rain.png><link rel=apple-touch-icon href=https://intervalrain.github.io/images/rain.png><link rel=mask-icon href=https://intervalrain.github.io/images/rain.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh-tw href=https://intervalrain.github.io/ai/3_4/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css integrity=sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js integrity=sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script src=https://utteranc.es/client.js repo=intervalrain.github.io issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script><meta property="og:url" content="https://intervalrain.github.io/ai/3_4/"><meta property="og:site_name" content="Rain Hu's Workspace"><meta property="og:title" content="[AI] 3-4. 線性迴歸"><meta property="og:description" content="The introduction to linear regression"><meta property="og:locale" content="zh-tw"><meta property="og:type" content="article"><meta property="article:section" content="ai"><meta property="article:published_time" content="2024-12-19T15:01:12+08:00"><meta property="article:modified_time" content="2024-12-19T15:01:12+08:00"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="[AI] 3-4. 線性迴歸"><meta name=twitter:description content="The introduction to linear regression"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"[AI] 3-4. 線性迴歸","item":"https://intervalrain.github.io/ai/3_4/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[AI] 3-4. 線性迴歸","name":"[AI] 3-4. 線性迴歸","description":"The introduction to linear regression","keywords":["AI"],"articleBody":"目標 機器學習的目標有很多種，參考李宏毅教授的機器學習課程，可以用下面一張圖來概述。 Task 代表機器學習的目標 Regression: 透過迴歸來預測值。 Classification: 處理分類問題。 Structed Learning: 生成結構化的資訊(現在稱為生成式 AI, GenAI) Scenario 代表解決問題的策略 Supervised Learning: 使用已標記的訓練數據進行訓練 Semi-supervised Learning: 使用有標記與無標記的訓練數據進行訓練 Unsupervised Learning: 不使用標記的訓練數據進行訓據，由模型自行發現模式與結構 Reinforcement Learning: 透過「獎勵」與「懲罰」來學習。 Transfer Learning: 將一個任務學習到的知識應用到相關的新任務 Method 指應用的方法 Linear Model Deep Learning SVM Decision Tree KNN 線性迴歸 暴力解 假設我們大概知道答案的區間，我們可以暴力求解，將每一個 w, b 代入求最小的 (w, b) 組合 這個方法的缺點是，計算量很大，且我們求值的方式不是連續的，精準度不夠。 import sys areas = data[:,0] prices = data[:,1] def compute_loss(y_pred, y): return (y_pred - y)**2 best_w = 0. best_b = 0. min_loss = sys.float_info.max # 猜 w=30-50, step = 0.1 # 猜 b=200-600 step = 1 for i in range(200): for j in range(400): w = 30 + i*0.1 b = 200 + j*1 loss = 0. for area, price in zip(areas, prices): y_pred = w * area + b loss += compute_loss(y_pred, price) if loss \u003c min_loss: min_loss = loss best_w = w best_b = b w=35.1 b=599 線性代數解法 假如我們學過線性代數，我們想得到它的歸性迴歸方程式，我們的作法會是： 設迴歸方程式為 $$\\text{y}=\\text{wx}+\\text{b}\\quad\\quad (1)$$\n我們要求最小平方差 $$\\text{L}=\\sum_{i=0}^n(\\text{y}_i-\\text{y})^2\\quad\\quad (2)$$\n將 (1) 代入 (2)\n$$\\text{L}=\\sum_{i=0}^n(\\text{y}_i-\\text{wx}-\\text{b})^2\\quad\\quad (3)$$\n學過線性代數，我們知道要求極值，可以對其求導數為0，並設 w 與 b 互不為函數，故我們對其個別做偏微分等於0。 $$\\frac{\\partial\\text{L}}{\\partial\\text{w}}=0$$\n$$\\frac{\\partial\\text{L}}{\\partial\\text{b}}=0$$\n對 b 做偏微分 $$\\frac{\\partial\\text{L}}{\\partial\\text{b}}=-2\\sum_{i=0}^n(\\text{y}_i-\\text{wx}_i-\\text{b})=0$$\n$$\\sum_{i=0}^n(\\text{y}_i-\\text{wx}_i-\\text{b})=0$$\n$$\\sum_{i=0}^n\\text{y}_i-\\text{w}\\sum _{i=0}^n\\text{x}_i-\\text{nb}=0$$\n$$\\text{n}\\bar{\\text{y}}-\\text{n}\\bar{\\text{wx}}-\\text{nb}=0$$\n$$\\text{b}=\\bar{\\text{y}}-\\text{w}\\bar{\\text{x}}\\quad\\quad (4)$$\n對 w 做偏微分 $$\\frac{\\partial\\text{L}}{\\partial\\text{w}}=-2\\sum_{i=0}^n\\text{x}_i(\\text{y}_i-\\text{wx}_i-\\text{b})=0$$\n$$\\sum_{i=0}^n\\text{x}_i(\\text{y}_i-\\text{wx}_i-\\text{b})=0$$\n$$\\sum_{i=0}^n\\text{x}_i\\text{y}_i-\\text{w}\\sum _{i=0}^n\\text{x}_i^2-\\text{b}\\sum _{i=0}^n\\text{x}_i=0$$\n代入 (4) $$\\sum_{i=0}^n\\text{x}_i\\text{y}_i-\\text{w}\\sum _{i=0}^n\\text{x}_i^2-(\\bar{\\text{y}}-\\text{w}\\bar{\\text{x}})\\sum _{i=0}^n\\text{x}_i=0$$\n$$\\sum_{i=0}^n\\text{x}_i\\text{y}_i-\\text{w}\\sum _{i=0}^n\\text{x}_i^2-\\bar{\\text{y}}\\sum _{i=0}^n\\text{x}_i+\\text{w}\\sum _{i=0}^n\\text{x}_i\\bar{\\text{x}}=0$$\n$$\\text{w}(\\sum _{i=0}^n\\text{x}_i\\bar{\\text{x}}-\\sum _{i=0}^n\\text{x}_i^2)=\\bar{\\text{y}}\\sum _{i=0}^n\\text{x}_i-\\sum _{i=0}^n\\text{x}_i\\text{y}_i$$\n$$\\text{w}(\\text{n}\\bar{\\text{x}}^2-\\sum _{i=0}^n\\text{x}_i^2)=\\text{n}\\bar{\\text{x}}\\bar{\\text{y}}-\\sum _{i=0}^n\\text{x}_i\\text{y}_i$$\n$$\\text{w}=\\frac{\\sum\\text{x}_i\\text{y}_i-\\text{n}\\bar{\\text{x}}\\bar{\\text{y}}}{\\sum\\text{x}_i^2-\\text{n}\\bar{\\text{x}}^2}$$\n$$\\text{w}=\\frac{\\sum\\text{y}_i(\\text{x}_i-\\bar{\\text{x}})}{\\sum\\text{x}_i(\\text{x}_i-\\bar{\\text{x}})}$$\n$$\\text{w}=\\frac{\\sum(\\text{y}-\\bar{\\text{y}})(\\text{x}-\\bar{\\text{x}})}{\\sum(\\text{x}_i-\\bar{\\text{x}})^2}$$\n$$\\text{w}=\\frac{S_{XY}}{S_{XX}}\\quad\\quad(5)$$\n換言之，我們可以透過 (4) 與 (5) 式直接求得迴歸方程式 $$\\text{y}=\\frac{S_{XY}}{S_{XX}}\\text{x}+(\\bar{\\text{y}}-\\frac{S_{XY}}{S_{XX}}\\bar{\\text{x}})$$\n其中\n$$S_{XY}=\\sum(\\text{x}_i-\\bar{\\text{x}})(\\text{y}_i-\\bar{\\text{y}})=\\sum\\text{x}_i\\text{y}_i-\\text{n}\\bar{\\text{x}}\\bar{\\text{y}}$$\n$$S_{XX}=\\sum(\\text{x}_i-\\bar{\\text{x}})^2=\\sum\\text{x}_i^2-\\text{n}\\bar{\\text{x}}^2$$\n直接運用於 sample:\nimport matplotlib.pyplot as plt meanx = data[:, 0].mean() meany = data[:, 1].mean() sxy = 0.0 sxx = 0.0 for i in range(data.shape[0]): sxy += (data[i,0] - meanx)*(data[i,1] - meany) sxx += (data[i,0] - meanx)**2 w = sxy/sxx b = meany - w*meanx plt.figure(figsize=(10, 6)) plt.scatter(data[:, 0], data[:, 1], alpha=0.5) plt.plot(data[:, 0], w*data[:, 0] + b, color='red', label='Regression Line') plt.xlabel('Size (ping)') plt.ylabel('Total Price (10k)') plt.title('House Price versus House Size') plt.legend() plt.grid(True) plt.show() print(f\"w = {w:.4f}\") print(f\"b = {b:.4f}\") w = 34.9738 b = 602.5411 梯度下降(gradient descent) 但事實上，在機器學習的領域要處理的不一定是上述這種只有兩維的問題，多維的問題會有多個梯度為0的地方，代表我們需要求出全部梯度為0的地方，再逐一代入我們的 loss function，最後找出 loss 最小的一組答案。 再者是，加入 activation function 後的方程式，變得並非上述案例中的容易微分。 import numpy as np import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt from matplotlib import cm # 1. 資料正規化函數 def normalize_data(data): return (data - np.mean(data, axis=0)) / np.std(data, axis=0) # 2. 建立並訓練模型的函數 def train_linear_regression(x_norm, y_norm, learning_rate=0.01, epochs=10): # 建立模型 model = keras.Sequential([ keras.layers.Dense(1, input_shape=(1,)) ]) # 編譯模型 optimizer = keras.optimizers.SGD(learning_rate=learning_rate) model.compile(optimizer=optimizer, loss='mse') # 用於記錄訓練過程的參數 history = {'w': [], 'b': [], 'loss': []} class ParameterHistory(keras.callbacks.Callback): def on_epoch_begin(self, epoch, logs=None): w = self.model.layers[0].get_weights()[0][0][0] b = self.model.layers[0].get_weights()[1][0] loss = self.model.evaluate(x_norm, y_norm, verbose=0) history['w'].append(w) history['b'].append(b) history['loss'].append(loss) # 訓練模型 parameter_history = ParameterHistory() model.fit(x_norm, y_norm, epochs=epochs, verbose=0, callbacks=[parameter_history]) # 記錄最後一次的參數 w = model.layers[0].get_weights()[0][0][0] b = model.layers[0].get_weights()[1][0] loss = model.evaluate(x_norm, y_norm, verbose=0) history['w'].append(w) history['b'].append(b) history['loss'].append(loss) return model, history # 3. 視覺化函數 def plot_training_process(x_raw, y_raw, x_norm, y_norm, history): # 創建圖表 fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6)) # 將正規化的係數轉換回原始尺度 w_raw_history = [w * np.std(y_raw) / np.std(x_raw) for w in history['w']] b_raw_history = [(b * np.std(y_raw) + np.mean(y_raw) - w * np.std(y_raw) * np.mean(x_raw) / np.std(x_raw)) for w, b in zip(history['w'], history['b'])] # Contour plot with raw scale margin_w = (max(w_raw_history) - min(w_raw_history)) * 0.5 margin_b = (max(b_raw_history) - min(b_raw_history)) * 0.5 w_raw_range = np.linspace(min(w_raw_history)-margin_w, max(w_raw_history)+margin_w, 100) b_raw_range = np.linspace(min(b_raw_history)-margin_b, max(b_raw_history)+margin_b, 100) W_RAW, B_RAW = np.meshgrid(w_raw_range, b_raw_range) Z = np.zeros_like(W_RAW) # 計算每個點的 MSE（在原始尺度上） for i in range(W_RAW.shape[0]): for j in range(W_RAW.shape[1]): y_pred = W_RAW[i,j] * x_raw + B_RAW[i,j] Z[i,j] = np.mean((y_pred - y_raw) ** 2) CS = ax1.contour(W_RAW, B_RAW, Z, levels=20) ax1.clabel(CS, inline=True, fontsize=8) ax1.plot(w_raw_history, b_raw_history, 'r.-', label='Training path') ax1.set_xlabel('w (原始尺度)') ax1.set_ylabel('b (原始尺度)') ax1.set_title('Contour Plot with Training Path (原始尺度)') ax1.legend() # Raw data scatter plot with regression lines ax2.scatter(x_raw, y_raw, alpha=0.5, label='Raw data') ax2.set_ylim(700, 2300) # 繪製每一輪的回歸線 x_plot = np.linspace(min(x_raw), max(x_raw), 100) colors = cm.rainbow(np.linspace(0, 1, len(w_raw_history))) for i, (w, b) in enumerate(zip(w_raw_history, b_raw_history)): y_plot = w * x_plot + b ax2.plot(x_plot, y_plot, color=colors[i], alpha=0.3) ax2.set_xlabel('Area (坪)') ax2.set_ylabel('Price (萬)') ax2.set_title('Raw Data with Regression Lines') plt.tight_layout() plt.show() # 載入數據 data = load_data() x_raw, y_raw = data[:, 0], data[:, 1] # 轉換為 TensorFlow 格式 x_raw = x_raw.reshape(-1, 1) y_raw = y_raw.reshape(-1, 1) # 正規化數據 x_norm = normalize_data(x_raw) y_norm = normalize_data(y_raw) # 訓練模型 model, history = train_linear_regression(x_norm, y_norm) # 視覺化結果 plot_training_process(x_raw.flatten(), y_raw.flatten(), x_norm.flatten(), y_norm.flatten(), history) # 輸出最終結果 final_w = history['w'][-1] final_b = history['b'][-1] final_loss = history['loss'][-1] # 將係數轉換回原始尺度 w_raw = final_w * np.std(y_raw) / np.std(x_raw) b_raw = (final_b * np.std(y_raw) + np.mean(y_raw) - final_w * np.std(y_raw) * np.mean(x_raw) / np.std(x_raw)) print(f\"Final equation: y = {w_raw[0]:.2f}x + {b_raw[0]:.2f}\") print(f\"Final normalized loss: {final_loss:.6f}\") 批次訓練(Batch)的概念 我們可以在不同的時機點來更新 w 與 b，假設我們的訓練次數為 3000，那 epochs 為3000。且樣本數為 1000。 批次訓練(batch training)，代表的是我們總共做 3000 次的更新，每次都是利用全部 1000 筆樣本算出來的 dw 與 db 去做調整。 SGD(Stochastic gradient descent)，代表我們做 3000 * 1000 次的更新，每一個樣本計算出 dw 與 db 就立即去調整。 小批次訓練(mini-batch training)則是介於批次訓練與 SGD 之間，假設我們每 200 個樣本做一次更新，實際上會做 3000 * 5 次更新。 import numpy as np import matplotlib.pyplot as plt class LinearRegression: def __init__(self, learning_rate=0.0000001): self.w = 0.0 self.b = 0.0 self.lr = learning_rate self.loss_history = [] def predict(self, X): return self.w * X + self.b def compute_loss(self, X, y): y_pred = self.predict(X) return np.mean((y_pred - y) ** 2) def compute_gradients(self, X, y): y_pred = self.predict(X) error = y_pred - y dw = np.mean(2 * error * X) db = np.mean(2 * error) return dw, db def train_batch(self, X, y, epochs=3000): \"\"\"Full batch gradient descent\"\"\" for epoch in range(epochs): # Compute gradients using all data dw, db = self.compute_gradients(X, y) # Update parameters self.w -= self.lr * dw self.b -= self.lr * db # Record loss if epoch % 100 == 0: loss = self.compute_loss(X, y) self.loss_history.append(loss) print(f\"Epoch {epoch}, Loss: {loss:.2f}\") def train_mini_batch(self, X, y, batch_size=2, epochs=3000): \"\"\"Mini-batch gradient descent\"\"\" n_samples = len(X) for epoch in range(epochs): # Shuffle the data indices = np.random.permutation(n_samples) X_shuffled = X[indices] y_shuffled = y[indices] # Mini-batch training for i in range(0, n_samples, batch_size): X_batch = X_shuffled[i:i+batch_size] y_batch = y_shuffled[i:i+batch_size] # Compute gradients using batch data dw, db = self.compute_gradients(X_batch, y_batch) # Update parameters self.w -= self.lr * dw self.b -= self.lr * db # Record loss for the whole dataset if epoch % 100 == 0: loss = self.compute_loss(X, y) self.loss_history.append(loss) print(f\"Epoch {epoch}, Loss: {loss:.2f}\") def train_sgd(self, X, y, epochs=3000): \"\"\"Stochastic gradient descent\"\"\" n_samples = len(X) for epoch in range(epochs): # Shuffle the data indices = np.random.permutation(n_samples) X_shuffled = X[indices] y_shuffled = y[indices] # SGD training (batch_size = 1) for i in range(n_samples): X_sample = X_shuffled[i:i+1] y_sample = y_shuffled[i:i+1] # Compute gradients using single sample dw, db = self.compute_gradients(X_sample, y_sample) # Update parameters self.w -= self.lr * dw self.b -= self.lr * db # Record loss for the whole dataset if epoch % 100 == 0: loss = self.compute_loss(X, y) self.loss_history.append(loss) print(f\"Epoch {epoch}, Loss: {loss:.2f}\") (areas, prices) = load_data() models = { 'Batch': LinearRegression(learning_rate=1e-7), 'Mini-batch': LinearRegression(learning_rate=1e-7), 'SGD': LinearRegression(learning_rate=5e-8) } models['Batch'].train_batch(areas, prices) models['Mini-batch'].train_mini_batch(areas, prices) models['SGD'].train_sgd(areas, prices) plt.figure(figsize=(10, 6)) for name, model in models.items(): plt.plot(range(0, 3000, 100), model.loss_history, label=name) plt.xlabel('Epoch') plt.ylabel('Loss') plt.title('Loss Comparison') plt.legend() plt.grid(True) plt.show() for name, model in models.items(): print(f\"\\n{name} Results:\") print(f\"w = {model.w:.6f}\") print(f\"b = {model.b:.6f}\") print(f\"Final Loss = {model.loss_histroy[-1]:.2f}\") 從更新次數來看，SGD 的更新次數 \u003e 小批次訓練 \u003e 批次訓練，SGD 所耗的時間同樣也比小批次訓練與批次訓練長，但實際上 loss 收斂的情形也比較好嗎？\n從下圖比較可見，收斂情況最佳的反而是小批次訓練，我們比較三種方法，總結一下成果： 批次訓練 Batch Gradient Descent (BGD)\n每次更新使用所有數據 穩定但計算量大 容易找到局部最優解 小批次訓練 Mini-batch Gradient Descent\n每次使用一小批數據 平衡了計算效率和更新穩定性 常用於實際應用 Stochastic Gradient Descent (SGD)\n每次只使用一個樣本 更新頻繁，收斂較快但不穩定 需要較小的學習率 損失函數(loss function) 損失函數是用來衡量模型預測值與實際值之間差異的函數，以下是幾個常見的損失函數：\n均方誤差(Mean Squared Error, MSE) 常用於迴歸問題 對異常值敏感 $$ \\text{MSE}=\\frac{1}{\\text{n}}\\sum^n(\\text{y}_\\text{pred}-\\text{y} _\\text{true})^2 $$ 交叉熵損失(Cross Entropy Loss) 常用於分類問題 衡量兩個概率分布之間的差異 又分為二元交叉熵和多類別交叉熵 二元交叉熵(Binary Cross Entropy) $$ \\text{BCE}=-(\\text{y}_\\text{true}\\times \\log(\\text{y} _\\text{pred})+(1-\\text{y} _\\text{true})\\times \\log(1-\\text{y} _\\text{pred})) $$ 多類別交叉熵(Categorical Cross Entropy) $$ \\text{CCE}=-\\sum^n((\\text{y} _\\text{true})_i\\times\\log((\\text{y} _\\text{true})_i)) $$ 平均絕對誤差(Mean Absolute Error, MAE) 用於迴歸問題 相較 MSE 對異常值不那麼敏感 $$ \\text{MAE}=\\frac{1}{\\text{n}}\\sum^n|\\text{y} _\\text{pred}-\\text{y} _\\text{true}| $$ Hinge Loss 主要用於支持向量機(SVM) 特別適合最大間隔分類問題 $$ \\text{HL} = \\max(0, 1-\\text{y} _\\text{pred}\\times\\text{y} _\\text{true}) $$ 在選擇損失函數時需考慮\n問題類型(分類還是迴歸) 數據分布特性 對異常值的敏感度要求 模型的收斂速度要求 L1/L2 正則化(L1/L2 Regularization) 在考慮有多個特徵、且帶有 outlier 或雜訊時\nL1 正則化 (Lasso Regression)\nLasso (Least Absolute Shrinkage and Selection Operator) 定義：在損失函數中加入參數的絕對值項 $$ \\text{Loss} = \\text{MSE} + \\lambda \\times \\sum|w| $$ 特點： 傾向於產生稀疏解（某些參數會變成0） 適合用於特徵選擇 對異常值較不敏感 L2 正則化 (Ridge Regression)\n定義：在損失函數中加入參數的平方項 $$ \\text{Loss} = \\text{MSE} + \\lambda \\times \\sum(w^2) $$ 特點： 傾向於使所有參數值變小但不為0 計算導數較簡單 對共線性（多重共線性）問題有好處 import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split # 生成合成數據 np.random.seed(42) def generate_synthetic_data(n_samples=100): # 生成基本特徵 X1 = np.random.normal(0, 1, n_samples) # 面積 X2 = np.random.normal(0, 1, n_samples) # 房齡 # 生成共線性特徵（與面積高度相關的特徵，如房間數） X3 = 0.8 * X1 + 0.2 * np.random.normal(0, 1, n_samples) # 生成噪音特徵（完全無關的特徵） X4 = np.random.normal(0, 1, n_samples) # 組合特徵 X = np.column_stack([X1, X2, X3, X4]) # 生成目標值（房價） # 主要由X1和X2決定，X3有少許影響，X4完全不影響 y = 3 * X1 + 2 * X2 + 0.5 * X3 + np.random.normal(0, 0.1, n_samples) return X, y class RegularizedRegression: def __init__(self, learning_rate=1e-7, reg_type='l2', lambda_reg=0.1): self.w = 0. self.b = 0. self.lr = learning_rate self.reg_type = reg_type self.lambda_reg = lambda_reg self.loss_history = [] def predict(self, X): return self.w * X + self.b def compute_loss(self, X, y): y_pred = self.predict(X) mse = np.mean((y_pred - y) ** 2) if self.reg_type == 'l1': reg_term = self.lambda_reg * np.abs(self.w) elif self.reg_type == 'l2': reg_term = self.lambda_reg * (self.w ** 2) else: reg_term = 0 return mse + reg_term def compute_gradients(self, X, y): y_pred = self.predict(X) error = y_pred - y # mse 的梯度 dw_mse = np.mean(2 * error * X) db = np.mean(2 * error) # 正則化項的梯度 if self.reg_type == 'l1': dw_reg = self.lambda_reg * np.sign(self.w) elif self.reg_type == 'l2': dw_reg = self.lambda_reg * 2 * self.w else: dw_reg = 0 dw = dw_mse + dw_reg return dw, db def train(self, X, y, epochs=3000): for epoch in range(epochs): dw, db = self.compute_gradients(X, y) self.w -= self.lr * dw self.b -= self.lr * db def train(self, X, y, batch_size=None, epochs=3000): n_samples = len(X) if batch_size is None: batch_size = n_samples for epoch in range(epochs): # Shuffle the data indices = np.random.permutation(n_samples) X_shuffled = X[indices] y_shuffled = y[indices] # Mini-batch training for i in range(0, n_samples, batch_size): X_batch = X_shuffled[i:i+batch_size] y_batch = y_shuffled[i:i+batch_size] # Compute gradients using batch data dw, db = self.compute_gradients(X_batch, y_batch) # Update parameters self.w -= self.lr * dw self.b -= self.lr * db # Record loss for the whole dataset if epoch % 100 == 0: loss = self.compute_loss(X, y) self.loss_history.append(loss) print(f\"Epoch {epoch}, Loss: {loss:.2f}\") 觀察重點：\nL1 正則化（Lasso）： 傾向於將不重要的特徵（如 Noise）權重設為 0 在有共線性的特徵中選擇一個（Area vs Rooms） L2 正則化（Ridge）： 所有權重都被縮小 共線性特徵的權重會被平均分配 無正則化： 可能過度擬合噪音 在共線性特徵上表現不穩定 從結果可以看出：\nL1 正則化確實將無關特徵（Noise）的權重降到接近 0 L2 正則化讓所有權重都變得更小，但保持了相對重要性 無正則化的模型權重更大，更容易受噪音影響 主要的差異和實作細節：\n正則化項的加入 L1：在損失函數中加入 λ * |w| L2：在損失函數中加入 λ * w² 梯度計算 L1 的梯度：sign(w) * λ L2 的梯度：2 * λ * w 超參數 λ (lambda_reg) 控制正則化的強度 較大的 λ 會產生較小的權重 需要通過交叉驗證來選擇適當的值 使用場景 L1：特徵選擇，當你認為只有部分特徵是重要的 L2：處理共線性，當特徵之間有相關性 激活函數(activation function) 在設計 model 時，不是所有的問題都可以用線性模型來描述，此時我們就需要\n多層結構(至少一個隱藏層) 非線性激活函數 以下為三個重要的激活函數 Sigmoid (σ(x) = 1/(1+e^(-x)))\n輸出範圍：(0,1) 優點：適合二分類問題 缺點：容易出現梯度消失 ReLU (max(0,x))\n輸出範圍：[0,∞) 優點：計算簡單，不會有梯度消失 缺點：Dead ReLU 問題 Tanh (tanh(x))\n輸出範圍：(-1,1) 優點：零中心化 缺點：也有梯度消失問題 import numpy as np import matplotlib.pyplot as plt class Activation: @staticmethod def sigmoid(x): return 1 / (1 + np.exp(-x)) @staticmethod def sigmoid_derivative(x): sx = Activation.sigmoid(x) return sx * (1 - sx) @staticmethod def relu(x): return np.maximum(0, x) @staticmethod def relu_derivative(x): return np.where(x \u003e 0, 1, 0) @staticmethod def tanh(x): return np.tanh(x) @staticmethod def tanh_derivative(x): return 1 - np.tanh(x) ** 2 以 xor 為例來做以下的機器學習 import numpy as np import matplotlib.pyplot as plt class Activation: @staticmethod def sigmoid(x): return 1 / (1 + np.exp(-x)) @staticmethod def sigmoid_derivative(x): sx = Activation.sigmoid(x) return sx * (1 - sx) @staticmethod def relu(x): return np.maximum(0, x) @staticmethod def relu_derivative(x): return np.where(x \u003e 0, 1, 0) @staticmethod def tanh(x): return np.tanh(x) @staticmethod def tanh_derivative(x): return 1 - np.tanh(x)**2 class NeuralNetwork: def __init__(self, activation='sigmoid'): # 網絡架構: 2 -\u003e 4 -\u003e 1 self.W1 = np.random.randn(2, 4) * 0.1 # 輸入層到隱藏層的權重 self.b1 = np.zeros((1, 4)) # 隱藏層偏差 self.W2 = np.random.randn(4, 1) * 0.1 # 隱藏層到輸出層的權重 self.b2 = np.zeros((1, 1)) # 輸出層偏差 # 選擇激活函數 if activation == 'sigmoid': self.activation = Activation.sigmoid self.activation_derivative = Activation.sigmoid_derivative elif activation == 'relu': self.activation = Activation.relu self.activation_derivative = Activation.relu_derivative elif activation == 'tanh': self.activation = Activation.tanh self.activation_derivative = Activation.tanh_derivative self.loss_history = [] def forward(self, X): # 前向傳播 self.z1 = np.dot(X, self.W1) + self.b1 self.a1 = self.activation(self.z1) self.z2 = np.dot(self.a1, self.W2) + self.b2 self.a2 = self.activation(self.z2) return self.a2 def backward(self, X, y, learning_rate=0.1): m = X.shape[0] # 計算梯度 dz2 = self.a2 - y dW2 = np.dot(self.a1.T, dz2) / m db2 = np.sum(dz2, axis=0, keepdims=True) / m dz1 = np.dot(dz2, self.W2.T) * self.activation_derivative(self.z1) dW1 = np.dot(X.T, dz1) / m db1 = np.sum(dz1, axis=0, keepdims=True) / m # 更新權重 self.W2 -= learning_rate * dW2 self.b2 -= learning_rate * db2 self.W1 -= learning_rate * dW1 self.b1 -= learning_rate * db1 def train(self, X, y, epochs=10000, learning_rate=0.1): for epoch in range(epochs): # 前向傳播 output = self.forward(X) # 計算損失 loss = np.mean((output - y) ** 2) self.loss_history.append(loss) # 反向傳播 self.backward(X, y, learning_rate) if epoch % 1000 == 0: print(f\"Epoch {epoch}, Loss: {loss:.4f}\") def predict(self, X): return np.round(self.forward(X)) # 準備 XOR 數據 X = np.array([[0,0], [0,1], [1,0], [1,1]]) y = np.array([[0], [1], [1], [0]]) # 訓練不同激活函數的模型 activation_functions = ['sigmoid', 'relu', 'tanh'] models = {} for activation in activation_functions: print(f\"\\nTraining with {activation} activation:\") model = NeuralNetwork(activation=activation) model.train(X, y) models[activation] = model # 繪製損失曲線比較 plt.figure(figsize=(10, 6)) for activation, model in models.items(): plt.plot(model.loss_history, label=activation) plt.xlabel('Iteration') plt.ylabel('Loss') plt.title('Training Loss with Different Activation Functions') plt.legend() plt.grid(True) plt.show() # 測試預測結果 print(\"\\nPrediction Results:\") for activation, model in models.items(): print(f\"\\n{activation} activation:\") predictions = model.predict(X) for x, y_true, y_pred in zip(X, y, predictions): print(f\"Input: {x}, True: {y_true[0]}, Predicted: {y_pred[0]}\") # 視覺化決策邊界 plt.figure(figsize=(15, 5)) for i, (activation, model) in enumerate(models.items()): plt.subplot(1, 3, i+1) # 創建網格點 xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100), np.linspace(-0.5, 1.5, 100)) grid = np.c_[xx.ravel(), yy.ravel()] # 預測 Z = model.predict(grid) Z = Z.reshape(xx.shape) # 繪製決策邊界 plt.contourf(xx, yy, Z, alpha=0.4) plt.scatter(X[:, 0], X[:, 1], c=y, s=100) plt.title(f'{activation} Decision Boundary') plt.xlabel('Input 1') plt.ylabel('Input 2') plt.tight_layout() plt.show() Sigmoid 適用時機：\n二元分類問題的輸出層（因為輸出範圍是 0~1，適合表示機率） 需要將輸出限制在 0~1 之間的情境 較淺的網路（1-2層） 不建議用在：\n深層網路的中間層（因為容易發生梯度消失） 需要快速訓練的模型（因為計算exponential較慢） 對稱數據的問題（因為不是零中心化） ReLU 適用時機：\n深層網路的隱藏層（現代深度學習最常用） CNN（卷積神經網路） 需要快速訓練的大型網路 稀疏激活是可接受的場景 主要優點：\n計算簡單快速 能緩解梯度消失問題 能產生稀疏的表示（部分神經元輸出為0） Tanh 適用時機：\n需要零中心化輸出的場景（輸出範圍-1~1） RNN（循環神經網路）的隱藏層 數據本身是歸一化/標準化的情況 需要較強的梯度在接近零的區域 實際應用建議：\n常見的最佳實踐組合： class NeuralNetwork: def __init__(self): self.hidden_activation = ReLU # 隱藏層使用ReLU self.output_activation = Sigmoid # 二分類輸出層使用Sigmoid 根據任務選擇： 分類問題：輸出層用Sigmoid（二分類）或Softmax（多分類） 回歸問題：輸出層可以不用激活函數 特徵提取：中間層優先使用ReLU 特殊情況： 處理序列數據（如RNN）：優先考慮Tanh 處理圖像數據（如CNN）：優先考慮ReLU 如果ReLU表現不佳：可以嘗試LeakyReLU或ELU 其它變體 Leaky ReLu 適用時機： 當標準ReLU出現大量\"死亡\"神經元時 需要保留負值信息的場景 訓練初期希望網絡快速收斂 f(x) = x if x \u003e 0 else αx # (α通常為0.01) ELU (Exponential Linear Unit) 適用時機： 深層網絡需要更強的正則化 對噪聲較敏感的任務 需要更快收斂速度的場景 f(x) = x if x \u003e 0 else α(exp(x) - 1) SELU (Scaled ELU) 適用時機： 深層全連接網絡 需要自歸一化特性的場景 希望避免額外的批標準化層 f(x) = λ(x if x \u003e 0 else α(exp(x) - 1)) GELU(Gaussian Error Linear Unit) 適用時機： Transformer架構 BERT等預訓練模型 需要考慮輸入不確定性的場景 f(x) = x * P(X ≤ x) Swish 適用時機： 深層模型 需要更好泛化性能的場景 計算資源充足的情況 f(x) = x * sigmoid(βx) Summary 先嘗試 ReLU：\n最簡單且通常效果不錯 計算效率高 容易優化 如果遇到問題，按順序嘗試：\nDead ReLU問題 → Leaky ReLU 需要自歸一化 → SELU 用於Transformer → GELU 追求極致性能 → Swish 特殊情況：\n需要處理時序數據 → ELU或SELU 計算資源受限 → 堅持使用ReLU 特別關注梯度流動 → Leaky ReLU或ELU 優化器(Optimizers) 優化器是 backpropagation 時使用的更新策略，常見的優化器有：\n隨機梯度下降 SGD(Stochastic Gradient Descent) 最佳基本的優化器 適用時機： 數據量大且資源有限 問題較簡單 需要較好的泛化性能 class Optimizer: def __init__(self, learning_rate=0.01): self.lr = learning_rate def update(self, params, grads): raise NotImplementedError class SGD(Optimizer): def update(self, params, grads): for param, grad in zip(params, grads): param -= self.lr * grad return params 動量 Momemtum 加入動量項(動態調整學習率)，幫助越過局部最小值 適用時機： 梯度下降震盪嚴重時 需要加速收斂 有較多局部最小值時 class Momemtum(Optimizer): def __init__(self, learning_rate=0.01, momemtum=0.9): super().__init__(learning_rate) self.momemtum = momemtum self.velocities = None def update(self, params, grades): if self.velocities is None: self.velocities = [np.zeros_like(param) for param in params] for i (param, grad) in enumerate(zip(params, grads)): self.velocities[i] = self.momentum * self.velocities[i] - self.lr * grad param += self.velocities[i] return params 自適應學習率 RMSProp 動態調整學習率 使用時機： 處理非平穩問題 RNN訓練 梯度稀疏的問題 class RMSProp(Optimizer): def __init__(self, learning_rate=0.01, decay_rate=0.09, epsilon=1e-8): super().__init__(learning_rate) self.decay_rate = decay_rate self.epsilon = epsilon self.cache = None def update(self, params, grads): if self.cache is None: self.cache = [np.zeros_like(param) for param in params] for i, (param, grad) in enumerate(zip(params, grads)): self.cache[i] = self.decay_rate * self.cache[i] + (1 - self.decay_rate) * grad ** 2 param -= self.lr * grad / (np.sqrt(self.cache[i]) + self.epsilon) return params Adam 結合Momentum和RMSprop的優點 使用時機： 深度學習的默認選擇 需要快速收斂 大多數問題 class Adam(Optimizer): def __init__(self, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8): super().__init__(learning_rate) self.beta1 = beta1 self.beta2 = beta2 self.epsilon = epsilon self.m = None self.v = None self.t = 0 def update(self, params, grads): if self.m is None: self.m = [np.zeros_like(param) for param in params] self.v = [np.zeros_like(param) for param in params] self.t += 1 for i, (param, grad) in enumerate(zip(params, grads)): self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad**2 m_hat = self.m[i] / (1 - self.beta1**self.t) v_hat = self.v[i] / (1 - self.beta2**self.t) param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon) return params Summary 首選Adam： optimizer = Adam(learning_rate=0.001, beta1=0.9, beta2=0.999) 如果模型較大： optimizer = AdamW(learning_rate=0.001, weight_decay=0.01) 如果需要更好的泛化性能： optimizer = SGD(learning_rate=0.01, momentum=0.9) ","wordCount":"2637","inLanguage":"zh-tw","datePublished":"2024-12-19T15:01:12+08:00","dateModified":"2024-12-19T15:01:12+08:00","author":{"@type":"Person","name":"Rain Hu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://intervalrain.github.io/ai/3_4/"},"publisher":{"@type":"Organization","name":"Rain Hu's Workspace","logo":{"@type":"ImageObject","url":"https://intervalrain.github.io/images/rain.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://intervalrain.github.io/ accesskey=h title="Rain Hu's Workspace (Alt + H)"><img src=https://intervalrain.github.io/images/rain.png alt aria-label=logo height=35>Rain Hu's Workspace</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://intervalrain.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://intervalrain.github.io/aboutme title="About me"><span>About me</span></a></li><li><a href=https://intervalrain.github.io/archives title=Archives><span>Archives</span></a></li><li><a href=https://intervalrain.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://intervalrain.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://intervalrain.github.io/csharp/csharp title=C#><span>C#</span></a></li><li><a href=https://intervalrain.github.io/csindex title=CS><span>CS</span></a></li><li><a href=https://intervalrain.github.io/leetcode title=LeetCode><span>LeetCode</span></a></li><li><a href=https://intervalrain.github.io/ai title=AI><span>AI</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://intervalrain.github.io/>首頁</a></div><h1 class="post-title entry-hint-parent">[AI] 3-4. 線性迴歸</h1><div class=post-description>The introduction to linear regression</div><div class=post-meta><span title='2024-12-19 15:01:12 +0800 +0800'>December 19, 2024</span>&nbsp;·&nbsp;13 分鐘&nbsp;·&nbsp;Rain Hu&nbsp;|&nbsp;<a href=https://github.com/intervalrain/intervalrain.github.io/tree/main/content//AI/3_4.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目錄</span></summary><div class=inner><ul><li><a href=#%e7%9b%ae%e6%a8%99 aria-label=目標>目標</a></li><li><a href=#%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8 aria-label=線性迴歸>線性迴歸</a><ul><li><a href=#%e6%9a%b4%e5%8a%9b%e8%a7%a3 aria-label=暴力解>暴力解</a></li><li><a href=#%e7%b7%9a%e6%80%a7%e4%bb%a3%e6%95%b8%e8%a7%a3%e6%b3%95 aria-label=線性代數解法>線性代數解法</a></li><li><a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dgradient-descent aria-label="梯度下降(gradient descent)">梯度下降(gradient descent)</a></li></ul></li><li><a href=#%e6%89%b9%e6%ac%a1%e8%a8%93%e7%b7%b4batch%e7%9a%84%e6%a6%82%e5%bf%b5 aria-label=批次訓練(Batch)的概念>批次訓練(Batch)的概念</a></li><li><a href=#%e6%90%8d%e5%a4%b1%e5%87%bd%e6%95%b8loss-function aria-label="損失函數(loss function)">損失函數(loss function)</a><ul><li><a href=#%e5%9d%87%e6%96%b9%e8%aa%a4%e5%b7%aemean-squared-error-mse aria-label="均方誤差(Mean Squared Error, MSE)">均方誤差(Mean Squared Error, MSE)</a></li><li><a href=#%e4%ba%a4%e5%8f%89%e7%86%b5%e6%90%8d%e5%a4%b1cross-entropy-loss aria-label="交叉熵損失(Cross Entropy Loss)">交叉熵損失(Cross Entropy Loss)</a></li><li><a href=#%e5%b9%b3%e5%9d%87%e7%b5%95%e5%b0%8d%e8%aa%a4%e5%b7%aemean-absolute-error-mae aria-label="平均絕對誤差(Mean Absolute Error, MAE)">平均絕對誤差(Mean Absolute Error, MAE)</a></li><li><a href=#hinge-loss aria-label="Hinge Loss">Hinge Loss</a></li><li><a href=#l1l2-%e6%ad%a3%e5%89%87%e5%8c%96l1l2-regularization aria-label="L1/L2 正則化(L1/L2 Regularization)">L1/L2 正則化(L1/L2 Regularization)</a></li></ul></li><li><a href=#%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b8activation-function aria-label="激活函數(activation function)">激活函數(activation function)</a><ul><li><a href=#sigmoid aria-label=Sigmoid>Sigmoid</a></li><li><a href=#relu aria-label=ReLU>ReLU</a></li><li><a href=#tanh aria-label=Tanh>Tanh</a></li><li><a href=#%e5%85%b6%e5%ae%83%e8%ae%8a%e9%ab%94 aria-label=其它變體>其它變體</a></li><li><a href=#summary aria-label=Summary>Summary</a></li></ul></li><li><a href=#%e5%84%aa%e5%8c%96%e5%99%a8optimizers aria-label=優化器(Optimizers)>優化器(Optimizers)</a><ul><li><a href=#%e9%9a%a8%e6%a9%9f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d-sgdstochastic-gradient-descent aria-label="隨機梯度下降 SGD(Stochastic Gradient Descent)">隨機梯度下降 SGD(Stochastic Gradient Descent)</a></li><li><a href=#%e5%8b%95%e9%87%8f-momemtum aria-label="動量 Momemtum">動量 Momemtum</a></li><li><a href=#%e8%87%aa%e9%81%a9%e6%87%89%e5%ad%b8%e7%bf%92%e7%8e%87-rmsprop aria-label="自適應學習率 RMSProp">自適應學習率 RMSProp</a></li><li><a href=#adam aria-label=Adam>Adam</a></li><li><a href=#summary-1 aria-label=Summary>Summary</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=目標>目標<a hidden class=anchor aria-hidden=true href=#目標>#</a></h2><ul><li>機器學習的目標有很多種，參考李宏毅教授的機器學習課程，可以用下面一張圖來概述。
<img alt=terminology loading=lazy src=/ai/AI/3_4/terminology.png><ul><li>Task 代表機器學習的目標<ul><li>Regression: 透過迴歸來預測值。</li><li>Classification: 處理分類問題。</li><li>Structed Learning: 生成結構化的資訊(現在稱為生成式 AI, GenAI)</li></ul></li><li>Scenario 代表解決問題的策略<ul><li>Supervised Learning: 使用<strong>已標記</strong>的訓練數據進行訓練</li><li>Semi-supervised Learning: 使用<strong>有標記</strong>與<strong>無標記</strong>的訓練數據進行訓練</li><li>Unsupervised Learning: 不使用標記的訓練數據進行訓據，由模型自行發現模式與結構</li><li>Reinforcement Learning: 透過「獎勵」與「懲罰」來學習。</li><li>Transfer Learning: 將一個任務學習到的知識應用到相關的新任務</li></ul></li><li>Method 指應用的方法<ul><li>Linear Model</li><li>Deep Learning</li><li>SVM</li><li>Decision Tree</li><li>KNN</li></ul></li></ul></li></ul><h2 id=線性迴歸>線性迴歸<a hidden class=anchor aria-hidden=true href=#線性迴歸>#</a></h2><p><img alt=sample loading=lazy src=/ai/AI/3_4/sample.png></p><h3 id=暴力解>暴力解<a hidden class=anchor aria-hidden=true href=#暴力解>#</a></h3><ul><li>假設我們大概知道答案的區間，我們可以暴力求解，將每一個 w, b 代入求最小的 (w, b) 組合</li><li>這個方法的缺點是，計算量很大，且我們求值的方式不是連續的，精準度不夠。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> sys
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>areas <span style=color:#f92672>=</span> data[:,<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>prices <span style=color:#f92672>=</span> data[:,<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_loss</span>(y_pred, y):
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> (y_pred <span style=color:#f92672>-</span> y)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>best_w <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.</span>
</span></span><span style=display:flex><span>best_b <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.</span>
</span></span><span style=display:flex><span>min_loss <span style=color:#f92672>=</span> sys<span style=color:#f92672>.</span>float_info<span style=color:#f92672>.</span>max
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 猜 w=30-50, step = 0.1</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 猜 b=200-600 step = 1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>200</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>400</span>):
</span></span><span style=display:flex><span>        w <span style=color:#f92672>=</span> <span style=color:#ae81ff>30</span> <span style=color:#f92672>+</span> i<span style=color:#f92672>*</span><span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>        b <span style=color:#f92672>=</span> <span style=color:#ae81ff>200</span> <span style=color:#f92672>+</span> j<span style=color:#f92672>*</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> area, price <span style=color:#f92672>in</span> zip(areas, prices):
</span></span><span style=display:flex><span>            y_pred <span style=color:#f92672>=</span> w <span style=color:#f92672>*</span> area <span style=color:#f92672>+</span> b
</span></span><span style=display:flex><span>            loss <span style=color:#f92672>+=</span> compute_loss(y_pred, price)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> loss <span style=color:#f92672>&lt;</span> min_loss:
</span></span><span style=display:flex><span>            min_loss <span style=color:#f92672>=</span> loss
</span></span><span style=display:flex><span>            best_w <span style=color:#f92672>=</span> w
</span></span><span style=display:flex><span>            best_b <span style=color:#f92672>=</span> b
</span></span></code></pre></div><ul><li><code>w=35.1</code></li><li><code>b=599</code>
<img alt=brute_force loading=lazy src=/ai/AI/3_4/brute_force.png></li></ul><h3 id=線性代數解法>線性代數解法<a hidden class=anchor aria-hidden=true href=#線性代數解法>#</a></h3><ul><li>假如我們學過線性代數，我們想得到它的歸性迴歸方程式，我們的作法會是：<ul><li><p>設迴歸方程式為
$$\text{y}=\text{wx}+\text{b}\quad\quad (1)$$</p></li><li><p>我們要求最小平方差
$$\text{L}=\sum_{i=0}^n(\text{y}_i-\text{y})^2\quad\quad (2)$$</p></li><li><p>將 (1) 代入 (2)<br>$$\text{L}=\sum_{i=0}^n(\text{y}_i-\text{wx}-\text{b})^2\quad\quad (3)$$</p></li><li><p>學過線性代數，我們知道要求極值，可以對其求導數為0，並設 w 與 b 互不為函數，故我們對其個別做偏微分等於0。
$$\frac{\partial\text{L}}{\partial\text{w}}=0$$</p><p>$$\frac{\partial\text{L}}{\partial\text{b}}=0$$</p></li><li><p>對 b 做偏微分
$$\frac{\partial\text{L}}{\partial\text{b}}=-2\sum_{i=0}^n(\text{y}_i-\text{wx}_i-\text{b})=0$$</p><p>$$\sum_{i=0}^n(\text{y}_i-\text{wx}_i-\text{b})=0$$</p><p>$$\sum_{i=0}^n\text{y}_i-\text{w}\sum _{i=0}^n\text{x}_i-\text{nb}=0$$</p><p>$$\text{n}\bar{\text{y}}-\text{n}\bar{\text{wx}}-\text{nb}=0$$</p><p>$$\text{b}=\bar{\text{y}}-\text{w}\bar{\text{x}}\quad\quad (4)$$</p></li><li><p>對 w 做偏微分
$$\frac{\partial\text{L}}{\partial\text{w}}=-2\sum_{i=0}^n\text{x}_i(\text{y}_i-\text{wx}_i-\text{b})=0$$</p><p>$$\sum_{i=0}^n\text{x}_i(\text{y}_i-\text{wx}_i-\text{b})=0$$</p><p>$$\sum_{i=0}^n\text{x}_i\text{y}_i-\text{w}\sum _{i=0}^n\text{x}_i^2-\text{b}\sum _{i=0}^n\text{x}_i=0$$</p><ul><li>代入 (4)</li></ul><p>$$\sum_{i=0}^n\text{x}_i\text{y}_i-\text{w}\sum _{i=0}^n\text{x}_i^2-(\bar{\text{y}}-\text{w}\bar{\text{x}})\sum _{i=0}^n\text{x}_i=0$$</p><p>$$\sum_{i=0}^n\text{x}_i\text{y}_i-\text{w}\sum _{i=0}^n\text{x}_i^2-\bar{\text{y}}\sum _{i=0}^n\text{x}_i+\text{w}\sum _{i=0}^n\text{x}_i\bar{\text{x}}=0$$</p><p>$$\text{w}(\sum _{i=0}^n\text{x}_i\bar{\text{x}}-\sum _{i=0}^n\text{x}_i^2)=\bar{\text{y}}\sum _{i=0}^n\text{x}_i-\sum _{i=0}^n\text{x}_i\text{y}_i$$</p><p>$$\text{w}(\text{n}\bar{\text{x}}^2-\sum _{i=0}^n\text{x}_i^2)=\text{n}\bar{\text{x}}\bar{\text{y}}-\sum _{i=0}^n\text{x}_i\text{y}_i$$</p><p>$$\text{w}=\frac{\sum\text{x}_i\text{y}_i-\text{n}\bar{\text{x}}\bar{\text{y}}}{\sum\text{x}_i^2-\text{n}\bar{\text{x}}^2}$$</p><p>$$\text{w}=\frac{\sum\text{y}_i(\text{x}_i-\bar{\text{x}})}{\sum\text{x}_i(\text{x}_i-\bar{\text{x}})}$$</p><p>$$\text{w}=\frac{\sum(\text{y}-\bar{\text{y}})(\text{x}-\bar{\text{x}})}{\sum(\text{x}_i-\bar{\text{x}})^2}$$</p><p>$$\text{w}=\frac{S_{XY}}{S_{XX}}\quad\quad(5)$$</p></li><li><p>換言之，我們可以透過 (4) 與 (5) 式直接求得迴歸方程式
$$\text{y}=\frac{S_{XY}}{S_{XX}}\text{x}+(\bar{\text{y}}-\frac{S_{XY}}{S_{XX}}\bar{\text{x}})$$</p><p>其中</p><p>$$S_{XY}=\sum(\text{x}_i-\bar{\text{x}})(\text{y}_i-\bar{\text{y}})=\sum\text{x}_i\text{y}_i-\text{n}\bar{\text{x}}\bar{\text{y}}$$</p><p>$$S_{XX}=\sum(\text{x}_i-\bar{\text{x}})^2=\sum\text{x}_i^2-\text{n}\bar{\text{x}}^2$$</p></li><li><p>直接運用於 sample:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>meanx <span style=color:#f92672>=</span> data[:, <span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>meany <span style=color:#f92672>=</span> data[:, <span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sxy <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>sxx <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(data<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>sxy <span style=color:#f92672>+=</span> (data[i,<span style=color:#ae81ff>0</span>] <span style=color:#f92672>-</span> meanx)<span style=color:#f92672>*</span>(data[i,<span style=color:#ae81ff>1</span>] <span style=color:#f92672>-</span> meany)
</span></span><span style=display:flex><span>sxx <span style=color:#f92672>+=</span> (data[i,<span style=color:#ae81ff>0</span>] <span style=color:#f92672>-</span> meanx)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>w <span style=color:#f92672>=</span> sxy<span style=color:#f92672>/</span>sxx
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> meany <span style=color:#f92672>-</span> w<span style=color:#f92672>*</span>meanx
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(data[:, <span style=color:#ae81ff>0</span>], data[:, <span style=color:#ae81ff>1</span>], alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(data[:, <span style=color:#ae81ff>0</span>], w<span style=color:#f92672>*</span>data[:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>+</span> b, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;red&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Regression Line&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Size (ping)&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Total Price (10k)&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;House Price versus House Size&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>grid(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;w = </span><span style=color:#e6db74>{</span>w<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;b = </span><span style=color:#e6db74>{</span>b<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div></li></ul><img alt=sample_with_line loading=lazy src=/ai/AI/3_4/sample_with_line.png><ul><li><code>w = 34.9738</code></li><li><code>b = 602.5411</code></li></ul></li></ul><h3 id=梯度下降gradient-descent>梯度下降(gradient descent)<a hidden class=anchor aria-hidden=true href=#梯度下降gradient-descent>#</a></h3><ul><li>但事實上，在機器學習的領域要處理的不一定是上述這種只有兩維的問題，多維的問題會有多個梯度為0的地方，代表我們需要求出全部梯度為0的地方，再逐一代入我們的 loss function，最後找出 loss 最小的一組答案。</li><li>再者是，加入 activation function 後的方程式，變得並非上述案例中的容易微分。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> tensorflow <span style=color:#66d9ef>as</span> tf
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow <span style=color:#f92672>import</span> keras
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> matplotlib <span style=color:#f92672>import</span> cm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 1. 資料正規化函數</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>normalize_data</span>(data):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> (data <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>mean(data, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)) <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>std(data, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 2. 建立並訓練模型的函數</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_linear_regression</span>(x_norm, y_norm, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 建立模型</span>
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>        keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>, input_shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>,))
</span></span><span style=display:flex><span>    ])
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 編譯模型</span>
</span></span><span style=display:flex><span>    optimizer <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>optimizers<span style=color:#f92672>.</span>SGD(learning_rate<span style=color:#f92672>=</span>learning_rate)
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>compile(optimizer<span style=color:#f92672>=</span>optimizer, loss<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;mse&#39;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 用於記錄訓練過程的參數</span>
</span></span><span style=display:flex><span>    history <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;w&#39;</span>: [], <span style=color:#e6db74>&#39;b&#39;</span>: [], <span style=color:#e6db74>&#39;loss&#39;</span>: []}
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ParameterHistory</span>(keras<span style=color:#f92672>.</span>callbacks<span style=color:#f92672>.</span>Callback):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>on_epoch_begin</span>(self, epoch, logs<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>            w <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>layers[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>get_weights()[<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>            b <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>layers[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>get_weights()[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>            loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>evaluate(x_norm, y_norm, verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>            history[<span style=color:#e6db74>&#39;w&#39;</span>]<span style=color:#f92672>.</span>append(w)
</span></span><span style=display:flex><span>            history[<span style=color:#e6db74>&#39;b&#39;</span>]<span style=color:#f92672>.</span>append(b)
</span></span><span style=display:flex><span>            history[<span style=color:#e6db74>&#39;loss&#39;</span>]<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>    <span style=color:#75715e># 訓練模型</span>
</span></span><span style=display:flex><span>    parameter_history <span style=color:#f92672>=</span> ParameterHistory()
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>fit(x_norm, y_norm, epochs<span style=color:#f92672>=</span>epochs, verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, callbacks<span style=color:#f92672>=</span>[parameter_history])
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 記錄最後一次的參數</span>
</span></span><span style=display:flex><span>    w <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>layers[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>get_weights()[<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    b <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>layers[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>get_weights()[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>evaluate(x_norm, y_norm, verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    history[<span style=color:#e6db74>&#39;w&#39;</span>]<span style=color:#f92672>.</span>append(w)
</span></span><span style=display:flex><span>    history[<span style=color:#e6db74>&#39;b&#39;</span>]<span style=color:#f92672>.</span>append(b)
</span></span><span style=display:flex><span>    history[<span style=color:#e6db74>&#39;loss&#39;</span>]<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> model, history
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 3. 視覺化函數</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>plot_training_process</span>(x_raw, y_raw, x_norm, y_norm, history):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 創建圖表</span>
</span></span><span style=display:flex><span>    fig, (ax1, ax2) <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplots(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>15</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 將正規化的係數轉換回原始尺度</span>
</span></span><span style=display:flex><span>    w_raw_history <span style=color:#f92672>=</span> [w <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>std(x_raw) <span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> history[<span style=color:#e6db74>&#39;w&#39;</span>]]
</span></span><span style=display:flex><span>    b_raw_history <span style=color:#f92672>=</span> [(b <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>mean(y_raw) <span style=color:#f92672>-</span> 
</span></span><span style=display:flex><span>                     w <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>mean(x_raw) <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>std(x_raw))
</span></span><span style=display:flex><span>                     <span style=color:#66d9ef>for</span> w, b <span style=color:#f92672>in</span> zip(history[<span style=color:#e6db74>&#39;w&#39;</span>], history[<span style=color:#e6db74>&#39;b&#39;</span>])]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Contour plot with raw scale</span>
</span></span><span style=display:flex><span>    margin_w <span style=color:#f92672>=</span> (max(w_raw_history) <span style=color:#f92672>-</span> min(w_raw_history)) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>    margin_b <span style=color:#f92672>=</span> (max(b_raw_history) <span style=color:#f92672>-</span> min(b_raw_history)) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>    w_raw_range <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(min(w_raw_history)<span style=color:#f92672>-</span>margin_w, max(w_raw_history)<span style=color:#f92672>+</span>margin_w, <span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>    b_raw_range <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(min(b_raw_history)<span style=color:#f92672>-</span>margin_b, max(b_raw_history)<span style=color:#f92672>+</span>margin_b, <span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>    W_RAW, B_RAW <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>meshgrid(w_raw_range, b_raw_range)
</span></span><span style=display:flex><span>    Z <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros_like(W_RAW)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 計算每個點的 MSE（在原始尺度上）</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(W_RAW<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(W_RAW<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]):
</span></span><span style=display:flex><span>            y_pred <span style=color:#f92672>=</span> W_RAW[i,j] <span style=color:#f92672>*</span> x_raw <span style=color:#f92672>+</span> B_RAW[i,j]
</span></span><span style=display:flex><span>            Z[i,j] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean((y_pred <span style=color:#f92672>-</span> y_raw) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    CS <span style=color:#f92672>=</span> ax1<span style=color:#f92672>.</span>contour(W_RAW, B_RAW, Z, levels<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>clabel(CS, inline<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, fontsize<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>plot(w_raw_history, b_raw_history, <span style=color:#e6db74>&#39;r.-&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Training path&#39;</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>set_xlabel(<span style=color:#e6db74>&#39;w (原始尺度)&#39;</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>set_ylabel(<span style=color:#e6db74>&#39;b (原始尺度)&#39;</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>set_title(<span style=color:#e6db74>&#39;Contour Plot with Training Path (原始尺度)&#39;</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Raw data scatter plot with regression lines</span>
</span></span><span style=display:flex><span>    ax2<span style=color:#f92672>.</span>scatter(x_raw, y_raw, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Raw data&#39;</span>)
</span></span><span style=display:flex><span>    ax2<span style=color:#f92672>.</span>set_ylim(<span style=color:#ae81ff>700</span>, <span style=color:#ae81ff>2300</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 繪製每一輪的回歸線</span>
</span></span><span style=display:flex><span>    x_plot <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(min(x_raw), max(x_raw), <span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>    colors <span style=color:#f92672>=</span> cm<span style=color:#f92672>.</span>rainbow(np<span style=color:#f92672>.</span>linspace(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, len(w_raw_history)))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, (w, b) <span style=color:#f92672>in</span> enumerate(zip(w_raw_history, b_raw_history)):
</span></span><span style=display:flex><span>        y_plot <span style=color:#f92672>=</span> w <span style=color:#f92672>*</span> x_plot <span style=color:#f92672>+</span> b
</span></span><span style=display:flex><span>        ax2<span style=color:#f92672>.</span>plot(x_plot, y_plot, color<span style=color:#f92672>=</span>colors[i], alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    ax2<span style=color:#f92672>.</span>set_xlabel(<span style=color:#e6db74>&#39;Area (坪)&#39;</span>)
</span></span><span style=display:flex><span>    ax2<span style=color:#f92672>.</span>set_ylabel(<span style=color:#e6db74>&#39;Price (萬)&#39;</span>)
</span></span><span style=display:flex><span>    ax2<span style=color:#f92672>.</span>set_title(<span style=color:#e6db74>&#39;Raw Data with Regression Lines&#39;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>tight_layout()
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 載入數據</span>
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> load_data()
</span></span><span style=display:flex><span>x_raw, y_raw <span style=color:#f92672>=</span> data[:, <span style=color:#ae81ff>0</span>], data[:, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 轉換為 TensorFlow 格式</span>
</span></span><span style=display:flex><span>x_raw <span style=color:#f92672>=</span> x_raw<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>y_raw <span style=color:#f92672>=</span> y_raw<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 正規化數據</span>
</span></span><span style=display:flex><span>x_norm <span style=color:#f92672>=</span> normalize_data(x_raw)
</span></span><span style=display:flex><span>y_norm <span style=color:#f92672>=</span> normalize_data(y_raw)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 訓練模型</span>
</span></span><span style=display:flex><span>model, history <span style=color:#f92672>=</span> train_linear_regression(x_norm, y_norm)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 視覺化結果</span>
</span></span><span style=display:flex><span>plot_training_process(x_raw<span style=color:#f92672>.</span>flatten(), y_raw<span style=color:#f92672>.</span>flatten(), 
</span></span><span style=display:flex><span>                        x_norm<span style=color:#f92672>.</span>flatten(), y_norm<span style=color:#f92672>.</span>flatten(), history)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 輸出最終結果</span>
</span></span><span style=display:flex><span>final_w <span style=color:#f92672>=</span> history[<span style=color:#e6db74>&#39;w&#39;</span>][<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>final_b <span style=color:#f92672>=</span> history[<span style=color:#e6db74>&#39;b&#39;</span>][<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>final_loss <span style=color:#f92672>=</span> history[<span style=color:#e6db74>&#39;loss&#39;</span>][<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 將係數轉換回原始尺度</span>
</span></span><span style=display:flex><span>w_raw <span style=color:#f92672>=</span> final_w <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>std(x_raw)
</span></span><span style=display:flex><span>b_raw <span style=color:#f92672>=</span> (final_b <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>mean(y_raw) <span style=color:#f92672>-</span> 
</span></span><span style=display:flex><span>        final_w <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>mean(x_raw) <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>std(x_raw))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Final equation: y = </span><span style=color:#e6db74>{</span>w_raw[<span style=color:#ae81ff>0</span>]<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>x + </span><span style=color:#e6db74>{</span>b_raw[<span style=color:#ae81ff>0</span>]<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Final normalized loss: </span><span style=color:#e6db74>{</span>final_loss<span style=color:#e6db74>:</span><span style=color:#e6db74>.6f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p><img alt=sample_with_gradient_descent loading=lazy src=/ai/AI/3_4/sample_with_gradient_descent.png></p><h2 id=批次訓練batch的概念>批次訓練(Batch)的概念<a hidden class=anchor aria-hidden=true href=#批次訓練batch的概念>#</a></h2><ul><li>我們可以在不同的時機點來更新 w 與 b，假設我們的訓練次數為 3000，那 epochs 為3000。且樣本數為 1000。<ul><li>批次訓練(batch training)，代表的是我們總共做 3000 次的更新，每次都是利用全部 1000 筆樣本算出來的 dw 與 db 去做調整。</li><li>SGD(Stochastic gradient descent)，代表我們做 3000 * 1000 次的更新，每一個樣本計算出 dw 與 db 就立即去調整。</li><li>小批次訓練(mini-batch training)則是介於批次訓練與 SGD 之間，假設我們每 200 個樣本做一次更新，實際上會做 3000 * 5 次更新。</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>LinearRegression</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.0000001</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>w <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>b <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lr <span style=color:#f92672>=</span> learning_rate
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>loss_history <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(self, X):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>w <span style=color:#f92672>*</span> X <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_loss</span>(self, X, y):
</span></span><span style=display:flex><span>        y_pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>predict(X)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>mean((y_pred <span style=color:#f92672>-</span> y) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_gradients</span>(self, X, y):
</span></span><span style=display:flex><span>        y_pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>predict(X)
</span></span><span style=display:flex><span>        error <span style=color:#f92672>=</span> y_pred <span style=color:#f92672>-</span> y
</span></span><span style=display:flex><span>        dw <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean(<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> error <span style=color:#f92672>*</span> X)
</span></span><span style=display:flex><span>        db <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean(<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> error)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> dw, db
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_batch</span>(self, X, y, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>3000</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;Full batch gradient descent&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(epochs):
</span></span><span style=display:flex><span>            <span style=color:#75715e># Compute gradients using all data</span>
</span></span><span style=display:flex><span>            dw, db <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_gradients(X, y)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># Update parameters</span>
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>w <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> dw
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>b <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> db
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># Record loss</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> epoch <span style=color:#f92672>%</span> <span style=color:#ae81ff>100</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_loss(X, y)
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>loss_history<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Epoch </span><span style=color:#e6db74>{</span>epoch<span style=color:#e6db74>}</span><span style=color:#e6db74>, Loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_mini_batch</span>(self, X, y, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>3000</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;Mini-batch gradient descent&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        n_samples <span style=color:#f92672>=</span> len(X)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(epochs):
</span></span><span style=display:flex><span>            <span style=color:#75715e># Shuffle the data</span>
</span></span><span style=display:flex><span>            indices <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>permutation(n_samples)
</span></span><span style=display:flex><span>            X_shuffled <span style=color:#f92672>=</span> X[indices]
</span></span><span style=display:flex><span>            y_shuffled <span style=color:#f92672>=</span> y[indices]
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># Mini-batch training</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, n_samples, batch_size):
</span></span><span style=display:flex><span>                X_batch <span style=color:#f92672>=</span> X_shuffled[i:i<span style=color:#f92672>+</span>batch_size]
</span></span><span style=display:flex><span>                y_batch <span style=color:#f92672>=</span> y_shuffled[i:i<span style=color:#f92672>+</span>batch_size]
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>                <span style=color:#75715e># Compute gradients using batch data</span>
</span></span><span style=display:flex><span>                dw, db <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_gradients(X_batch, y_batch)
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>                <span style=color:#75715e># Update parameters</span>
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>w <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> dw
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>b <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> db
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># Record loss for the whole dataset</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> epoch <span style=color:#f92672>%</span> <span style=color:#ae81ff>100</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_loss(X, y)
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>loss_history<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Epoch </span><span style=color:#e6db74>{</span>epoch<span style=color:#e6db74>}</span><span style=color:#e6db74>, Loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_sgd</span>(self, X, y, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>3000</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;Stochastic gradient descent&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        n_samples <span style=color:#f92672>=</span> len(X)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(epochs):
</span></span><span style=display:flex><span>            <span style=color:#75715e># Shuffle the data</span>
</span></span><span style=display:flex><span>            indices <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>permutation(n_samples)
</span></span><span style=display:flex><span>            X_shuffled <span style=color:#f92672>=</span> X[indices]
</span></span><span style=display:flex><span>            y_shuffled <span style=color:#f92672>=</span> y[indices]
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># SGD training (batch_size = 1)</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(n_samples):
</span></span><span style=display:flex><span>                X_sample <span style=color:#f92672>=</span> X_shuffled[i:i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>                y_sample <span style=color:#f92672>=</span> y_shuffled[i:i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>                <span style=color:#75715e># Compute gradients using single sample</span>
</span></span><span style=display:flex><span>                dw, db <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_gradients(X_sample, y_sample)
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>                <span style=color:#75715e># Update parameters</span>
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>w <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> dw
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>b <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> db
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># Record loss for the whole dataset</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> epoch <span style=color:#f92672>%</span> <span style=color:#ae81ff>100</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_loss(X, y)
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>loss_history<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Epoch </span><span style=color:#e6db74>{</span>epoch<span style=color:#e6db74>}</span><span style=color:#e6db74>, Loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(areas, prices) <span style=color:#f92672>=</span> load_data()
</span></span><span style=display:flex><span>models <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;Batch&#39;</span>: LinearRegression(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-7</span>),
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;Mini-batch&#39;</span>: LinearRegression(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-7</span>),
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;SGD&#39;</span>: LinearRegression(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>5e-8</span>)
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>models[<span style=color:#e6db74>&#39;Batch&#39;</span>]<span style=color:#f92672>.</span>train_batch(areas, prices)
</span></span><span style=display:flex><span>models[<span style=color:#e6db74>&#39;Mini-batch&#39;</span>]<span style=color:#f92672>.</span>train_mini_batch(areas, prices)
</span></span><span style=display:flex><span>models[<span style=color:#e6db74>&#39;SGD&#39;</span>]<span style=color:#f92672>.</span>train_sgd(areas, prices)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> name, model <span style=color:#f92672>in</span> models<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(range(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>3000</span>, <span style=color:#ae81ff>100</span>), model<span style=color:#f92672>.</span>loss_history, label<span style=color:#f92672>=</span>name)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Epoch&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Loss Comparison&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>grid(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> name, model <span style=color:#f92672>in</span> models<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74> Results:&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;w = </span><span style=color:#e6db74>{</span>model<span style=color:#f92672>.</span>w<span style=color:#e6db74>:</span><span style=color:#e6db74>.6f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;b = </span><span style=color:#e6db74>{</span>model<span style=color:#f92672>.</span>b<span style=color:#e6db74>:</span><span style=color:#e6db74>.6f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Final Loss = </span><span style=color:#e6db74>{</span>model<span style=color:#f92672>.</span>loss_histroy[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><ul><li><p>從更新次數來看，SGD 的更新次數 > 小批次訓練 > 批次訓練，SGD 所耗的時間同樣也比小批次訓練與批次訓練長，但實際上 loss 收斂的情形也比較好嗎？</p></li><li><p>從下圖比較可見，收斂情況最佳的反而是小批次訓練，我們比較三種方法，總結一下成果：
<img alt=batch loading=lazy src=/ai/AI/3_4/batch.png></p></li><li><p>批次訓練 Batch Gradient Descent (BGD)</p><ul><li>每次更新使用所有數據</li><li>穩定但計算量大</li><li>容易找到局部最優解</li></ul></li><li><p>小批次訓練 Mini-batch Gradient Descent</p><ul><li>每次使用一小批數據</li><li>平衡了計算效率和更新穩定性</li><li>常用於實際應用</li></ul></li><li><p>Stochastic Gradient Descent (SGD)</p><ul><li>每次只使用一個樣本</li><li>更新頻繁，收斂較快但不穩定</li><li>需要較小的學習率</li></ul></li></ul><h2 id=損失函數loss-function>損失函數(loss function)<a hidden class=anchor aria-hidden=true href=#損失函數loss-function>#</a></h2><ul><li><p>損失函數是用來衡量模型預測值與實際值之間差異的函數，以下是幾個常見的損失函數：</p><h3 id=均方誤差mean-squared-error-mse>均方誤差(Mean Squared Error, MSE)<a hidden class=anchor aria-hidden=true href=#均方誤差mean-squared-error-mse>#</a></h3><ul><li>常用於迴歸問題</li><li>對異常值敏感
$$
\text{MSE}=\frac{1}{\text{n}}\sum^n(\text{y}_\text{pred}-\text{y} _\text{true})^2
$$</li></ul><h3 id=交叉熵損失cross-entropy-loss>交叉熵損失(Cross Entropy Loss)<a hidden class=anchor aria-hidden=true href=#交叉熵損失cross-entropy-loss>#</a></h3><ul><li>常用於分類問題</li><li>衡量兩個概率分布之間的差異</li><li>又分為二元交叉熵和多類別交叉熵<ul><li>二元交叉熵(Binary Cross Entropy)
$$
\text{BCE}=-(\text{y}_\text{true}\times \log(\text{y} _\text{pred})+(1-\text{y} _\text{true})\times \log(1-\text{y} _\text{pred}))
$$</li><li>多類別交叉熵(Categorical Cross Entropy)
$$
\text{CCE}=-\sum^n((\text{y} _\text{true})_i\times\log((\text{y} _\text{true})_i))
$$</li></ul></li></ul><h3 id=平均絕對誤差mean-absolute-error-mae>平均絕對誤差(Mean Absolute Error, MAE)<a hidden class=anchor aria-hidden=true href=#平均絕對誤差mean-absolute-error-mae>#</a></h3><ul><li>用於迴歸問題</li><li>相較 MSE 對異常值不那麼敏感
$$
\text{MAE}=\frac{1}{\text{n}}\sum^n|\text{y} _\text{pred}-\text{y} _\text{true}|
$$</li></ul><h3 id=hinge-loss>Hinge Loss<a hidden class=anchor aria-hidden=true href=#hinge-loss>#</a></h3><ul><li>主要用於支持向量機(SVM)</li><li>特別適合最大間隔分類問題
$$
\text{HL} = \max(0, 1-\text{y} _\text{pred}\times\text{y} _\text{true})
$$</li></ul></li><li><p>在選擇損失函數時需考慮</p><ol><li>問題類型(分類還是迴歸)</li><li>數據分布特性</li><li>對異常值的敏感度要求</li><li>模型的收斂速度要求</li></ol></li></ul><h3 id=l1l2-正則化l1l2-regularization>L1/L2 正則化(L1/L2 Regularization)<a hidden class=anchor aria-hidden=true href=#l1l2-正則化l1l2-regularization>#</a></h3><ul><li><p>在考慮有多個特徵、且帶有 outlier 或雜訊時</p></li><li><p>L1 正則化 (Lasso Regression)</p><ul><li>Lasso (Least Absolute Shrinkage and Selection Operator)</li><li>定義：在損失函數中加入參數的絕對值項
$$
\text{Loss} = \text{MSE} + \lambda \times \sum|w|
$$</li><li>特點：<ul><li>傾向於產生稀疏解（某些參數會變成0）</li><li>適合用於特徵選擇</li><li>對異常值較不敏感</li></ul></li></ul></li><li><p>L2 正則化 (Ridge Regression)</p><ul><li>定義：在損失函數中加入參數的平方項
$$
\text{Loss} = \text{MSE} + \lambda \times \sum(w^2)
$$</li><li>特點：<ul><li>傾向於使所有參數值變小但不為0</li><li>計算導數較簡單</li><li>對共線性（多重共線性）問題有好處</li></ul></li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> StandardScaler
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> train_test_split
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 生成合成數據</span>
</span></span><span style=display:flex><span>np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>seed(<span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_synthetic_data</span>(n_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 生成基本特徵</span>
</span></span><span style=display:flex><span>    X1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, n_samples)  <span style=color:#75715e># 面積</span>
</span></span><span style=display:flex><span>    X2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, n_samples)  <span style=color:#75715e># 房齡</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 生成共線性特徵（與面積高度相關的特徵，如房間數）</span>
</span></span><span style=display:flex><span>    X3 <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.8</span> <span style=color:#f92672>*</span> X1 <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.2</span> <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, n_samples)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 生成噪音特徵（完全無關的特徵）</span>
</span></span><span style=display:flex><span>    X4 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, n_samples)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 組合特徵</span>
</span></span><span style=display:flex><span>    X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>column_stack([X1, X2, X3, X4])
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 生成目標值（房價）</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 主要由X1和X2決定，X3有少許影響，X4完全不影響</span>
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span> <span style=color:#f92672>*</span> X1 <span style=color:#f92672>+</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> X2 <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> X3 <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.1</span>, n_samples)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> X, y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>RegularizedRegression</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-7</span>, reg_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;l2&#39;</span>, lambda_reg<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>w <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>b <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lr <span style=color:#f92672>=</span> learning_rate
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>reg_type <span style=color:#f92672>=</span> reg_type
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lambda_reg <span style=color:#f92672>=</span> lambda_reg
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>loss_history <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(self, X):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>w <span style=color:#f92672>*</span> X <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_loss</span>(self, X, y):
</span></span><span style=display:flex><span>        y_pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>predict(X)
</span></span><span style=display:flex><span>        mse <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean((y_pred <span style=color:#f92672>-</span> y) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>reg_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;l1&#39;</span>:
</span></span><span style=display:flex><span>            reg_term <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>lambda_reg <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>abs(self<span style=color:#f92672>.</span>w)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> self<span style=color:#f92672>.</span>reg_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;l2&#39;</span>:
</span></span><span style=display:flex><span>            reg_term <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>lambda_reg <span style=color:#f92672>*</span> (self<span style=color:#f92672>.</span>w <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            reg_term <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> mse <span style=color:#f92672>+</span> reg_term
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_gradients</span>(self, X, y):
</span></span><span style=display:flex><span>        y_pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>predict(X)
</span></span><span style=display:flex><span>        error <span style=color:#f92672>=</span> y_pred <span style=color:#f92672>-</span> y
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># mse 的梯度</span>
</span></span><span style=display:flex><span>        dw_mse <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean(<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> error <span style=color:#f92672>*</span> X)
</span></span><span style=display:flex><span>        db <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean(<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> error)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 正則化項的梯度</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>reg_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;l1&#39;</span>:
</span></span><span style=display:flex><span>            dw_reg <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>lambda_reg <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>sign(self<span style=color:#f92672>.</span>w)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> self<span style=color:#f92672>.</span>reg_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;l2&#39;</span>:
</span></span><span style=display:flex><span>            dw_reg <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>lambda_reg <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>w
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            dw_reg <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        dw <span style=color:#f92672>=</span> dw_mse <span style=color:#f92672>+</span> dw_reg
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> dw, db
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train</span>(self, X, y, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>3000</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(epochs):
</span></span><span style=display:flex><span>            dw, db <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_gradients(X, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>w <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> dw
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>b <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> db
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train</span>(self, X, y, batch_size<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>3000</span>):
</span></span><span style=display:flex><span>        n_samples <span style=color:#f92672>=</span> len(X)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> batch_size <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            batch_size <span style=color:#f92672>=</span> n_samples
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(epochs):
</span></span><span style=display:flex><span>            <span style=color:#75715e># Shuffle the data</span>
</span></span><span style=display:flex><span>            indices <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>permutation(n_samples)
</span></span><span style=display:flex><span>            X_shuffled <span style=color:#f92672>=</span> X[indices]
</span></span><span style=display:flex><span>            y_shuffled <span style=color:#f92672>=</span> y[indices]
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># Mini-batch training</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, n_samples, batch_size):
</span></span><span style=display:flex><span>                X_batch <span style=color:#f92672>=</span> X_shuffled[i:i<span style=color:#f92672>+</span>batch_size]
</span></span><span style=display:flex><span>                y_batch <span style=color:#f92672>=</span> y_shuffled[i:i<span style=color:#f92672>+</span>batch_size]
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>                <span style=color:#75715e># Compute gradients using batch data</span>
</span></span><span style=display:flex><span>                dw, db <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_gradients(X_batch, y_batch)
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>                <span style=color:#75715e># Update parameters</span>
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>w <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> dw
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>b <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> db
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># Record loss for the whole dataset</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> epoch <span style=color:#f92672>%</span> <span style=color:#ae81ff>100</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_loss(X, y)
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>loss_history<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Epoch </span><span style=color:#e6db74>{</span>epoch<span style=color:#e6db74>}</span><span style=color:#e6db74>, Loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p><img alt=regularization loading=lazy src=/ai/AI/3_4/relugarization.png></p><ul><li><p>觀察重點：</p><ul><li>L1 正則化（Lasso）：<ul><li>傾向於將不重要的特徵（如 Noise）權重設為 0</li><li>在有共線性的特徵中選擇一個（Area vs Rooms）</li></ul></li><li>L2 正則化（Ridge）：<ul><li>所有權重都被縮小</li><li>共線性特徵的權重會被平均分配</li></ul></li><li>無正則化：<ul><li>可能過度擬合噪音</li><li>在共線性特徵上表現不穩定</li></ul></li></ul></li><li><p>從結果可以看出：</p><ul><li>L1 正則化確實將無關特徵（Noise）的權重降到接近 0</li><li>L2 正則化讓所有權重都變得更小，但保持了相對重要性</li><li>無正則化的模型權重更大，更容易受噪音影響</li></ul></li><li><p>主要的差異和實作細節：</p><ol><li><strong>正則化項的加入</strong></li></ol><ul><li>L1：在損失函數中加入 <code>λ * |w|</code></li><li>L2：在損失函數中加入 <code>λ * w²</code></li></ul><ol start=2><li><strong>梯度計算</strong></li></ol><ul><li>L1 的梯度：<code>sign(w) * λ</code></li><li>L2 的梯度：<code>2 * λ * w</code></li></ul><ol start=3><li><strong>超參數 λ (lambda_reg)</strong></li></ol><ul><li>控制正則化的強度</li><li>較大的 λ 會產生較小的權重</li><li>需要通過交叉驗證來選擇適當的值</li></ul><ol start=4><li><strong>使用場景</strong></li></ol><ul><li>L1：特徵選擇，當你認為只有部分特徵是重要的</li><li>L2：處理共線性，當特徵之間有相關性</li></ul></li></ul><h2 id=激活函數activation-function>激活函數(activation function)<a hidden class=anchor aria-hidden=true href=#激活函數activation-function>#</a></h2><ul><li><p>在設計 model 時，不是所有的問題都可以用線性模型來描述，此時我們就需要</p><ol><li>多層結構(至少一個隱藏層)</li><li>非線性激活函數</li></ol></li><li><p>以下為三個重要的激活函數
<img alt=activation loading=lazy src=/ai/AI/3_4/Curves-of-the-Sigmoid-Tanh-and-ReLu-activation-functions.png></p><ol><li><p>Sigmoid (σ(x) = 1/(1+e^(-x)))</p><ul><li>輸出範圍：(0,1)</li><li>優點：適合二分類問題</li><li>缺點：容易出現梯度消失</li></ul></li><li><p>ReLU (max(0,x))</p><ul><li>輸出範圍：[0,∞)</li><li>優點：計算簡單，不會有梯度消失</li><li>缺點：Dead ReLU 問題</li></ul></li><li><p>Tanh (tanh(x))</p><ul><li>輸出範圍：(-1,1)</li><li>優點：零中心化</li><li>缺點：也有梯度消失問題</li></ul></li></ol></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Activation</span>:
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sigmoid</span>(x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>exp(<span style=color:#f92672>-</span>x))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sigmoid_derivative</span>(x):
</span></span><span style=display:flex><span>        sx <span style=color:#f92672>=</span> Activation<span style=color:#f92672>.</span>sigmoid(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> sx <span style=color:#f92672>*</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> sx)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>relu</span>(x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>maximum(<span style=color:#ae81ff>0</span>, x)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>relu_derivative</span>(x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>where(x <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tanh</span>(x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>tanh(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tanh_derivative</span>(x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>tanh(x) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>
</span></span></code></pre></div><ul><li>以 <code>xor</code> 為例來做以下的機器學習</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Activation</span>:
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sigmoid</span>(x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>exp(<span style=color:#f92672>-</span>x))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sigmoid_derivative</span>(x):
</span></span><span style=display:flex><span>        sx <span style=color:#f92672>=</span> Activation<span style=color:#f92672>.</span>sigmoid(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> sx <span style=color:#f92672>*</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> sx)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>relu</span>(x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>maximum(<span style=color:#ae81ff>0</span>, x)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>relu_derivative</span>(x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>where(x <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tanh</span>(x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>tanh(x)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tanh_derivative</span>(x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>tanh(x)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>NeuralNetwork</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;sigmoid&#39;</span>):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 網絡架構: 2 -&gt; 4 -&gt; 1</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>W1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.1</span>  <span style=color:#75715e># 輸入層到隱藏層的權重</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>b1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>4</span>))             <span style=color:#75715e># 隱藏層偏差</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>W2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>1</span>) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.1</span>  <span style=color:#75715e># 隱藏層到輸出層的權重</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>b2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>))             <span style=color:#75715e># 輸出層偏差</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 選擇激活函數</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> activation <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;sigmoid&#39;</span>:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>activation <span style=color:#f92672>=</span> Activation<span style=color:#f92672>.</span>sigmoid
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>activation_derivative <span style=color:#f92672>=</span> Activation<span style=color:#f92672>.</span>sigmoid_derivative
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> activation <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;relu&#39;</span>:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>activation <span style=color:#f92672>=</span> Activation<span style=color:#f92672>.</span>relu
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>activation_derivative <span style=color:#f92672>=</span> Activation<span style=color:#f92672>.</span>relu_derivative
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> activation <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;tanh&#39;</span>:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>activation <span style=color:#f92672>=</span> Activation<span style=color:#f92672>.</span>tanh
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>activation_derivative <span style=color:#f92672>=</span> Activation<span style=color:#f92672>.</span>tanh_derivative
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>loss_history <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, X):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 前向傳播</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>z1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(X, self<span style=color:#f92672>.</span>W1) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b1
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>a1 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>activation(self<span style=color:#f92672>.</span>z1)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>z2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(self<span style=color:#f92672>.</span>a1, self<span style=color:#f92672>.</span>W2) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b2
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>a2 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>activation(self<span style=color:#f92672>.</span>z2)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>a2
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>backward</span>(self, X, y, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>):
</span></span><span style=display:flex><span>        m <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 計算梯度</span>
</span></span><span style=display:flex><span>        dz2 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>a2 <span style=color:#f92672>-</span> y
</span></span><span style=display:flex><span>        dW2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(self<span style=color:#f92672>.</span>a1<span style=color:#f92672>.</span>T, dz2) <span style=color:#f92672>/</span> m
</span></span><span style=display:flex><span>        db2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(dz2, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) <span style=color:#f92672>/</span> m
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        dz1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(dz2, self<span style=color:#f92672>.</span>W2<span style=color:#f92672>.</span>T) <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>activation_derivative(self<span style=color:#f92672>.</span>z1)
</span></span><span style=display:flex><span>        dW1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(X<span style=color:#f92672>.</span>T, dz1) <span style=color:#f92672>/</span> m
</span></span><span style=display:flex><span>        db1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(dz1, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) <span style=color:#f92672>/</span> m
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 更新權重</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>W2 <span style=color:#f92672>-=</span> learning_rate <span style=color:#f92672>*</span> dW2
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>b2 <span style=color:#f92672>-=</span> learning_rate <span style=color:#f92672>*</span> db2
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>W1 <span style=color:#f92672>-=</span> learning_rate <span style=color:#f92672>*</span> dW1
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>b1 <span style=color:#f92672>-=</span> learning_rate <span style=color:#f92672>*</span> db1
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train</span>(self, X, y, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(epochs):
</span></span><span style=display:flex><span>            <span style=color:#75715e># 前向傳播</span>
</span></span><span style=display:flex><span>            output <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>forward(X)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># 計算損失</span>
</span></span><span style=display:flex><span>            loss <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean((output <span style=color:#f92672>-</span> y) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>loss_history<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># 反向傳播</span>
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>backward(X, y, learning_rate)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> epoch <span style=color:#f92672>%</span> <span style=color:#ae81ff>1000</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Epoch </span><span style=color:#e6db74>{</span>epoch<span style=color:#e6db74>}</span><span style=color:#e6db74>, Loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(self, X):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>round(self<span style=color:#f92672>.</span>forward(X))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 準備 XOR 數據</span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>], [<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>]])
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>1</span>], [<span style=color:#ae81ff>1</span>], [<span style=color:#ae81ff>0</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 訓練不同激活函數的模型</span>
</span></span><span style=display:flex><span>activation_functions <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;sigmoid&#39;</span>, <span style=color:#e6db74>&#39;relu&#39;</span>, <span style=color:#e6db74>&#39;tanh&#39;</span>]
</span></span><span style=display:flex><span>models <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> activation <span style=color:#f92672>in</span> activation_functions:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Training with </span><span style=color:#e6db74>{</span>activation<span style=color:#e6db74>}</span><span style=color:#e6db74> activation:&#34;</span>)
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> NeuralNetwork(activation<span style=color:#f92672>=</span>activation)
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>train(X, y)
</span></span><span style=display:flex><span>    models[activation] <span style=color:#f92672>=</span> model
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 繪製損失曲線比較</span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> activation, model <span style=color:#f92672>in</span> models<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(model<span style=color:#f92672>.</span>loss_history, label<span style=color:#f92672>=</span>activation)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Iteration&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Training Loss with Different Activation Functions&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>grid(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 測試預測結果</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Prediction Results:&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> activation, model <span style=color:#f92672>in</span> models<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>activation<span style=color:#e6db74>}</span><span style=color:#e6db74> activation:&#34;</span>)
</span></span><span style=display:flex><span>    predictions <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(X)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> x, y_true, y_pred <span style=color:#f92672>in</span> zip(X, y, predictions):
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Input: </span><span style=color:#e6db74>{</span>x<span style=color:#e6db74>}</span><span style=color:#e6db74>, True: </span><span style=color:#e6db74>{</span>y_true[<span style=color:#ae81ff>0</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>, Predicted: </span><span style=color:#e6db74>{</span>y_pred[<span style=color:#ae81ff>0</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 視覺化決策邊界</span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>15</span>, <span style=color:#ae81ff>5</span>))
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i, (activation, model) <span style=color:#f92672>in</span> enumerate(models<span style=color:#f92672>.</span>items()):
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>subplot(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 創建網格點</span>
</span></span><span style=display:flex><span>    xx, yy <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>meshgrid(np<span style=color:#f92672>.</span>linspace(<span style=color:#f92672>-</span><span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>1.5</span>, <span style=color:#ae81ff>100</span>),
</span></span><span style=display:flex><span>                        np<span style=color:#f92672>.</span>linspace(<span style=color:#f92672>-</span><span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>1.5</span>, <span style=color:#ae81ff>100</span>))
</span></span><span style=display:flex><span>    grid <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>c_[xx<span style=color:#f92672>.</span>ravel(), yy<span style=color:#f92672>.</span>ravel()]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 預測</span>
</span></span><span style=display:flex><span>    Z <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(grid)
</span></span><span style=display:flex><span>    Z <span style=color:#f92672>=</span> Z<span style=color:#f92672>.</span>reshape(xx<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 繪製決策邊界</span>
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>contourf(xx, yy, Z, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.4</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>scatter(X[:, <span style=color:#ae81ff>0</span>], X[:, <span style=color:#ae81ff>1</span>], c<span style=color:#f92672>=</span>y, s<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>activation<span style=color:#e6db74>}</span><span style=color:#e6db74> Decision Boundary&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Input 1&#39;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Input 2&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>tight_layout()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p><img alt=loss_plot loading=lazy src=/ai/AI/3_4/loss_plot.png>
<img alt=prediction_plot loading=lazy src=/ai/AI/3_4/prediction.png></p><h3 id=sigmoid>Sigmoid<a hidden class=anchor aria-hidden=true href=#sigmoid>#</a></h3><p>適用時機：</p><ul><li>二元分類問題的<strong>輸出層</strong>（因為輸出範圍是 0~1，適合表示機率）</li><li>需要將輸出限制在 0~1 之間的情境</li><li>較淺的網路（1-2層）</li></ul><p>不建議用在：</p><ul><li>深層網路的中間層（因為容易發生梯度消失）</li><li>需要快速訓練的模型（因為計算exponential較慢）</li><li>對稱數據的問題（因為不是零中心化）</li></ul><h3 id=relu>ReLU<a hidden class=anchor aria-hidden=true href=#relu>#</a></h3><p>適用時機：</p><ul><li>深層網路的<strong>隱藏層</strong>（現代深度學習最常用）</li><li>CNN（卷積神經網路）</li><li>需要快速訓練的大型網路</li><li>稀疏激活是可接受的場景</li></ul><p>主要優點：</p><ul><li>計算簡單快速</li><li>能緩解梯度消失問題</li><li>能產生稀疏的表示（部分神經元輸出為0）</li></ul><h3 id=tanh>Tanh<a hidden class=anchor aria-hidden=true href=#tanh>#</a></h3><p>適用時機：</p><ul><li>需要零中心化輸出的場景（輸出範圍-1~1）</li><li>RNN（循環神經網路）的隱藏層</li><li>數據本身是歸一化/標準化的情況</li><li>需要較強的梯度在接近零的區域</li></ul><p>實際應用建議：</p><ol><li><strong>常見的最佳實踐組合</strong>：</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>NeuralNetwork</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>hidden_activation <span style=color:#f92672>=</span> ReLU    <span style=color:#75715e># 隱藏層使用ReLU</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>output_activation <span style=color:#f92672>=</span> Sigmoid  <span style=color:#75715e># 二分類輸出層使用Sigmoid</span>
</span></span></code></pre></div><ol start=2><li><strong>根據任務選擇</strong>：</li></ol><ul><li>分類問題：輸出層用Sigmoid（二分類）或Softmax（多分類）</li><li>回歸問題：輸出層可以不用激活函數</li><li>特徵提取：中間層優先使用ReLU</li></ul><ol start=3><li><strong>特殊情況</strong>：</li></ol><ul><li>處理序列數據（如RNN）：優先考慮Tanh</li><li>處理圖像數據（如CNN）：優先考慮ReLU</li><li>如果ReLU表現不佳：可以嘗試LeakyReLU或ELU</li></ul><h3 id=其它變體>其它變體<a hidden class=anchor aria-hidden=true href=#其它變體>#</a></h3><ol><li>Leaky ReLu</li></ol><ul><li>適用時機：<ul><li>當標準ReLU出現大量"死亡"神經元時</li><li>需要保留負值信息的場景</li><li>訓練初期希望網絡快速收斂</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    f(x) <span style=color:#f92672>=</span> x <span style=color:#66d9ef>if</span> x <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>else</span> αx <span style=color:#75715e># (α通常為0.01)</span>
</span></span></code></pre></div><ol start=2><li>ELU (Exponential Linear Unit)</li></ol><ul><li>適用時機：<ul><li>深層網絡需要更強的正則化</li><li>對噪聲較敏感的任務</li><li>需要更快收斂速度的場景</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    f(x) <span style=color:#f92672>=</span> x <span style=color:#66d9ef>if</span> x <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>else</span> α(exp(x) <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><ol start=3><li>SELU (Scaled ELU)</li></ol><ul><li>適用時機：<ul><li>深層全連接網絡</li><li>需要自歸一化特性的場景</li><li>希望避免額外的批標準化層</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    f(x) <span style=color:#f92672>=</span> λ(x <span style=color:#66d9ef>if</span> x <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>else</span> α(exp(x) <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>))
</span></span></code></pre></div><ol start=4><li>GELU(Gaussian Error Linear Unit)</li></ol><ul><li>適用時機：<ul><li>Transformer架構</li><li>BERT等預訓練模型</li><li>需要考慮輸入不確定性的場景</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    f(x) <span style=color:#f92672>=</span> x <span style=color:#f92672>*</span> P(X <span style=color:#960050;background-color:#1e0010>≤</span> x)
</span></span></code></pre></div><ol start=5><li>Swish</li></ol><ul><li>適用時機：<ul><li>深層模型</li><li>需要更好泛化性能的場景</li><li>計算資源充足的情況</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    f(x) <span style=color:#f92672>=</span> x <span style=color:#f92672>*</span> sigmoid(βx)
</span></span></code></pre></div><h3 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h3><ol><li><p>先嘗試 ReLU：</p><ul><li>最簡單且通常效果不錯</li><li>計算效率高</li><li>容易優化</li></ul></li><li><p>如果遇到問題，按順序嘗試：</p><ul><li>Dead ReLU問題 → Leaky ReLU</li><li>需要自歸一化 → SELU</li><li>用於Transformer → GELU</li><li>追求極致性能 → Swish</li></ul></li><li><p>特殊情況：</p><ul><li>需要處理時序數據 → ELU或SELU</li><li>計算資源受限 → 堅持使用ReLU</li><li>特別關注梯度流動 → Leaky ReLU或ELU</li></ul></li></ol><h2 id=優化器optimizers>優化器(Optimizers)<a hidden class=anchor aria-hidden=true href=#優化器optimizers>#</a></h2><ul><li><p>優化器是 backpropagation 時使用的更新策略，常見的優化器有：</p><h3 id=隨機梯度下降-sgdstochastic-gradient-descent>隨機梯度下降 SGD(Stochastic Gradient Descent)<a hidden class=anchor aria-hidden=true href=#隨機梯度下降-sgdstochastic-gradient-descent>#</a></h3><ul><li>最佳基本的優化器</li><li>適用時機：<ul><li>數據量大且資源有限</li><li>問題較簡單</li><li>需要較好的泛化性能</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Optimizer</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lr <span style=color:#f92672>=</span> learning_rate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update</span>(self, params, grads):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>NotImplementedError</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>SGD</span>(Optimizer):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update</span>(self, params, grads):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> param, grad <span style=color:#f92672>in</span> zip(params, grads):
</span></span><span style=display:flex><span>            param <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> grad
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> params
</span></span></code></pre></div><h3 id=動量-momemtum>動量 Momemtum<a hidden class=anchor aria-hidden=true href=#動量-momemtum>#</a></h3><ul><li>加入動量項(動態調整學習率)，幫助越過局部最小值</li><li>適用時機：<ul><li>梯度下降震盪嚴重時</li><li>需要加速收斂</li><li>有較多局部最小值時</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Momemtum</span>(Optimizer):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, momemtum<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__(learning_rate)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>momemtum <span style=color:#f92672>=</span> momemtum
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>velocities <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update</span>(self, params, grades):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>velocities <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>velocities <span style=color:#f92672>=</span> [np<span style=color:#f92672>.</span>zeros_like(param) <span style=color:#66d9ef>for</span> param <span style=color:#f92672>in</span> params]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i (param, grad) <span style=color:#f92672>in</span> enumerate(zip(params, grads)):
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>velocities[i] <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>momentum <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>velocities[i] <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> grad
</span></span><span style=display:flex><span>            param <span style=color:#f92672>+=</span> self<span style=color:#f92672>.</span>velocities[i]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> params
</span></span></code></pre></div><h3 id=自適應學習率-rmsprop>自適應學習率 RMSProp<a hidden class=anchor aria-hidden=true href=#自適應學習率-rmsprop>#</a></h3><ul><li>動態調整學習率</li><li>使用時機：<ul><li>處理非平穩問題</li><li>RNN訓練</li><li>梯度稀疏的問題</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>RMSProp</span>(Optimizer):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, decay_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.09</span>, epsilon<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-8</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__(learning_rate)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>decay_rate <span style=color:#f92672>=</span> decay_rate
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>epsilon <span style=color:#f92672>=</span> epsilon
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>cache <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update</span>(self, params, grads):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>cache <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>cache <span style=color:#f92672>=</span> [np<span style=color:#f92672>.</span>zeros_like(param) <span style=color:#66d9ef>for</span> param <span style=color:#f92672>in</span> params]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i, (param, grad) <span style=color:#f92672>in</span> enumerate(zip(params, grads)):
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>cache[i] <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>decay_rate <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>cache[i] <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>decay_rate) <span style=color:#f92672>*</span> grad <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>            param <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> grad <span style=color:#f92672>/</span> (np<span style=color:#f92672>.</span>sqrt(self<span style=color:#f92672>.</span>cache[i]) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>epsilon)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> params
</span></span></code></pre></div><h3 id=adam>Adam<a hidden class=anchor aria-hidden=true href=#adam>#</a></h3><ul><li>結合Momentum和RMSprop的優點</li><li>使用時機：<ul><li>深度學習的默認選擇</li><li>需要快速收斂</li><li>大多數問題</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Adam</span>(Optimizer):
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> __init__(self, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, beta1<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>, beta2<span style=color:#f92672>=</span><span style=color:#ae81ff>0.999</span>, epsilon<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-8</span>):
</span></span><span style=display:flex><span>    super()<span style=color:#f92672>.</span>__init__(learning_rate)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>beta1 <span style=color:#f92672>=</span> beta1
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>beta2 <span style=color:#f92672>=</span> beta2
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>epsilon <span style=color:#f92672>=</span> epsilon
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>m <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>v <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>t <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update</span>(self, params, grads):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>m <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>m <span style=color:#f92672>=</span> [np<span style=color:#f92672>.</span>zeros_like(param) <span style=color:#66d9ef>for</span> param <span style=color:#f92672>in</span> params]
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>v <span style=color:#f92672>=</span> [np<span style=color:#f92672>.</span>zeros_like(param) <span style=color:#66d9ef>for</span> param <span style=color:#f92672>in</span> params]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>t <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, (param, grad) <span style=color:#f92672>in</span> enumerate(zip(params, grads)):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>m[i] <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>beta1 <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>m[i] <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>beta1) <span style=color:#f92672>*</span> grad
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>v[i] <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>beta2 <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>v[i] <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>beta2) <span style=color:#f92672>*</span> grad<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        m_hat <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>m[i] <span style=color:#f92672>/</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>beta1<span style=color:#f92672>**</span>self<span style=color:#f92672>.</span>t)
</span></span><span style=display:flex><span>        v_hat <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>v[i] <span style=color:#f92672>/</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>beta2<span style=color:#f92672>**</span>self<span style=color:#f92672>.</span>t)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        param <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> m_hat <span style=color:#f92672>/</span> (np<span style=color:#f92672>.</span>sqrt(v_hat) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>epsilon)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> params
</span></span></code></pre></div><p><img alt=optimizer loading=lazy src=/ai/AI/3_4/optimizer.png></p><h3 id=summary-1>Summary<a hidden class=anchor aria-hidden=true href=#summary-1>#</a></h3><ul><li>首選Adam：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> Adam(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.001</span>, beta1<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>, beta2<span style=color:#f92672>=</span><span style=color:#ae81ff>0.999</span>)
</span></span></code></pre></div><ul><li>如果模型較大：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> AdamW(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.001</span>, weight_decay<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>)
</span></span></code></pre></div><ul><li>如果需要更好的泛化性能：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> SGD(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, momentum<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>)
</span></span></code></pre></div></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://intervalrain.github.io/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://intervalrain.github.io/ai/3_3/><span class=title>« 上一頁</span><br><span>[AI] 3-3. 使用 TensorFlow 與 Keras 函式庫</span>
</a><a class=next href=https://intervalrain.github.io/ai/3_5/><span class=title>下一頁 »</span><br><span>[AI] 3-5. 邏輯斯迴歸(logistic regression)</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://intervalrain.github.io/>Rain Hu's Workspace</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="複製";function s(){t.innerHTML="已複製！",setTimeout(()=>{t.innerHTML="複製"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>