<!doctype html><html lang=zh-tw dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[AI] 2-2. 張量 Tensor | Rain Hu's Workspace</title><meta name=keywords content="AI"><meta name=description content="The concept of tensor"><meta name=author content="Rain Hu"><link rel=canonical href=https://intervalrain.github.io/><meta name=google-site-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0cefe5a1d95e3d0f0cce057d37c60cd238d1a4af825090f831a18f21671f621d.css integrity="sha256-DO/lodlePQ8MzgV9N8YM0jjRpK+CUJD4MaGPIWcfYh0=" rel="preload stylesheet" as=style><link rel=icon href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=16x16 href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=32x32 href=https://intervalrain.github.io/images/rain.png><link rel=apple-touch-icon href=https://intervalrain.github.io/images/rain.png><link rel=mask-icon href=https://intervalrain.github.io/images/rain.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh-tw href=https://intervalrain.github.io/ai/2_2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css integrity=sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js integrity=sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script src=https://utteranc.es/client.js repo=intervalrain.github.io issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script><meta property="og:url" content="https://intervalrain.github.io/ai/2_2/"><meta property="og:site_name" content="Rain Hu's Workspace"><meta property="og:title" content="[AI] 2-2. 張量 Tensor"><meta property="og:description" content="The concept of tensor"><meta property="og:locale" content="zh-tw"><meta property="og:type" content="article"><meta property="article:section" content="ai"><meta property="article:published_time" content="2024-12-15T14:26:38+08:00"><meta property="article:modified_time" content="2024-12-15T14:26:38+08:00"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="[AI] 2-2. 張量 Tensor"><meta name=twitter:description content="The concept of tensor"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"[AI] 2-2. 張量 Tensor","item":"https://intervalrain.github.io/ai/2_2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[AI] 2-2. 張量 Tensor","name":"[AI] 2-2. 張量 Tensor","description":"The concept of tensor","keywords":["AI"],"articleBody":"張量 Tensor 是神經網路的資料表示法。 在 Python，我們常用 NumPy 陣列來作為機器學習的基礎資料結構，說 NumPy 陣列也稱為張量。\n張量的維、階、軸 階(rank): 又稱為軸(axis)，代表陣列的軸數。 維(dimension): 某一階的元素個數。 1. 純量 (0D 張量) 純量是單一個數值，也稱為 0 維張量 (0D Tensor)\nx = 5 2. 向量 (1D 張量) 向量是包含單一軸的數列，也稱為 1 維張量 (1D Tensor)\nx = [1, 2, 3] 3. 矩陣 (2D 張量) 矩陣是二維的數據結構，也稱為 2 維張量 (2D Tensor)\nx = [[1, 2, 3], [4, 5, 6]] 4. 3D 張量與高階張量 3D 張量：在二維矩陣基礎上增加一個深度維度，常用於處理圖片數據 (例如 RGB 通道)。 高階張量 (nD 張量)：當張量的維度超過 3D 時，用於更高維度的資料表示，例如影片、文字數據等。 x = [[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]]] 5. 張量的屬性 張量擁有幾個關鍵屬性，用於描述其結構與內容：\n軸 (Axes) 或 ndim\n張量的軸數量，也就是維度的數量。 純量的軸為 0，向量為 1，矩陣為 2，依此類推。 形狀 (Shape)\n每個軸的元素數量，形狀用 tuple 表示。 範例：矩陣 [[1, 2, 3], [4, 5, 6]] 的 shape 為 (2, 3)。 數據類型 (dtype) 張量中元素的資料類型，例如 float32、int32。 範例：x = np.array([1, 2, 3], dtype=np.float32)。 6. batch 的概念 在深度學習中，為了加速訓練過程，通常將多筆資料合併為一個批次 (Batch)，作為張量的第一維度。例如：\n一個批次包含 32 張圖片 (每張圖片為 28x28 像素)： shape = (32, 28, 28) 批次的概念允許高效處理資料，特別是在 GPU 上。\n7. 張量運算 keras.layers.Dense(512, activation='relu) 的概念可以比擬成：\n它是一個計算公式: output = relu(w * input + b)\nw 是權重(Weights)，用包學習資料間的關係。 b 是偏差(Bias)，用來調整輸出值。 relu 是一種激活函數，它讓輸出值變得非線性。 具體而言就是 (double x): double =\u003e max(0.0, x); 逐元素運算: NumPy 的運算中，很重要的就是逐元素(element-wise) 運算，意思是對張量中每個數值進行各自獨立的運算，如上述的加法與 relu 運算都是逐元素運算，非常適合平行處理，也就是向量化執行(vectorized implementations)。\n在 GPU 上執行 Tensorflow 程式碼時，會透過全面向量化的 CUDA 架構來執行逐元素運算，加快運算的效率。 張量擴張(Boardcasting): 在不考慮特例的情形，將兩個不同軸數的張量相加， NumPy 會對較小的張量進行擴張以匹配形狀較大的張量，包含：\n較小的張量會加入新的軸(擴張軸)以匹配較大的張量。 較小的張量會在這些新的軸上重複寫入元素，以匹配較大張量的形狀。 import numpy as np x = np.array((1,2,3,4,5)) y = np.array((1)) z = x + y z \u003e\u003e\u003e array([2,3,4,5,6]) 實際的流程如： y = np.array((1,2,3,4,5,6,7,8,9,10)) \u003e\u003e\u003e array([1,2,3,4,5,6,7,8,9,10]) y1 = np.expand_dims(y, axis=0) \u003e\u003e\u003e array([[1,2,3,4,5,6,7,8,9,10]]) y2 = np.concatenate([y1]*2, axios=0) \u003e\u003e\u003e array([[1,2,3,4,5,6,7,8,9,10], [1,2,3,4,5,6,7,8,9,10]]) 張量的點積運算\n在 NumPy 中會使用 np.dot 函式來進行點積運算 z = np.dot(x,y) 向量 x 與向量 y 做點積，在一般邏輯上可表示成： def naive_vector_dot(x, y): assert len(x.shape) == 1 assert len(y.shape) == 1 assert x.shape[0] == y.shape[0] z = 0. for i in range(x.shape[0]): z += x[i] * y[i] return z 矩陣 x 與向量 y 做點積，在一般邏輯上可表示成： def naive_matrix_vector_dot(x, y): assert len(x.shape) == 2 assert len(y.shape) == 1 assert x.shape[1] == y.shape[0] z = np.zeros(x.shape[0]) for i in range(x.shape[0]): for j in range(x.shape[1]): z[i] += x[i, j] * y[j] return z 張量重塑\n重塑就是調整張量各軸內的元素數，而張量元素總數不變的一種手法。 如在前一回使用過的資料前處理 train_images = train_images.reshap((60000, 28*28)) 就是一種重塑 常見的重塑還有矩陣轉置(transposition)，其實就是 x[i, :] =\u003e x[:, i] x = np.transpose(x) 張量運算的幾何解釋\n1. 平移(Translation) 平移是一種將空間中的點沿特定方向移動的操作。\n在幾何上，平移不是線性運算，需要用齊次座標表示。\n假設平移向量為 \\(\\mathbf{t} = \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix}\\)，則點 \\(\\mathbf{p} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}\\) 被平移後的位置為：\n$$ \\mathbf{p}’ = \\mathbf{p} + \\mathbf{t} = \\begin{bmatrix} x\\\\ y \\end{bmatrix} + \\begin{bmatrix} t_x\\\\ t_y \\end{bmatrix} = \\begin{bmatrix} x + t_x \\\\ y + t_y \\end{bmatrix} $$\n在齊次座標中，可以用矩陣形式表示為：\n$$ \\begin{bmatrix} x’ \\\\ y’ \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 0 \u0026 t_x \\\\ 0 \u0026 1 \u0026 t_y \\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} . \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} $$\n2. 旋轉 (Rotation) 旋轉是將點繞某個固定點(通常是原點)旋轉一個角度的操作。\n旋轉矩陣(以逆時針旋轉角度 \\(\\theta\\) ) 為： $$ \\mathbf{R} = \\begin{bmatrix} \\cos\\theta \u0026 -\\sin\\theta \\\\ \\sin\\theta \u0026 \\cos\\theta \\end{bmatrix} $$ 對於 對於點 \\(\\mathbf{p} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}\\)，旋轉後的位置為： $$ \\mathbf{p}’ = \\mathbf{R} \\cdot \\mathbf{p} = \\begin{bmatrix} \\cos\\theta \u0026 -\\sin\\theta \\\\ \\sin\\theta \u0026 \\cos\\theta \\end{bmatrix} . \\begin{bmatrix} x \\\\ y \\end{bmatrix} $$ 在齊次座標中表示為:\n$$ \\begin{bmatrix} x’ \\\\ y’ \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\cos\\theta \u0026 -\\sin\\theta \u0026 0 \\\\ \\sin\\theta \u0026 \\cos\\theta \u0026 0 \\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} . \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} $$\n3. 縮放 (Scaling) 縮放改變點的大小，可以分別對 \\(x\\) 和 \\(y\\) 軸進行不同比例的縮放。\n縮放矩陣為： $$ \\mathbf{S} = \\begin{bmatrix} s_x \u0026 0 \\\\ 0 \u0026 s_y \\end{bmatrix} $$\n對於點 \\(\\mathbf{p} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}\\)，縮放後的位置為：\n$$ \\mathbf{p}’ = \\mathbf{S} \\cdot \\mathbf{p} = \\begin{bmatrix} s_x \u0026 0 \\\\ 0 \u0026 s_y \\end{bmatrix} . \\begin{bmatrix} x \\\\ y \\end{bmatrix} $$\n在齊次座標中表示為： $$ \\begin{bmatrix} x’ \\\\ y’ \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} s_x \u0026 0 \u0026 0 \\\\ 0 \u0026 s_y \u0026 0 \\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} . \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} $$\n4. 線性變換 (Linear Transformation) 線性變換是縮放、旋轉或剪切等操作的統稱，可以用矩陣表示。\n一般的線性變換矩陣為：\n$$ \\mathbf{A} = \\begin{bmatrix} a_{11} \u0026 a_{12} \\\\ a_{21} \u0026 a_{22} \\end{bmatrix} $$\n對於點 \\(\\mathbf{p} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}\\)，線性變換後的位置為：\n$$ \\mathbf{p}’ = \\mathbf{A} \\cdot \\mathbf{p} = \\begin{bmatrix} a_{11} \u0026 a_{12} \\\\ a_{21} \u0026 a_{22} \\end{bmatrix} . \\begin{bmatrix} x \\\\ y \\end{bmatrix} $$\n在齊次座標中： $$ \\begin{bmatrix} x’ \\\\ y’ \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a_{11} \u0026 a_{12} \u0026 0 \\\\ a_{21} \u0026 a_{22} \u0026 0 \\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} . \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} $$\n5. 仿射變換 (Affine Transformation) 仿射變換是線性變換加上平移的組合。\n在齊次座標中，仿射變換可表示為：\n$$ \\mathbf{T} = \\begin{bmatrix} a_{11} \u0026 a_{12} \u0026 t_x \\\\ a_{21} \u0026 a_{22} \u0026 t_y \\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} $$\n對於點 \\(\\mathbf{p} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}\\)，仿射變換後的位置為： $$ \\begin{bmatrix} x’ \\\\ y’ \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a_{11} \u0026 a_{12} \u0026 t_x \\\\ a_{21} \u0026 a_{22} \u0026 t_y \\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} . \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} $$\n由於仿射是線性變換(矩陣的點積)與平移運算(向量加法)的結合，其實就是密集層會用到的 \\(\\mathbf{y}=\\mathbf{W}\\cdot\\mathbf{x}+\\mathbf{b}\\)。一個沒有激活函數的密集層就是仿射層。 換句話說，在沒有激活函數的狀況下進行了無數次的仿射變換，可以等同於一次仿射變換 $$ \\begin{align*} y_1 \u0026= W_1 \\cdot x + b_1 \\\\ y_2 \u0026= W_2 \\cdot y_1 + b_2 \\end{align*} $$ 將 \\(y_1\\) 代入 \\(y_2\\) 的公式： $$ \\begin{align*} y_2 \u0026= W_2 \\cdot (W_1 \\cdot x + b_1) + b_2 \\\\ y_2 \u0026= W_2 \\cdot W_1 \\cdot x + W_2 \\cdot b_1 + b_2 \\end{align*} $$ 設 \\(W = W_2 \\cdot W_1\\) 和 \\(b = W_2 \\cdot b_1 + b_2\\)，則： $$ y_2 = W \\cdot x + b $$ 這個結論非常重要，代表：如果我們建構了多個密集層的神經網路，卻沒有搭配任何的激活函數，其效果等同於一個密集層，換言之，這個「深層」的神經網路模型不過是一個線性模型。 ","wordCount":"840","inLanguage":"zh-tw","datePublished":"2024-12-15T14:26:38+08:00","dateModified":"2024-12-15T14:26:38+08:00","author":{"@type":"Person","name":"Rain Hu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://intervalrain.github.io/ai/2_2/"},"publisher":{"@type":"Organization","name":"Rain Hu's Workspace","logo":{"@type":"ImageObject","url":"https://intervalrain.github.io/images/rain.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://intervalrain.github.io/ accesskey=h title="Rain Hu's Workspace (Alt + H)"><img src=https://intervalrain.github.io/images/rain.png alt aria-label=logo height=35>Rain Hu's Workspace</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://intervalrain.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://intervalrain.github.io/aboutme title="About me"><span>About me</span></a></li><li><a href=https://intervalrain.github.io/archives title=Archives><span>Archives</span></a></li><li><a href=https://intervalrain.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://intervalrain.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://intervalrain.github.io/csharp/csharp title=C#><span>C#</span></a></li><li><a href=https://intervalrain.github.io/csindex title=CS><span>CS</span></a></li><li><a href=https://intervalrain.github.io/leetcode title=LeetCode><span>LeetCode</span></a></li><li><a href=https://intervalrain.github.io/ai title=AI><span>AI</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://intervalrain.github.io/>首頁</a></div><h1 class="post-title entry-hint-parent">[AI] 2-2. 張量 Tensor</h1><div class=post-description>The concept of tensor</div><div class=post-meta><span title='2024-12-15 14:26:38 +0800 +0800'>December 15, 2024</span>&nbsp;·&nbsp;4 分鐘&nbsp;·&nbsp;Rain Hu&nbsp;|&nbsp;<a href=https://github.com/intervalrain/intervalrain.github.io/tree/main/content//AI/2_2.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目錄</span></summary><div class=inner><ul><li><a href=#%e5%bc%b5%e9%87%8f%e7%9a%84%e7%b6%ad%e9%9a%8e%e8%bb%b8 aria-label=張量的維、階、軸>張量的維、階、軸</a><ul><li><a href=#1-%e7%b4%94%e9%87%8f-0d-%e5%bc%b5%e9%87%8f aria-label="1. 純量 (0D 張量)">1. 純量 (0D 張量)</a></li><li><a href=#2-%e5%90%91%e9%87%8f-1d-%e5%bc%b5%e9%87%8f aria-label="2. 向量 (1D 張量)">2. 向量 (1D 張量)</a></li><li><a href=#3-%e7%9f%a9%e9%99%a3-2d-%e5%bc%b5%e9%87%8f aria-label="3. 矩陣 (2D 張量)">3. 矩陣 (2D 張量)</a></li><li><a href=#4-3d-%e5%bc%b5%e9%87%8f%e8%88%87%e9%ab%98%e9%9a%8e%e5%bc%b5%e9%87%8f aria-label="4. 3D 張量與高階張量">4. 3D 張量與高階張量</a></li><li><a href=#5-%e5%bc%b5%e9%87%8f%e7%9a%84%e5%b1%ac%e6%80%a7 aria-label="5. 張量的屬性">5. 張量的屬性</a></li><li><a href=#6-batch-%e7%9a%84%e6%a6%82%e5%bf%b5 aria-label="6. batch 的概念">6. batch 的概念</a></li><li><a href=#7-%e5%bc%b5%e9%87%8f%e9%81%8b%e7%ae%97 aria-label="7. 張量運算">7. 張量運算</a><ul><li><a href=#1-%e5%b9%b3%e7%a7%bbtranslation aria-label="1. 平移(Translation)">1. 平移(Translation)</a></li><li><a href=#2-%e6%97%8b%e8%bd%89-rotation aria-label="2. 旋轉 (Rotation)">2. 旋轉 (Rotation)</a></li><li><a href=#3-%e7%b8%ae%e6%94%be-scaling aria-label="3. 縮放 (Scaling)">3. 縮放 (Scaling)</a></li><li><a href=#4-%e7%b7%9a%e6%80%a7%e8%ae%8a%e6%8f%9b-linear-transformation aria-label="4. 線性變換 (Linear Transformation)">4. 線性變換 (Linear Transformation)</a></li><li><a href=#5-%e4%bb%bf%e5%b0%84%e8%ae%8a%e6%8f%9b-affine-transformation aria-label="5. 仿射變換 (Affine Transformation)">5. 仿射變換 (Affine Transformation)</a></li></ul></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>張量 Tensor 是神經網路的資料表示法。
在 Python，我們常用 NumPy 陣列來作為機器學習的基礎資料結構，說 NumPy 陣列也稱為張量。</p><h2 id=張量的維階軸>張量的維、階、軸<a hidden class=anchor aria-hidden=true href=#張量的維階軸>#</a></h2><ul><li>階(rank): 又稱為軸(axis)，代表陣列的軸數。</li><li>維(dimension): 某一階的元素個數。</li></ul><h3 id=1-純量-0d-張量>1. 純量 (0D 張量)<a hidden class=anchor aria-hidden=true href=#1-純量-0d-張量>#</a></h3><p>純量是單一個數值，也稱為 0 維張量 (0D Tensor)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>
</span></span></code></pre></div><h3 id=2-向量-1d-張量>2. 向量 (1D 張量)<a hidden class=anchor aria-hidden=true href=#2-向量-1d-張量>#</a></h3><p>向量是包含單一軸的數列，也稱為 1 維張量 (1D Tensor)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>]
</span></span></code></pre></div><h3 id=3-矩陣-2d-張量>3. 矩陣 (2D 張量)<a hidden class=anchor aria-hidden=true href=#3-矩陣-2d-張量>#</a></h3><p>矩陣是二維的數據結構，也稱為 2 維張量 (2D Tensor)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> [[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>],
</span></span><span style=display:flex><span>     [<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>]]
</span></span></code></pre></div><h3 id=4-3d-張量與高階張量>4. 3D 張量與高階張量<a hidden class=anchor aria-hidden=true href=#4-3d-張量與高階張量>#</a></h3><ul><li>3D 張量：在二維矩陣基礎上增加一個深度維度，常用於處理圖片數據 (例如 RGB 通道)。</li><li>高階張量 (nD 張量)：當張量的維度超過 3D 時，用於更高維度的資料表示，例如影片、文字數據等。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> [[[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>], 
</span></span><span style=display:flex><span>      [<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>], 
</span></span><span style=display:flex><span>      [<span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>9</span>]],
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>     [[<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>11</span>, <span style=color:#ae81ff>12</span>], 
</span></span><span style=display:flex><span>      [<span style=color:#ae81ff>13</span>, <span style=color:#ae81ff>14</span>, <span style=color:#ae81ff>15</span>], 
</span></span><span style=display:flex><span>      [<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>17</span>, <span style=color:#ae81ff>18</span>]]]
</span></span></code></pre></div><h3 id=5-張量的屬性>5. 張量的屬性<a hidden class=anchor aria-hidden=true href=#5-張量的屬性>#</a></h3><p>張量擁有幾個關鍵屬性，用於描述其結構與內容：</p><ol><li>軸 (Axes) 或 ndim<br>張量的軸數量，也就是維度的數量。<ul><li>純量的軸為 0，向量為 1，矩陣為 2，依此類推。</li></ul></li><li>形狀 (Shape)<br>每個軸的元素數量，形狀用 tuple 表示。<ul><li>範例：矩陣 [[1, 2, 3], [4, 5, 6]] 的 shape 為 (2, 3)。</li></ul></li><li>數據類型 (dtype)
張量中元素的資料類型，例如 float32、int32。<ul><li>範例：x = np.array([1, 2, 3], dtype=np.float32)。</li></ul></li></ol><h3 id=6-batch-的概念>6. batch 的概念<a hidden class=anchor aria-hidden=true href=#6-batch-的概念>#</a></h3><p>在深度學習中，為了加速訓練過程，通常將多筆資料合併為一個批次 (Batch)，作為張量的第一維度。例如：</p><p>一個批次包含 32 張圖片 (每張圖片為 28x28 像素)：
shape = (32, 28, 28)
批次的概念允許高效處理資料，特別是在 GPU 上。</p><h3 id=7-張量運算>7. 張量運算<a hidden class=anchor aria-hidden=true href=#7-張量運算>#</a></h3><p><code>keras.layers.Dense(512, activation='relu)</code> 的概念可以比擬成：</p><ul><li><p>它是一個計算公式: <code>output = relu(w * input + b)</code></p><ul><li><code>w</code> 是權重(Weights)，用包學習資料間的關係。</li><li><code>b</code> 是偏差(Bias)，用來調整輸出值。</li><li><code>relu</code> 是一種激活函數，它讓輸出值變得非線性。<ul><li>具體而言就是 <code>(double x): double => max(0.0, x);</code></li></ul></li></ul></li><li><p>逐元素運算: NumPy 的運算中，很重要的就是逐元素(element-wise) 運算，意思是對張量中每個數值進行各自獨立的運算，如上述的加法與 relu 運算都是逐元素運算，非常適合平行處理，也就是<strong>向量化執行(vectorized implementations)</strong>。</p><ul><li>在 GPU 上執行 Tensorflow 程式碼時，會透過全面向量化的 CUDA 架構來執行逐元素運算，加快運算的效率。</li></ul></li><li><p>張量擴張(Boardcasting): 在不考慮特例的情形，將兩個不同軸數的張量相加， NumPy 會對較小的張量進行擴張以匹配形狀較大的張量，包含：</p><ul><li>較小的張量會加入新的軸(擴張軸)以匹配較大的張量。</li><li>較小的張量會在這些新的軸上重複寫入元素，以匹配較大張量的形狀。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array((<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>))
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array((<span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> y
</span></span><span style=display:flex><span>z
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> array([<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>])
</span></span></code></pre></div><ul><li>實際的流程如：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array((<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>,<span style=color:#ae81ff>7</span>,<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>9</span>,<span style=color:#ae81ff>10</span>))
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> array([<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>,<span style=color:#ae81ff>7</span>,<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>9</span>,<span style=color:#ae81ff>10</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>expand_dims(y, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> array([[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>,<span style=color:#ae81ff>7</span>,<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>9</span>,<span style=color:#ae81ff>10</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate([y1]<span style=color:#f92672>*</span><span style=color:#ae81ff>2</span>, axios<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> array([[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>,<span style=color:#ae81ff>7</span>,<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>9</span>,<span style=color:#ae81ff>10</span>],
</span></span><span style=display:flex><span>           [<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>,<span style=color:#ae81ff>7</span>,<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>9</span>,<span style=color:#ae81ff>10</span>]])
</span></span></code></pre></div></li><li><p>張量的點積運算</p><ul><li>在 NumPy 中會使用 <code>np.dot</code> 函式來進行點積運算</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>z <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(x,y)
</span></span></code></pre></div><ul><li>向量 x 與向量 y 做點積，在一般邏輯上可表示成：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>naive_vector_dot</span>(x, y):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> len(x<span style=color:#f92672>.</span>shape) <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> len(y<span style=color:#f92672>.</span>shape) <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> x<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>==</span> y<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    z <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(x<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>        z <span style=color:#f92672>+=</span> x[i] <span style=color:#f92672>*</span> y[i]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> z
</span></span></code></pre></div><ul><li>矩陣 x 與向量 y 做點積，在一般邏輯上可表示成：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>naive_matrix_vector_dot</span>(x, y):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> len(x<span style=color:#f92672>.</span>shape) <span style=color:#f92672>==</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> len(y<span style=color:#f92672>.</span>shape) <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> x<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>==</span> y<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    z <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros(x<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(x<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(x<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]):
</span></span><span style=display:flex><span>            z[i] <span style=color:#f92672>+=</span> x[i, j] <span style=color:#f92672>*</span> y[j]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> z
</span></span></code></pre></div></li><li><p>張量重塑</p><ul><li>重塑就是調整張量各軸內的元素數，而張量元素總數不變的一種手法。</li><li>如在前一回使用過的資料前處理 <code>train_images = train_images.reshap((60000, 28*28))</code> 就是一種重塑</li><li>常見的重塑還有<strong>矩陣轉置(transposition)</strong>，其實就是 <code>x[i, :] => x[:, i]</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>transpose(x)
</span></span></code></pre></div></li><li><p>張量運算的幾何解釋</p></li></ul><h4 id=1-平移translation>1. 平移(Translation)<a hidden class=anchor aria-hidden=true href=#1-平移translation>#</a></h4><p>平移是一種將空間中的點沿特定方向移動的操作。<br>在幾何上，平移不是線性運算，需要用<strong>齊次座標</strong>表示。<br>假設平移向量為 \(\mathbf{t} = \begin{bmatrix} t_x \\ t_y \end{bmatrix}\)，則點 \(\mathbf{p} = \begin{bmatrix} x \\ y \end{bmatrix}\) 被平移後的位置為：</p><p>$$
\mathbf{p}&rsquo; = \mathbf{p} + \mathbf{t} =
\begin{bmatrix}
x\\
y
\end{bmatrix}
+
\begin{bmatrix}
t_x\\
t_y
\end{bmatrix}
=
\begin{bmatrix}
x + t_x \\
y + t_y
\end{bmatrix}
$$</p><p>在齊次座標中，可以用矩陣形式表示為：</p><p>$$
\begin{bmatrix}
x&rsquo; \\
y&rsquo; \\
1
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & t_x \\
0 & 1 & t_y \\
0 & 0 & 1
\end{bmatrix}
.
\begin{bmatrix}
x \\
y \\
1
\end{bmatrix}
$$</p><h4 id=2-旋轉-rotation>2. 旋轉 (Rotation)<a hidden class=anchor aria-hidden=true href=#2-旋轉-rotation>#</a></h4><p>旋轉是將點繞某個固定點(通常是原點)旋轉一個角度的操作。<br>旋轉矩陣(以逆時針旋轉角度 \(\theta\) ) 為：
$$
\mathbf{R} =
\begin{bmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{bmatrix}
$$
對於
對於點 \(\mathbf{p} = \begin{bmatrix} x \\ y \end{bmatrix}\)，旋轉後的位置為：
$$
\mathbf{p}&rsquo; = \mathbf{R} \cdot \mathbf{p} =
\begin{bmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{bmatrix}
.
\begin{bmatrix}
x \\
y
\end{bmatrix}
$$
在齊次座標中表示為:</p><p>$$
\begin{bmatrix}
x&rsquo; \\
y&rsquo; \\
1
\end{bmatrix}
=
\begin{bmatrix}
\cos\theta & -\sin\theta & 0 \\
\sin\theta & \cos\theta & 0 \\
0 & 0 & 1
\end{bmatrix}
.
\begin{bmatrix}
x \\
y \\
1
\end{bmatrix}
$$</p><h4 id=3-縮放-scaling>3. 縮放 (Scaling)<a hidden class=anchor aria-hidden=true href=#3-縮放-scaling>#</a></h4><p>縮放改變點的大小，可以分別對 \(x\) 和 \(y\) 軸進行不同比例的縮放。</p><p>縮放矩陣為：
$$
\mathbf{S} =
\begin{bmatrix}
s_x & 0 \\
0 & s_y
\end{bmatrix}
$$</p><p>對於點 \(\mathbf{p} = \begin{bmatrix} x \\ y \end{bmatrix}\)，縮放後的位置為：<br>$$
\mathbf{p}&rsquo; = \mathbf{S} \cdot \mathbf{p} =
\begin{bmatrix}
s_x & 0 \\
0 & s_y
\end{bmatrix}
.
\begin{bmatrix}
x \\
y
\end{bmatrix}
$$</p><p>在齊次座標中表示為：
$$
\begin{bmatrix}
x&rsquo; \\
y&rsquo; \\
1
\end{bmatrix}
=
\begin{bmatrix}
s_x & 0 & 0 \\
0 & s_y & 0 \\
0 & 0 & 1
\end{bmatrix}
.
\begin{bmatrix}
x \\
y \\
1
\end{bmatrix}
$$</p><h4 id=4-線性變換-linear-transformation>4. 線性變換 (Linear Transformation)<a hidden class=anchor aria-hidden=true href=#4-線性變換-linear-transformation>#</a></h4><p>線性變換是縮放、旋轉或剪切等操作的統稱，可以用矩陣表示。<br>一般的線性變換矩陣為：<br>$$
\mathbf{A} =
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{bmatrix}
$$</p><p>對於點 \(\mathbf{p} = \begin{bmatrix} x \\ y \end{bmatrix}\)，線性變換後的位置為：<br>$$
\mathbf{p}&rsquo; = \mathbf{A} \cdot \mathbf{p} =
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{bmatrix}
.
\begin{bmatrix}
x \\
y
\end{bmatrix}
$$</p><p>在齊次座標中：
$$
\begin{bmatrix}
x&rsquo; \\
y&rsquo; \\
1
\end{bmatrix}
=
\begin{bmatrix}
a_{11} & a_{12} & 0 \\
a_{21} & a_{22} & 0 \\
0 & 0 & 1
\end{bmatrix}
.
\begin{bmatrix}
x \\
y \\
1
\end{bmatrix}
$$</p><h4 id=5-仿射變換-affine-transformation>5. 仿射變換 (Affine Transformation)<a hidden class=anchor aria-hidden=true href=#5-仿射變換-affine-transformation>#</a></h4><p>仿射變換是線性變換加上平移的組合。<br>在齊次座標中，仿射變換可表示為：<br>$$
\mathbf{T} =
\begin{bmatrix}
a_{11} & a_{12} & t_x \\
a_{21} & a_{22} & t_y \\
0 & 0 & 1
\end{bmatrix}
$$</p><p>對於點 \(\mathbf{p} = \begin{bmatrix} x \\ y \end{bmatrix}\)，仿射變換後的位置為：
$$
\begin{bmatrix}
x&rsquo; \\
y&rsquo; \\
1
\end{bmatrix}
=
\begin{bmatrix}
a_{11} & a_{12} & t_x \\
a_{21} & a_{22} & t_y \\
0 & 0 & 1
\end{bmatrix}
.
\begin{bmatrix}
x \\
y \\
1
\end{bmatrix}
$$</p><ul><li>由於仿射是線性變換(矩陣的點積)與平移運算(向量加法)的結合，其實就是密集層會用到的 \(\mathbf{y}=\mathbf{W}\cdot\mathbf{x}+\mathbf{b}\)。一個沒有激活函數的密集層就是仿射層。</li><li>換句話說，在沒有激活函數的狀況下進行了無數次的仿射變換，可以等同於一次仿射變換
$$
\begin{align*}
y_1 &= W_1 \cdot x + b_1 \\
y_2 &= W_2 \cdot y_1 + b_2
\end{align*}
$$
將 \(y_1\) 代入 \(y_2\) 的公式：
$$
\begin{align*}
y_2 &= W_2 \cdot (W_1 \cdot x + b_1) + b_2 \\
y_2 &= W_2 \cdot W_1 \cdot x + W_2 \cdot b_1 + b_2
\end{align*}
$$
設 \(W = W_2 \cdot W_1\) 和 \(b = W_2 \cdot b_1 + b_2\)，則：
$$
y_2 = W \cdot x + b
$$</li><li>這個結論非常重要，代表：如果我們建構了多個密集層的神經網路，卻沒有搭配任何的激活函數，其效果等同於一個密集層，換言之，這個「深層」的神經網路模型不過是一個線性模型。</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://intervalrain.github.io/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://intervalrain.github.io/ai/2_1/><span class=title>« 上一頁</span><br><span>[AI] 2-1. 初試神經網路-手寫辨識 mnist</span>
</a><a class=next href=https://intervalrain.github.io/ai/2_3/><span class=title>下一頁 »</span><br><span>[AI] 2-3. 優化器 Optimizer</span></a></nav><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://intervalrain.github.io/>Rain Hu's Workspace</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="複製";function s(){t.innerHTML="已複製！",setTimeout(()=>{t.innerHTML="複製"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>