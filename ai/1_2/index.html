<!doctype html><html lang=zh-tw dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[AI] 1-2. 機器學習的基礎技術 | Rain Hu's Workspace</title>
<meta name=keywords content="AI"><meta name=description content="The fundamentals of Machine Learning"><meta name=author content="Rain Hu"><link rel=canonical href=https://intervalrain.github.io/><meta name=google-site-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.662816b9df27c772d2b97c5f5f6bf4f2c5531051a330015f0ad4135736d0e56a.css integrity="sha256-ZigWud8nx3LSuXxfX2v08sVTEFGjMAFfCtQTVzbQ5Wo=" rel="preload stylesheet" as=style><link rel=icon href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=16x16 href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=32x32 href=https://intervalrain.github.io/images/rain.png><link rel=apple-touch-icon href=https://intervalrain.github.io/images/rain.png><link rel=mask-icon href=https://intervalrain.github.io/images/rain.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh-tw href=https://intervalrain.github.io/ai/1_2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css integrity=sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js integrity=sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script src=https://utteranc.es/client.js repo=intervalrain.github.io issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script><meta property="og:url" content="https://intervalrain.github.io/ai/1_2/"><meta property="og:site_name" content="Rain Hu's Workspace"><meta property="og:title" content="[AI] 1-2. 機器學習的基礎技術"><meta property="og:description" content="The fundamentals of Machine Learning"><meta property="og:locale" content="zh-tw"><meta property="og:type" content="article"><meta property="article:section" content="ai"><meta property="article:published_time" content="2024-12-03T15:42:52+08:00"><meta property="article:modified_time" content="2024-12-03T15:42:52+08:00"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="[AI] 1-2. 機器學習的基礎技術"><meta name=twitter:description content="The fundamentals of Machine Learning"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"[AI] 1-2. 機器學習的基礎技術","item":"https://intervalrain.github.io/ai/1_2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[AI] 1-2. 機器學習的基礎技術","name":"[AI] 1-2. 機器學習的基礎技術","description":"The fundamentals of Machine Learning","keywords":["AI"],"articleBody":"機率建模(Probabilistic modeling) 單純貝氏演算法(Naive Bayes thorem) 單純貝氏定理是一種基於機率理論的分類方法，適用於文本分類等任務，其核心概念是基於特徵條件獨立的假設來計算後驗機率。\n以下是一個使用 Python 使現文本分類的範例\nfrom sklearn.naive_bayes import MultinomialNB from sklearn.feature_extraction.text import CountVectorizer # 示範文本數據 texts = [\"這部電影很好看\", \"服務很差\", \"餐點美味\", \"環境很糟\"] labels = [\"正面\", \"負面\", \"正面\", \"負面\"] # 將文本轉換為特徵向量 vectorizer = CountVectorizer() X = vectorizer.fit_transform(texts) # 訓練模型 clf = MultinomialNB() clf.fit(X, labels) # 預測新文本 new_text = [\"這家餐廳很棒\"] new_X = vectorizer.transform(new_text) prediction = clf.predict(new_X) 邏輯迴歸(logistic regression, logreg) 邏輯迴歸是一種二元分類問題的基礎演算法。儘管名稱中有「迴歸」，但實際上是一個分類模型，通過 sigmoid 函數 將線性預測轉換成機率值。\nfrom sklearn.linear_model import LogisticRegression import numpy as np # 示範數據 X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]]) y = np.array([0, 0, 1, 1]) # 創建和訓練模型 model = LogisticRegression() model.fit(X, y) # 預測 prediction = model.predict([[2.5, 3.5]]) 早期的神經網路 1980年代的神經網路發展奠定了現代深度學習的基礎。反向傳播算法的發明是一個重要突破，它提供了一種有效的方法來訓練多層神經網路。\n1989年 Yann LeCun 結合卷積神經網路(Convolutional Neural Network, CNN) 和反向傳播，應用於手寫數字分類問題。\n反向傳播(Backpropagation) 反向傳播是一種梯度下降最佳化(gradient-descent-optimization)的演算法，用於調整神經網路中的權重。其過程可以簡化為：\n前向傳播：輸入數據通過網路產生預測 計算誤差：比較預測值與實際值 反向傳播誤差：從輸出層向輸入層計算梯度 更新權重：使用梯度下降來優化網路參數 以下是一個使用 PyTorch 實現簡單神經網路的例子：\nimport torch import torch.nn as nn class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(784, 128) self.layer2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.layer1(x)) x = self.layer2(x) return x # 創建模型 model = SimpleNN() Kernel methods 與 SVM 支持向量機（Support Vector Methods, SVM）是一種強大的分類算法，通過在高維空間中尋找最優分類超平面來工作。\n目標是兩種類別的資料點間，找到最佳決策邊界(decision boundaries) 步驟： 映射到高維空間，決策邊界通常是一個超曲面 找到最大化邊界(maximizing the margin)，以製造最大的 margin Kernel tricks 將資料映射到高維空間在理論上可行，但在實際上卻常常很難處理(因為維度高，計算負載變大)\n我們不必在高維度空間把每一個資料點都做座標轉換，然後在高維度上計算決策超平面，取而代之，只要計算高維空間中點與點之間的距離即可。\nkernel function 定義是「初始空間中任意兩點」映射到「目標表示空間中對應點之間的距離」。 白話解釋，可以將 Kernel tricks 理解為一種「偷懶」的聰明方法。想像我們要分類兩組點，在二維平面上無法用直線分開，需要將點映射到三維空間。但直接計算三維座標會很耗時，kernel tricks 讓我們可以直接計算點之間在高維空間的距離，而不需要真的去計算高維座標。 from sklearn.svm import SVC # 創建SVM分類器 svm = SVC(kernel='rbf') # 使用RBF核函數 svm.fit(X_train, y_train) # 預測 predictions = svm.predict(X_test) 但在 SVM 中，只有決策超平面是學習而來的，kernel function 是需要透過人工設計的。SVM 在簡單分類問題上表現很好，是少數具備廣泛理論支持也經得起嚴謹數學分析的機器學習演算法。但 SVM 難以擴展到大型資料集。SVM 是一種淺層方法，需要有效的手動萃取有用的表示法，該步驟稱為特徵工程(feature engineering)。 打個比方，圖片的原始像素無法很好的分類手寫數字，需要人工找到有用的表示法，如像素直方圖。 決策樹、隨機森林和梯度提升機器 決策樹 Decision Tree 決策樹模擬人類決策過程，通過一系列問題將數據分類。它像是一個流程圖，從根節點開始，根據特徵值選擇分支，最終到達葉節點得到預測結果。\nfrom sklearn.tree import DecisionTreeClassifier # 創建決策樹 tree = DecisionTreeClassifier(max_depth=3) tree.fit(X_train, y_train) 隨機森林 Random Forest 隨機森林是多個決策樹的集成，每棵樹使用隨機選擇的特徵和數據樣本進行訓練。最終預測是所有樹的投票結果。\nfrom sklearn.ensemble import RandomForestClassifier # 創建隨機森林 rf = RandomForestClassifier(n_estimators=100) rf.fit(X_train, y_train) 梯度提升機 Gradient Boosting Machines (GBM) 梯度提升機是一種迭代式的集成方法，每次迭代都試圖糾正之前模型的錯誤。\nfrom sklearn.ensemble import GradientBoostingClassifier # 創建梯度提升機 gbm = GradientBoostingClassifier(n_estimators=100) gbm.fit(X_train, y_train) 深度學習最大的優勢 讓特徵工程變成自動化 淺層學習有快速遞減(fast-diminishing) 的現象，因為「在三層模型中最佳的第一表示層，並非單層或雙層模型中最佳的第一表示層。」。 想像你在蓋一棟三層樓的房子。如果你先蓋一層樓的房子，再加蓋第二層，最後加蓋第三層，這樣的結果往往不如一開始就規劃好三層樓一起設計來得好。因為當你一開始就知道要蓋三層，你可以更好地規劃整體結構，確保每一層都能最好地支持整棟建築。\n聯合學習：調整模型內部一個參數，其它相關參數也會自動調整。 現代機器學習框架應用 主流框架比較 Scikit-learn：適用於傳統機器學習算法 優點：API簡單，適合快速原型開發 應用：特徵工程、傳統算法實現 TensorFlow/Keras： 優點：生產環境部署成熟，生態系統完整 應用：深度學習模型開發和部署 PyTorch： 優點：動態計算圖，研究友好 應用：研究實驗，快速迭代開發 ","wordCount":"281","inLanguage":"zh-tw","datePublished":"2024-12-03T15:42:52+08:00","dateModified":"2024-12-03T15:42:52+08:00","author":{"@type":"Person","name":"Rain Hu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://intervalrain.github.io/ai/1_2/"},"publisher":{"@type":"Organization","name":"Rain Hu's Workspace","logo":{"@type":"ImageObject","url":"https://intervalrain.github.io/images/rain.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://intervalrain.github.io/ accesskey=h title="Rain Hu's Workspace (Alt + H)"><img src=https://intervalrain.github.io/images/rain.png alt aria-label=logo height=35>Rain Hu's Workspace</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://intervalrain.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://intervalrain.github.io/aboutme title="About me"><span>About me</span></a></li><li><a href=https://intervalrain.github.io/archives title=Archives><span>Archives</span></a></li><li><a href=https://intervalrain.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://intervalrain.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://intervalrain.github.io/csharp/csharp title=C#><span>C#</span></a></li><li><a href=https://intervalrain.github.io/csindex title=CS><span>CS</span></a></li><li><a href=https://intervalrain.github.io/leetcode title=LeetCode><span>LeetCode</span></a></li><li><a href=https://intervalrain.github.io/ai title=AI><span>AI</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://intervalrain.github.io/>首頁</a></div><h1 class="post-title entry-hint-parent">[AI] 1-2. 機器學習的基礎技術</h1><div class=post-description>The fundamentals of Machine Learning</div><div class=post-meta><span title='2024-12-03 15:42:52 +0800 +0800'>December 3, 2024</span>&nbsp;·&nbsp;2 分鐘&nbsp;·&nbsp;Rain Hu&nbsp;|&nbsp;<a href=https://github.com/intervalrain/intervalrain.github.io/tree/main/content//AI/1_2.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目錄</span></summary><div class=inner><ul><li><a href=#%e6%a9%9f%e7%8e%87%e5%bb%ba%e6%a8%a1probabilistic-modeling aria-label="機率建模(Probabilistic modeling)">機率建模(Probabilistic modeling)</a><ul><li><a href=#%e5%96%ae%e7%b4%94%e8%b2%9d%e6%b0%8f%e6%bc%94%e7%ae%97%e6%b3%95naive-bayes-thorem aria-label="單純貝氏演算法(Naive Bayes thorem)">單純貝氏演算法(Naive Bayes thorem)</a></li><li><a href=#%e9%82%8f%e8%bc%af%e8%bf%b4%e6%ad%b8logistic-regression-logreg aria-label="邏輯迴歸(logistic regression, logreg)">邏輯迴歸(logistic regression, logreg)</a></li></ul></li><li><a href=#%e6%97%a9%e6%9c%9f%e7%9a%84%e7%a5%9e%e7%b6%93%e7%b6%b2%e8%b7%af aria-label=早期的神經網路>早期的神經網路</a><ul><li><a href=#%e5%8f%8d%e5%90%91%e5%82%b3%e6%92%adbackpropagation aria-label=反向傳播(Backpropagation)>反向傳播(Backpropagation)</a></li></ul></li><li><a href=#kernel-methods-%e8%88%87-svm aria-label="Kernel methods 與 SVM">Kernel methods 與 SVM</a><ul><li><a href=#kernel-tricks aria-label="Kernel tricks">Kernel tricks</a></li></ul></li><li><a href=#%e6%b1%ba%e7%ad%96%e6%a8%b9%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97%e5%92%8c%e6%a2%af%e5%ba%a6%e6%8f%90%e5%8d%87%e6%a9%9f%e5%99%a8 aria-label=決策樹、隨機森林和梯度提升機器>決策樹、隨機森林和梯度提升機器</a><ul><li><a href=#%e6%b1%ba%e7%ad%96%e6%a8%b9-decision-tree aria-label="決策樹 Decision Tree">決策樹 Decision Tree</a></li><li><a href=#%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97-random-forest aria-label="隨機森林 Random Forest">隨機森林 Random Forest</a></li><li><a href=#%e6%a2%af%e5%ba%a6%e6%8f%90%e5%8d%87%e6%a9%9f-gradient-boosting-machines-gbm aria-label="梯度提升機 Gradient Boosting Machines (GBM)">梯度提升機 Gradient Boosting Machines (GBM)</a></li></ul></li><li><a href=#%e6%b7%b1%e5%ba%a6%e5%ad%b8%e7%bf%92%e6%9c%80%e5%a4%a7%e7%9a%84%e5%84%aa%e5%8b%a2 aria-label=深度學習最大的優勢>深度學習最大的優勢</a></li><li><a href=#%e7%8f%be%e4%bb%a3%e6%a9%9f%e5%99%a8%e5%ad%b8%e7%bf%92%e6%a1%86%e6%9e%b6%e6%87%89%e7%94%a8 aria-label=現代機器學習框架應用>現代機器學習框架應用</a><ul><li><a href=#%e4%b8%bb%e6%b5%81%e6%a1%86%e6%9e%b6%e6%af%94%e8%bc%83 aria-label=主流框架比較>主流框架比較</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=機率建模probabilistic-modeling>機率建模(Probabilistic modeling)<a hidden class=anchor aria-hidden=true href=#機率建模probabilistic-modeling>#</a></h2><h3 id=單純貝氏演算法naive-bayes-thorem>單純貝氏演算法(Naive Bayes thorem)<a hidden class=anchor aria-hidden=true href=#單純貝氏演算法naive-bayes-thorem>#</a></h3><p>單純貝氏定理是一種基於機率理論的分類方法，適用於文本分類等任務，其核心概念是基於特徵條件獨立的假設來計算後驗機率。</p><p>以下是一個使用 Python 使現文本分類的範例</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.naive_bayes <span style=color:#f92672>import</span> MultinomialNB
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.feature_extraction.text <span style=color:#f92672>import</span> CountVectorizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 示範文本數據</span>
</span></span><span style=display:flex><span>texts <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;這部電影很好看&#34;</span>, <span style=color:#e6db74>&#34;服務很差&#34;</span>, <span style=color:#e6db74>&#34;餐點美味&#34;</span>, <span style=color:#e6db74>&#34;環境很糟&#34;</span>]
</span></span><span style=display:flex><span>labels <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;正面&#34;</span>, <span style=color:#e6db74>&#34;負面&#34;</span>, <span style=color:#e6db74>&#34;正面&#34;</span>, <span style=color:#e6db74>&#34;負面&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 將文本轉換為特徵向量</span>
</span></span><span style=display:flex><span>vectorizer <span style=color:#f92672>=</span> CountVectorizer()
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> vectorizer<span style=color:#f92672>.</span>fit_transform(texts)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 訓練模型</span>
</span></span><span style=display:flex><span>clf <span style=color:#f92672>=</span> MultinomialNB()
</span></span><span style=display:flex><span>clf<span style=color:#f92672>.</span>fit(X, labels)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 預測新文本</span>
</span></span><span style=display:flex><span>new_text <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;這家餐廳很棒&#34;</span>]
</span></span><span style=display:flex><span>new_X <span style=color:#f92672>=</span> vectorizer<span style=color:#f92672>.</span>transform(new_text)
</span></span><span style=display:flex><span>prediction <span style=color:#f92672>=</span> clf<span style=color:#f92672>.</span>predict(new_X)
</span></span></code></pre></div><h3 id=邏輯迴歸logistic-regression-logreg>邏輯迴歸(logistic regression, logreg)<a hidden class=anchor aria-hidden=true href=#邏輯迴歸logistic-regression-logreg>#</a></h3><p>邏輯迴歸是一種二元分類問題的基礎演算法。儘管名稱中有「迴歸」，但實際上是一個分類模型，通過 <strong>sigmoid 函數</strong> 將線性預測轉換成機率值。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> LogisticRegression
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 示範數據</span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>], [<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>], [<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>], [<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>]])
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 創建和訓練模型</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> LogisticRegression()
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 預測</span>
</span></span><span style=display:flex><span>prediction <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict([[<span style=color:#ae81ff>2.5</span>, <span style=color:#ae81ff>3.5</span>]])
</span></span></code></pre></div><h2 id=早期的神經網路>早期的神經網路<a hidden class=anchor aria-hidden=true href=#早期的神經網路>#</a></h2><p>1980年代的神經網路發展奠定了現代深度學習的基礎。反向傳播算法的發明是一個重要突破，它提供了一種有效的方法來訓練多層神經網路。</p><p>1989年 Yann LeCun 結合卷積神經網路(Convolutional Neural Network, CNN) 和反向傳播，應用於手寫數字分類問題。</p><h3 id=反向傳播backpropagation>反向傳播(Backpropagation)<a hidden class=anchor aria-hidden=true href=#反向傳播backpropagation>#</a></h3><p>反向傳播是一種梯度下降最佳化(gradient-descent-optimization)的演算法，用於調整神經網路中的權重。其過程可以簡化為：</p><ol><li>前向傳播：輸入數據通過網路產生預測</li><li>計算誤差：比較預測值與實際值</li><li>反向傳播誤差：從輸出層向輸入層計算梯度</li><li>更新權重：使用梯度下降來優化網路參數</li></ol><p>以下是一個使用 PyTorch 實現簡單神經網路的例子：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>SimpleNN</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super(SimpleNN, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>784</span>, <span style=color:#ae81ff>128</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>layer1(x))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer2(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 創建模型</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> SimpleNN()
</span></span></code></pre></div><h2 id=kernel-methods-與-svm>Kernel methods 與 SVM<a hidden class=anchor aria-hidden=true href=#kernel-methods-與-svm>#</a></h2><p>支持向量機（Support Vector Methods, SVM）是一種強大的分類算法，通過在高維空間中尋找最優分類超平面來工作。</p><ul><li>目標是兩種類別的資料點間，找到最佳決策邊界(decision boundaries)<ul><li>步驟：<ol><li>映射到高維空間，決策邊界通常是一個超曲面</li><li>找到最大化邊界(maximizing the margin)，以製造最大的 margin</li></ol></li></ul></li></ul><h3 id=kernel-tricks>Kernel tricks<a hidden class=anchor aria-hidden=true href=#kernel-tricks>#</a></h3><p>將資料映射到高維空間在理論上可行，但在實際上卻常常很難處理(因為維度高，計算負載變大)<br>我們不必在高維度空間把每一個資料點都做座標轉換，然後在高維度上計算決策超平面，取而代之，只要計算高維空間中點與點之間的距離即可。</p><ul><li>kernel function 定義是「初始空間中任意兩點」映射到「目標表示空間中對應點之間的距離」。</li><li>白話解釋，可以將 Kernel tricks 理解為一種「偷懶」的聰明方法。想像我們要分類兩組點，在二維平面上無法用直線分開，需要將點映射到三維空間。但直接計算三維座標會很耗時，kernel tricks 讓我們可以直接計算點之間在高維空間的距離，而不需要真的去計算高維座標。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.svm <span style=color:#f92672>import</span> SVC
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 創建SVM分類器</span>
</span></span><span style=display:flex><span>svm <span style=color:#f92672>=</span> SVC(kernel<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;rbf&#39;</span>)  <span style=color:#75715e># 使用RBF核函數</span>
</span></span><span style=display:flex><span>svm<span style=color:#f92672>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 預測</span>
</span></span><span style=display:flex><span>predictions <span style=color:#f92672>=</span> svm<span style=color:#f92672>.</span>predict(X_test)
</span></span></code></pre></div><ul><li>但在 SVM 中，只有決策超平面是學習而來的，kernel function 是需要透過人工設計的。SVM 在簡單分類問題上表現很好，是少數具備廣泛理論支持也經得起嚴謹數學分析的機器學習演算法。但 SVM 難以擴展到大型資料集。SVM 是一種淺層方法，需要有效的手動萃取有用的表示法，該步驟稱為<strong>特徵工程(feature engineering)</strong>。</li><li>打個比方，圖片的原始像素無法很好的分類手寫數字，需要人工找到有用的表示法，如像素直方圖。</li></ul><h2 id=決策樹隨機森林和梯度提升機器>決策樹、隨機森林和梯度提升機器<a hidden class=anchor aria-hidden=true href=#決策樹隨機森林和梯度提升機器>#</a></h2><h3 id=決策樹-decision-tree>決策樹 Decision Tree<a hidden class=anchor aria-hidden=true href=#決策樹-decision-tree>#</a></h3><p>決策樹模擬人類決策過程，通過一系列問題將數據分類。它像是一個流程圖，從根節點開始，根據特徵值選擇分支，最終到達葉節點得到預測結果。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.tree <span style=color:#f92672>import</span> DecisionTreeClassifier
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 創建決策樹</span>
</span></span><span style=display:flex><span>tree <span style=color:#f92672>=</span> DecisionTreeClassifier(max_depth<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>tree<span style=color:#f92672>.</span>fit(X_train, y_train)
</span></span></code></pre></div><h3 id=隨機森林-random-forest>隨機森林 Random Forest<a hidden class=anchor aria-hidden=true href=#隨機森林-random-forest>#</a></h3><p>隨機森林是多個決策樹的集成，每棵樹使用隨機選擇的特徵和數據樣本進行訓練。最終預測是所有樹的投票結果。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.ensemble <span style=color:#f92672>import</span> RandomForestClassifier
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 創建隨機森林</span>
</span></span><span style=display:flex><span>rf <span style=color:#f92672>=</span> RandomForestClassifier(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>rf<span style=color:#f92672>.</span>fit(X_train, y_train)
</span></span></code></pre></div><h3 id=梯度提升機-gradient-boosting-machines-gbm>梯度提升機 Gradient Boosting Machines (GBM)<a hidden class=anchor aria-hidden=true href=#梯度提升機-gradient-boosting-machines-gbm>#</a></h3><p>梯度提升機是一種迭代式的集成方法，每次迭代都試圖糾正之前模型的錯誤。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.ensemble <span style=color:#f92672>import</span> GradientBoostingClassifier
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 創建梯度提升機</span>
</span></span><span style=display:flex><span>gbm <span style=color:#f92672>=</span> GradientBoostingClassifier(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>gbm<span style=color:#f92672>.</span>fit(X_train, y_train)
</span></span></code></pre></div><h2 id=深度學習最大的優勢>深度學習最大的優勢<a hidden class=anchor aria-hidden=true href=#深度學習最大的優勢>#</a></h2><ul><li><strong>讓特徵工程變成自動化</strong></li><li>淺層學習有<strong>快速遞減(fast-diminishing)</strong> 的現象，因為「在三層模型中最佳的第一表示層，並非單層或雙層模型中最佳的第一表示層。」。<blockquote><p>想像你在蓋一棟三層樓的房子。如果你先蓋一層樓的房子，再加蓋第二層，最後加蓋第三層，這樣的結果往往不如一開始就規劃好三層樓一起設計來得好。因為當你一開始就知道要蓋三層，你可以更好地規劃整體結構，確保每一層都能最好地支持整棟建築。</p></blockquote></li><li>聯合學習：調整模型內部一個參數，其它相關參數也會自動調整。</li></ul><h2 id=現代機器學習框架應用>現代機器學習框架應用<a hidden class=anchor aria-hidden=true href=#現代機器學習框架應用>#</a></h2><h3 id=主流框架比較>主流框架比較<a hidden class=anchor aria-hidden=true href=#主流框架比較>#</a></h3><ol><li>Scikit-learn：適用於傳統機器學習算法</li></ol><ul><li>優點：API簡單，適合快速原型開發</li><li>應用：特徵工程、傳統算法實現</li></ul><ol start=2><li>TensorFlow/Keras：</li></ol><ul><li>優點：生產環境部署成熟，生態系統完整</li><li>應用：深度學習模型開發和部署</li></ul><ol start=3><li>PyTorch：</li></ol><ul><li>優點：動態計算圖，研究友好</li><li>應用：研究實驗，快速迭代開發</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://intervalrain.github.io/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://intervalrain.github.io/ai/1_1/><span class=title>« 上一頁</span><br><span>[AI] 1-1. 何謂人工智慧?</span>
</a><a class=next href=https://intervalrain.github.io/ai/1_3/><span class=title>下一頁 »</span><br><span>[AI] 1-3. 深度學習的發展</span></a></nav><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://intervalrain.github.io/>Rain Hu's Workspace</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="複製";function s(){t.innerHTML="已複製！",setTimeout(()=>{t.innerHTML="複製"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>