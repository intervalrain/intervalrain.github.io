<!doctype html><html lang=zh-tw dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AI | Rain Hu's Workspace</title>
<meta name=keywords content><meta name=description content="Rain Hu 記錄生活、工作、學習、個人創作的空間，包含了音樂創作、文字創作、演算法筆記、資工學習資料、架站資料、程式語言筆記、Leetcode 解題分析等各式各樣的資訊"><meta name=author content="Rain Hu, intervarrain, 陣雨"><link rel=canonical href=https://intervalrain.github.io/tags/ai/><meta name=google-site-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.662816b9df27c772d2b97c5f5f6bf4f2c5531051a330015f0ad4135736d0e56a.css integrity="sha256-ZigWud8nx3LSuXxfX2v08sVTEFGjMAFfCtQTVzbQ5Wo=" rel="preload stylesheet" as=style><link rel=icon href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=16x16 href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=32x32 href=https://intervalrain.github.io/images/rain.png><link rel=apple-touch-icon href=https://intervalrain.github.io/images/rain.png><link rel=mask-icon href=https://intervalrain.github.io/images/rain.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://intervalrain.github.io/tags/ai/index.xml><link rel=alternate hreflang=zh-tw href=https://intervalrain.github.io/tags/ai/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://utteranc.es/client.js repo=intervalrain.github.io issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script><meta property="og:url" content="https://intervalrain.github.io/tags/ai/"><meta property="og:site_name" content="Rain Hu's Workspace"><meta property="og:title" content="AI"><meta property="og:description" content="Rain Hu 記錄生活、工作、學習、個人創作的空間，包含了音樂創作、文字創作、演算法筆記、資工學習資料、架站資料、程式語言筆記、Leetcode 解題分析等各式各樣的資訊"><meta property="og:locale" content="zh-tw"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="AI"><meta name=twitter:description content="Rain Hu 記錄生活、工作、學習、個人創作的空間，包含了音樂創作、文字創作、演算法筆記、資工學習資料、架站資料、程式語言筆記、Leetcode 解題分析等各式各樣的資訊"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://intervalrain.github.io/ accesskey=h title="Rain Hu's Workspace (Alt + H)"><img src=https://intervalrain.github.io/images/rain.png alt aria-label=logo height=35>Rain Hu's Workspace</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://intervalrain.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://intervalrain.github.io/aboutme title="About me"><span>About me</span></a></li><li><a href=https://intervalrain.github.io/archives title=Archives><span>Archives</span></a></li><li><a href=https://intervalrain.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://intervalrain.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://intervalrain.github.io/csharp/csharp title=C#><span>C#</span></a></li><li><a href=https://intervalrain.github.io/csindex title=CS><span>CS</span></a></li><li><a href=https://intervalrain.github.io/leetcode title=LeetCode><span>LeetCode</span></a></li><li><a href=https://intervalrain.github.io/ai title=AI><span>AI</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://intervalrain.github.io/>首頁</a>&nbsp;»&nbsp;<a href=https://intervalrain.github.io/tags/>Tags</a></div><h1>AI</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 3-5. 邏輯斯迴歸(logistic regression)</h2></header><div class=entry-content><p>貝式定理 假設有一個抽獎箱內有紅球與藍球，球上有標示 A 與 B 類別。 $$ \begin{array}{|c|c|} \hline A&amp;B\\\hline \blue{\text{●}}\blue{\text{●}}\blue{\text{●}}\red{\text{●}}&\blue{\text{●}}\blue{\text{●}}\red{\text{●}}\red{\text{●}}\red{\text{●}}\red{\text{●}}\\\hline \end{array} $$
我們抽到藍球，它是來自於 A 的機率為何，即求 \(P(A|\blue{\text{●}})\)？ 根據貝式定理： $$ P(A|x)=\frac{P(x|A)P(A)}{P(x|A)P(A)+P(x|B)P(B)} $$
先驗機率 \(P(A)=\frac{\text{A的球數}}{\text{總球數}}=\frac{4}{10}\) \(P(B)=\frac{\text{B的球數}}{\text{總球數}}=\frac{6}{10}\) 條件機率 \(P(\blue{\text{●}}|A)=\frac{\text{A中的}\blue{\text{●}}}{\text{A的總球數}}=\frac{3}{4}\) \(P(\blue{\text{●}}|B)=\frac{\text{B中的}\blue{\text{●}}}{\text{B的總球數}}=\frac{2}{6}\) 套入公式可得 $$ P(A|\blue{\text{●}})=\frac{P(\blue{\text{●}}|A)P(A)}{P(\blue{\text{●}}|A)P(A)+P(\blue{\text{●}}|B)P(B)}=\frac{3/4\times4/10}{3/4\times4/10+2/6\times6/10}=\frac{3}{5} $$ 假設今天猜中類別才能得獎，已經知道是藍球的情況下，來自 A 的機率是 0.6，來自 B 的機率是 0.4，所以我們理論上會選擇 A，因為機率較大。換言之，在機器學習中，我們判斷一個二元分類的問題，我們會將分類判給機率 > 0.5 的那個類別。 高斯分布 一維的高斯分分機率密度函數(probability density function, pdf)為： $$ f_{\mu,\sigma}(x)=\frac{1}{\sqrt{2\pi\sigma^2}}exp\bigg\lbrace-\frac{(x-\mu)^2}{2\sigma^2}\bigg\rbrace $$ 其中 \(\mu\) 為平均數(Mean)，決定分布的中心位置。 \(\sigma\) 為標準差(Standard Deviation)，決定分布的寬度。 擴展到 n 維向量的多維高斯分布機率密度函數為： $$ f_{\mu,\Sigma}(x)=\frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp\bigg\lbrace-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\bigg\rbrace $$ 其中 x 是一階張量 $$ x = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} $$
...</p></div><footer class=entry-footer><span title='2024-12-19 15:55:12 +0800 +0800'>December 19, 2024</span>&nbsp;·&nbsp;5 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 3-5. 邏輯斯迴歸(logistic regression)" href=https://intervalrain.github.io/ai/3_5/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 3-4. 線性迴歸</h2></header><div class=entry-content><p>目標 機器學習的目標有很多種，參考李宏毅教授的機器學習課程，可以用下面一張圖來概述。 Task 代表機器學習的目標 Regression: 透過迴歸來預測值。 Classification: 處理分類問題。 Structed Learning: 生成結構化的資訊(現在稱為生成式 AI, GenAI) Scenario 代表解決問題的策略 Supervised Learning: 使用已標記的訓練數據進行訓練 Semi-supervised Learning: 使用有標記與無標記的訓練數據進行訓練 Unsupervised Learning: 不使用標記的訓練數據進行訓據，由模型自行發現模式與結構 Reinforcement Learning: 透過「獎勵」與「懲罰」來學習。 Transfer Learning: 將一個任務學習到的知識應用到相關的新任務 Method 指應用的方法 Linear Model Deep Learning SVM Decision Tree KNN 線性迴歸 暴力解 假設我們大概知道答案的區間，我們可以暴力求解，將每一個 w, b 代入求最小的 (w, b) 組合 這個方法的缺點是，計算量很大，且我們求值的方式不是連續的，精準度不夠。 import sys areas = data[:,0] prices = data[:,1] def compute_loss(y_pred, y): return (y_pred - y)**2 best_w = 0. best_b = 0. min_loss = sys.float_info.max # 猜 w=30-50, step = 0.1 # 猜 b=200-600 step = 1 for i in range(200): for j in range(400): w = 30 + i*0.1 b = 200 + j*1 loss = 0. for area, price in zip(areas, prices): y_pred = w * area + b loss += compute_loss(y_pred, price) if loss &lt; min_loss: min_loss = loss best_w = w best_b = b w=35.1 b=599 線性代數解法 假如我們學過線性代數，我們想得到它的歸性迴歸方程式，我們的作法會是： 設迴歸方程式為 $$\text{y}=\text{wx}+\text{b}\quad\quad (1)$$
...</p></div><footer class=entry-footer><span title='2024-12-19 15:01:12 +0800 +0800'>December 19, 2024</span>&nbsp;·&nbsp;13 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 3-4. 線性迴歸" href=https://intervalrain.github.io/ai/3_4/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 3-3. 使用 TensorFlow 與 Keras 函式庫</h2></header><div class=entry-content><p>建立張量與變數 張量 引入 tensorflow import tensorflow as tf 建立張量並以 0 作為初始值 x = tf.ones(shape=(2,1)) print(x) >>> tf.Tensor( [[1.] [1.]], shape=(2, 1), dtype=float32) 建立張量並以 1 作為初始值 y = tf.zeros(shape=(2,1)) print(y) >>> tf.Tensor( [[0.] [0.]], shape=(2, 1), dtype=float32) 建立張量，並以常數初始化 a = tf.constant(((1.,4.),(9.,16.))) print(a) >>> tf.Tensor( [[ 1. 4.] [ 9. 16.]], shape=(2, 2), dtype=float32) 建立由亂數組成的張量 常態分佈 # 亂數從平均值 0、標準差 1 的常態分佈中抽取出來 # 等同於 np.random.normal(size=(3,1), loc=0., scale=1.) z = tf.random.normal(shape=(3,1), mean=0., stddev=1.) print(z) >>> tf.Tensor( [[-1.1859674 ] [ 0.3267818 ] [ 0.11066005]], shape=(3, 1), dtype=float32) 均勻分布 # 亂數從 0 到 1 之間均勻分布抽取出來 # 等同於 np.random.uniform(size=(3,1), low=0., high=1.) k = tf.random.uniform(shape=(3,1), minval=0., maxval=1.) print(k) >>> tf.Tensor( [[0.97643256] [0.13791454] [0.7854562 ]], shape=(3, 1), dtype=float32) NumPy 與 TensorFlow 的差異在於，TensorFlow 的張量為常數 ，無法修改。 將 numpy 轉換成 tensorflow x = np.ones((2,2)) x[0, 0] = 0 y = tf.convert_to_tensor(x) print(y) >>> tf.Tensor( [[0. 1.] [1. 1.]], shape=(2, 2), dtype=float64) 變數 創建變數 直接用 tuple 初始化 v = tf.Variable(((3.,1.,3.),(2.,3.,2.))) print(v) >>> &lt;tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=array( [[3., 1., 3.], [2., 3., 2.]], dtype=float32)> 用隨機值初始化 v = tf.Variable(initial_value=tf.random.normal(shape=(3,1))) print(v) >>> &lt;tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy=array( [[2.1368823 ], [0.8101084 ], [0.47075817]], dtype=float32)> 變數賦值 透過 assign() 方法對變數賦值 v.assign(tf.ones(shape=(3,1))) >>> &lt;tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=array( [[1.], [1.], [1.]], dtype=float32)> 對變數局部賦值 v[0,0].assign(0) >>> &lt;tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=array( [[0., 1., 3.], [2., 3., 2.]], dtype=float32)> 變數運算 加法 v.assign_add(tf.ones((2,3))) >>> &lt;tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=array( [[1., 2., 4.], [3., 4., 3.]], dtype=float32)> 減法 v.assign_sub(2*tf.ones((2,3))) >>> &lt;tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=array( [[-1., 0., 2.], [ 1., 2., 1.]], dtype=float32)> 張量操作 基本數學運算 初始化 a = tf.constant(((1.,4.),(9.,16.))) >>> tf.Tensor( [[ 1. 4.] [ 9. 16.]], shape=(2, 2), dtype=float32) 平方 a = tf.square(a) print(a) >>> tf.Tensor( [[ 1. 16.] [ 81. 256.]], shape=(2, 2), dtype=float32) 平方根 a = tf.sqrt(a) print(a) >>> tf.Tensor( [[ 1. 4.] [ 9. 16.]], shape=(2, 2), dtype=float32) 加法(逐元素相加) b = tf.ones((2,2)) print(a+b) >>> tf.Tensor( [[ 2. 5.] [10. 17.]], shape=(2, 2), dtype=float32) 點積 b = tf.constant(((1.,-1.),(-1.,1.))) c = tf.matmul(a,b) print(c) >>> tf.Tensor( [[-3. 3.] [-7. 7.]], shape=(2, 2), dtype=float32) 相乘(逐元素相乘) d = a*b print(d) >>> tf.Tensor( [[ 1. -4.] [-9. 16.]], shape=(2, 2), dtype=float32) 微分 計算一階梯度
...</p></div><footer class=entry-footer><span title='2024-12-18 16:56:55 +0800 +0800'>December 18, 2024</span>&nbsp;·&nbsp;3 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 3-3. 使用 TensorFlow 與 Keras 函式庫" href=https://intervalrain.github.io/ai/3_3/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 3-2. Keras 介紹</h2></header><div class=entry-content><p>Keras 是建立在 TensorFlow 上的 Python 深度學習 API，提供了簡易方法來定義、訓練深度學習的模型。
可以想成 TensorFlow 負責張量運算、Keras 是演算法。
Keras（高層 API）：
負責深度學習模型的高階抽象 提供用戶友好的介面 處理模型定義和訓練流程 eg. Layer, Model, Optimizers, Loss functions, Metrics TensorFlow（中層）：
處理底層的數學運算 管理計算圖和自動微分 優化運算效率 eg. Tensor, Variable、GradientTape Hardware（硬體層）：
實際執行計算任務 提供不同的計算加速選項 優化特定類型的運算 eg. CPU, GPU, TPU</p></div><footer class=entry-footer><span title='2024-12-18 16:45:50 +0800 +0800'>December 18, 2024</span>&nbsp;·&nbsp;1 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 3-2. Keras 介紹" href=https://intervalrain.github.io/ai/3_2/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 3-1. TensorFlow 介紹</h2></header><div class=entry-content><p>TensorFlow 是一個開源的 Python 機器學習框架，主要由 Google 開發。
TensorFlow 可以做到以下的事：
可進行自動微分、計算梯度。 可在 CPU 上運行，也可以在 GPU 及 TPU 上運行。 可以將運算程序分散到多台機器上共同執行。 可以匯出為各種不同語言的程式，包含 C++, JavaScript 或 TensorFlow Lite，使得 TensorFlow 應用可以輕鬆部署到各種實際場景。</p></div><footer class=entry-footer><span title='2024-12-18 16:40:40 +0800 +0800'>December 18, 2024</span>&nbsp;·&nbsp;1 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 3-1. TensorFlow 介紹" href=https://intervalrain.github.io/ai/3_1/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 2-3. 優化器 Optimizer</h2></header><div class=entry-content><p>在神經網路中，每個神經層都通過以下的方式進行資料轉換
output = relu(dot(W, input) + b) W 和 b 為該層的屬性張量，統稱為該層的權重(weights) 或可訓練參數(trainable parameters)，分別為 內核(kernel) 屬性和 偏值(bias) 屬性。 我們的目標是要透過訓練(training) 來進行權重的微調，進而得到最小的損失值。 參數調大或調小，可能會對應到「損失值變大」或「損失值變小」兩種可能(如果兩者皆變小，稱為 saddle point，兩者皆變大，可能為 local minium)。
但如果透過兩個方向的正向傳播來檢查微調的方向，效率很低，於是科學家引入了梯度下降法。 梯度下降法(gradient descent) 張量運算的導數稱為梯度(gradient)。 純量函數的導數相當於在函數曲線上找某一點的斜率。 張量函數上求梯度，相當於在函數描繪的多維曲面上求曲率。 y_pred = dot(W, x) loss_value = loss(y_pred, y_true) 假設只考慮 W 為變數，我們可以將 loss_value 視為 W 的函數。
loss_value = loss(dot(W, x), y_true) = f(W) 假設 W 的當前值為 W0，那麼函數 f 在 W0 這點的導數就是
$$ f’(W)=\frac{d\text{(loss\_value)}}{d\text{W}}|\text{W}_0 $$ 我們將上式寫成
g = grad(loss_value, W0) 代表了在 W0 附近，loss_value = f(W) 的梯度最陡方向之張量，我們可以透過往往「斜率反方向」移動來降低 f(W)。
...</p></div><footer class=entry-footer><span title='2024-12-16 15:48:11 +0800 +0800'>December 16, 2024</span>&nbsp;·&nbsp;4 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 2-3. 優化器 Optimizer" href=https://intervalrain.github.io/ai/2_3/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 2-2. 張量 Tensor</h2></header><div class=entry-content><p>張量 Tensor 是神經網路的資料表示法。 在 Python，我們常用 NumPy 陣列來作為機器學習的基礎資料結構，說 NumPy 陣列也稱為張量。
張量的維、階、軸 階(rank): 又稱為軸(axis)，代表陣列的軸數。 維(dimension): 某一階的元素個數。 1. 純量 (0D 張量) 純量是單一個數值，也稱為 0 維張量 (0D Tensor)
x = 5 2. 向量 (1D 張量) 向量是包含單一軸的數列，也稱為 1 維張量 (1D Tensor)
x = [1, 2, 3] 3. 矩陣 (2D 張量) 矩陣是二維的數據結構，也稱為 2 維張量 (2D Tensor)
x = [[1, 2, 3], [4, 5, 6]] 4. 3D 張量與高階張量 3D 張量：在二維矩陣基礎上增加一個深度維度，常用於處理圖片數據 (例如 RGB 通道)。 高階張量 (nD 張量)：當張量的維度超過 3D 時，用於更高維度的資料表示，例如影片、文字數據等。 x = [[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]]] 5. 張量的屬性 張量擁有幾個關鍵屬性，用於描述其結構與內容：
...</p></div><footer class=entry-footer><span title='2024-12-15 14:26:38 +0800 +0800'>December 15, 2024</span>&nbsp;·&nbsp;4 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 2-2. 張量 Tensor" href=https://intervalrain.github.io/ai/2_2/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 2-1. 初試神經網路-手寫辨識 mnist</h2></header><div class=entry-content><p>MNIST 是經典的手寫數字圖片資料集，已經內建在 tensorflow 裡面，這個資料集可以相當於是深度學習的 “Hello World”，是由美國國家標準與技術研究院(National Institute of Standard and Technology) 所提供。 1. 認識 MNIST 資料集 透過 tensorflow 引入資料集 from tensorflow.keras.datasets import mnist (train_images, train_labels), (test_images, test_labels) = mnist.load_data() 查看資料的大小 train_images.shape len(train_labels) test_images.shape test_labels min(test_labels), max(test_labels) 結果 (60000, 28, 28) # 代表 train_images 是 60000 張 28*28 的圖片 60000 # 代表 train_labels 同樣也有 60000 份 (10000, 28, 28) # 代表 test_images 有 10000 張 28*28 的圖片 array([7, 2, 1, ..., 4, 5, 6], dtype=uint8) (0, 9) # 代表 test_labels 是 0-9 的數字，資料型別是 uint8 2. 神經網路架構 接下來的操作流程是： 將測資 train_images 和 train_labels 餵給神經網路 神經網路學習分類圖片，與每張圖片的標籤對比，分類錯誤就修正(學習) 最後對 test_images 進行預測，並驗證結果看是否與 test_labels 吻合 from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ layers.Dense(512, activation="relu") layers.Dense(10, activation="softmax") ]) 組成神經網路的基本元件為層(layer)，一個層就是一個資料處理的模組。可以視之為資料的過濾器。具體而言，每一層都會從資料中萃取出特定的轉換或是表示法(representation)，這些特定的表示法會有助於解決某些問題。大多數深度學習模型會將許多層連接在一起，漸次執行資料萃取(data distillation)。 在範例中，神經網路由兩個密集層(Dense layers)緊密連接組成，密集層也稱為全連接(fully connected) 神經層。第二個密集層是有 10 個輸出的 softmax 層，最終會輸出 10 個機率評分的陣列，每個評分就是對應到每一個數字的機率。 model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) 為了讓神經網路接受訓練，還需要準備三個元件才能進行編譯。
...</p></div><footer class=entry-footer><span title='2024-12-15 13:05:53 +0800 +0800'>December 15, 2024</span>&nbsp;·&nbsp;2 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 2-1. 初試神經網路-手寫辨識 mnist" href=https://intervalrain.github.io/ai/2_1/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 1-3. 深度學習的發展</h2></header><div class=entry-content><p>卷積神經網路與反向傳播這兩個應用於電腦視覺的深度學習關鍵概念，在 1989 年就已經被研究透徹。長短期記憶(Long Short-Term Memory, LSTM) 演算法是時間序列的深度學習基礎，也於 1997 年發展出來，之後幾乎沒有什麼演進。那麼為什麼深度學習會在 2012 年後才開始蓬勃發展呢？
技術演變 硬體 資料集和競賽評比 演算法的進步 因為機器學習的發展是經由實驗結果來驗證(而不是由理論引導)，所有只有當資料和硬體可支撐新思維時，才能促進演算法的進步。
硬體 從 1990 到 2010 年，CPU 提升了約 5000 倍，但這仍然無法供應電腦視覺或語音辨識的典型深度學習模型。
在 2000 年代，由 NVIDIA 和 AMD 等公司投資大規模平行運算晶片(graphical processing units, 圖形處理單元, GPU)，以應愈來愈逼真的影音遊戲。在 2007 年，NVIDIA 推出了 CUDA，一個針對 GPU 的程式開發介面。從物理建模開始，只需要 GPU 就可以在各種高度平行化運算中取代大量 CPU。深度神經網路主要由許多矩陣多項式構成，也屬於高度平行化處理。因此在 2011 年，開始有研究人員(Dan Ciresan, Alex Krizhevsky)以 CUDA 來開發神經網路。
資料 除了過去 20 年儲存設備爆發式的發展外，網際網路的興起才是真正影響資料源的重大關鍵，大量來自於網路的資料被應用於機器學習上。
ImageNet 提供了大量的已被標註(labled)的影像，包含許多大尺寸的影像資料，還有最重要的，其相關的年度影像辨識競賽。
Kaggle 是一個專注於資料科學和機器學習的競賽平台，於 2010 年成立。這個平台不僅提供大量的公開資料集，還定期舉辦競賽，讓研究者和工程師能夠透過實際問題來測試與優化他們的演算法。Kaggle 競賽的主題多元，涵蓋醫學影像分析、自然語言處理、時間序列預測等。透過這樣的競爭環境，不僅促進了演算法的進步，也加速了實驗結果的分享與技術的迭代。
ImageNet 與 Kaggle 的共同點在於，它們都提供了一個標準化的評測基準，使得研究人員能夠客觀地比較不同的模型表現。此外，這些平台的出現，使得深度學習的研究與應用從少數實驗室走向全球，更多人參與的結果是加速了技術的發展。
演算法的進步 深度學習模型的核心挑戰之一在於深層網路結構中的梯度傳播問題（gradient propagation）。隨著網路層數的增加，用於訓練神經網路的回饋訊號可能會逐漸消失。這個問題在早期深度學習的研究中，極大地限制了神經網路的深度及其表現能力。然而，隨著多項關鍵演算法改進的提出，這一瓶頸逐漸被克服，深層網路得以成功訓練並展現強大的性能。
...</p></div><footer class=entry-footer><span title='2024-12-06 16:26:30 +0800 +0800'>December 6, 2024</span>&nbsp;·&nbsp;1 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 1-3. 深度學習的發展" href=https://intervalrain.github.io/ai/1_3/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 1-2. 機器學習的基礎技術</h2></header><div class=entry-content><p>機率建模(Probabilistic modeling) 單純貝氏演算法(Naive Bayes thorem) 單純貝氏定理是一種基於機率理論的分類方法，適用於文本分類等任務，其核心概念是基於特徵條件獨立的假設來計算後驗機率。
以下是一個使用 Python 使現文本分類的範例
from sklearn.naive_bayes import MultinomialNB from sklearn.feature_extraction.text import CountVectorizer # 示範文本數據 texts = ["這部電影很好看", "服務很差", "餐點美味", "環境很糟"] labels = ["正面", "負面", "正面", "負面"] # 將文本轉換為特徵向量 vectorizer = CountVectorizer() X = vectorizer.fit_transform(texts) # 訓練模型 clf = MultinomialNB() clf.fit(X, labels) # 預測新文本 new_text = ["這家餐廳很棒"] new_X = vectorizer.transform(new_text) prediction = clf.predict(new_X) 邏輯迴歸(logistic regression, logreg) 邏輯迴歸是一種二元分類問題的基礎演算法。儘管名稱中有「迴歸」，但實際上是一個分類模型，通過 sigmoid 函數 將線性預測轉換成機率值。
from sklearn.linear_model import LogisticRegression import numpy as np # 示範數據 X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]]) y = np.array([0, 0, 1, 1]) # 創建和訓練模型 model = LogisticRegression() model.fit(X, y) # 預測 prediction = model.predict([[2.5, 3.5]]) 早期的神經網路 1980年代的神經網路發展奠定了現代深度學習的基礎。反向傳播算法的發明是一個重要突破，它提供了一種有效的方法來訓練多層神經網路。
...</p></div><footer class=entry-footer><span title='2024-12-03 15:42:52 +0800 +0800'>December 3, 2024</span>&nbsp;·&nbsp;2 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 1-2. 機器學習的基礎技術" href=https://intervalrain.github.io/ai/1_2/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://intervalrain.github.io/tags/ai/>«&nbsp;上一頁&nbsp;
</a><a class=next href=https://intervalrain.github.io/tags/ai/page/3/>下一頁&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://intervalrain.github.io/>Rain Hu's Workspace</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>