<!doctype html><html lang=zh-tw dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AI | Rain Hu's Workspace</title>
<meta name=keywords content><meta name=description content="Rain Hu 記錄生活、工作、學習、個人創作的空間，包含了音樂創作、文字創作、演算法筆記、資工學習資料、架站資料、程式語言筆記、Leetcode 解題分析等各式各樣的資訊"><meta name=author content="Rain Hu, intervarrain, 陣雨"><link rel=canonical href=https://intervalrain.github.io/tags/ai/><meta name=google-site-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.662816b9df27c772d2b97c5f5f6bf4f2c5531051a330015f0ad4135736d0e56a.css integrity="sha256-ZigWud8nx3LSuXxfX2v08sVTEFGjMAFfCtQTVzbQ5Wo=" rel="preload stylesheet" as=style><link rel=icon href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=16x16 href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=32x32 href=https://intervalrain.github.io/images/rain.png><link rel=apple-touch-icon href=https://intervalrain.github.io/images/rain.png><link rel=mask-icon href=https://intervalrain.github.io/images/rain.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://intervalrain.github.io/tags/ai/index.xml><link rel=alternate hreflang=zh-tw href=https://intervalrain.github.io/tags/ai/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://utteranc.es/client.js repo=intervalrain.github.io issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script><meta property="og:url" content="https://intervalrain.github.io/tags/ai/"><meta property="og:site_name" content="Rain Hu's Workspace"><meta property="og:title" content="AI"><meta property="og:description" content="Rain Hu 記錄生活、工作、學習、個人創作的空間，包含了音樂創作、文字創作、演算法筆記、資工學習資料、架站資料、程式語言筆記、Leetcode 解題分析等各式各樣的資訊"><meta property="og:locale" content="zh-tw"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="AI"><meta name=twitter:description content="Rain Hu 記錄生活、工作、學習、個人創作的空間，包含了音樂創作、文字創作、演算法筆記、資工學習資料、架站資料、程式語言筆記、Leetcode 解題分析等各式各樣的資訊"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://intervalrain.github.io/ accesskey=h title="Rain Hu's Workspace (Alt + H)"><img src=https://intervalrain.github.io/images/rain.png alt aria-label=logo height=35>Rain Hu's Workspace</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://intervalrain.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://intervalrain.github.io/aboutme title="About me"><span>About me</span></a></li><li><a href=https://intervalrain.github.io/archives title=Archives><span>Archives</span></a></li><li><a href=https://intervalrain.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://intervalrain.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://intervalrain.github.io/csharp/csharp title=C#><span>C#</span></a></li><li><a href=https://intervalrain.github.io/csindex title=CS><span>CS</span></a></li><li><a href=https://intervalrain.github.io/leetcode title=LeetCode><span>LeetCode</span></a></li><li><a href=https://intervalrain.github.io/ai title=AI><span>AI</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://intervalrain.github.io/>首頁</a>&nbsp;»&nbsp;<a href=https://intervalrain.github.io/tags/>Tags</a></div><h1>AI</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 機器學習的流程</h2></header><div class=entry-content><p></p></div><footer class=entry-footer><span title='2025-01-12 17:54:31 +0800 +0800'>January 12, 2025</span>&nbsp;·&nbsp;0 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 機器學習的流程" href=https://intervalrain.github.io/ai/5_5/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 提高普適化能力</h2></header><div class=entry-content><p></p></div><footer class=entry-footer><span title='2025-01-12 17:54:28 +0800 +0800'>January 12, 2025</span>&nbsp;·&nbsp;0 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 提高普適化能力" href=https://intervalrain.github.io/ai/5_4/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 提升模型的表現</h2></header><div class=entry-content><p>要達到完美擬合(perfect fit)，勢必要先經歷過度擬合(overfitting)，否則我們無法預先知道邊界在哪裡。 因此我們要處理任何機器學習問題的初始目標，便是訓練出一個能展現基本普適化能力，並且會發生 overfitting 的模型，接著才可以專注在解決 overfitting 並提升模型的普適化能力。 我們會遇到三個 status: 訓練沒有成效，損失值降不下來。 訓練可行，但無法展現普適化，無法超越基準線。 訓練可行且表現也超越基準線，但無法達到 overfitting。 調整梯度下降的關鍵參數 當訓練沒有成效，代表損失無法下降。 一般而言，就算資料是隨機的，模型有辦法擬合，所以損失無法下降通常可以斷定是 gradient descent 的參數設置需要調整，包含: 優化器(optimizer) 初始權重(initial weight) 學習率(learning rate) 批次量(batch size) 從上圖可見，當學習率太大時，每一次的跨幅過大，使 loss 無法降低；當學習率太小時，每次跨幅太小，訓練的效率不好，當學習率適當時，可以達到最好的成效。 學習率比較的程式碼 import numpy as np import matplotlib.pyplot as plt def objective_function(x, y): return 0.7*x**2 + 1.3*y**2 def gradient(x, y): return np.array([1.4*x, 2.6*y]) def gradient_descent(learning_rate, start_point, n_iterations=50): path = [start_point] point = np.array(start_point) for _ in range(n_iterations): grad = gradient(point[0], point[1]) point = point - learning_rate * grad path.append(point.copy()) if np.linalg.norm(grad) &lt; 1e-6: break return np.array(path) x = np.linspace(-10, 10, 100) # 擴大範圍到 ±10 y = np.linspace(-10, 10, 100) X, Y = np.meshgrid(x, y) Z = objective_function(X, Y) plt.figure(figsize=(15, 6)) learning_rates = [0.77, 0.1, 0.01] titles = ['Learning Rate = 0.77', 'Learning Rate = 0.1', 'Learning Rate = 0.01'] start_point = np.array([4.0, 4.0]) for i, (lr, title) in enumerate(zip(learning_rates, titles)): plt.subplot(1, 3, i+1) plt.contour(X, Y, Z, levels=20, cmap='viridis') path = gradient_descent(lr, start_point) plt.plot(path[:, 0], path[:, 1], 'r.-', linewidth=1, markersize=3, label=f'Gradient Descent Path\nIterations: {len(path)}') plt.plot(path[0, 0], path[0, 1], 'g*', markersize=10, label='Start') plt.plot(path[-1, 0], path[-1, 1], 'r*', markersize=10, label='End') plt.title(title) plt.xlabel('x') plt.ylabel('y') plt.legend() plt.colorbar(label='Function Value') plt.grid(True) plt.tight_layout() plt.show() 使用不同的架構 選擇合適的模型架構處理不同類型的問題 DNN CNN(Convolutional Neural Networks) RNN(Recurrent Neural Networks) Transformer Autoencoders GAN(Generative Adversarial Networks) GNN(Graph Neural Networks) … 提升模型容量(capacity) 如果損失值的確有降低，代表模型的確有在擬合資料，但始終無終達到 overfitting，那可能是模型的 表徵能力(representational power) 不足。 這時可能需要更大的模型，以容納更多的資訊， 可以透過加深、加寬 layer 來提升模型的容量。 ...</p></div><footer class=entry-footer><span title='2025-01-12 17:54:27 +0800 +0800'>January 12, 2025</span>&nbsp;·&nbsp;2 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 提升模型的表現" href=https://intervalrain.github.io/ai/5_3/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 評估模型</h2></header><div class=entry-content><p>評估模型 機器學習模型就像是一個需要不斷練習的學生，我們需要適當的方法來評估它的學習成果。今天讓我們來了解如何科學地評估一個模型的表現。
何為訓練集、驗證集、測試集 在開始解釋不同的數據集之前，我們先來理解兩種重要的參數：
權重參數(weight parameter)：
這是模型通過學習自動調整的參數 就像學生在解題過程中學到的解題技巧 超參數(hyperparameter)：
這是我們需要手動設置的參數 就像是老師設定的學習進度和難度 數據集的三個部分各自扮演著不同的角色：
訓練集：
用於模型的主要學習階段 調整權重參數 佔總數據量約 60-80% 驗證集：
用於調整超參數 幫助我們選擇最佳的模型配置 佔總數據量約 10-20% 測試集：
用於最終的模型評估 模擬真實世界的應用場景 佔總數據量約 10-20% 資料洩漏(information leak): 指測試資料的訊息不當地影響了模型訓練，就像是考試前看到了試題，會導致模型評估結果不可靠。 進階驗驗方法 當資料不足時，就可能要採用一些進階方法來處理，以下介紹三種經典方法： 簡單拆分驗證(simple holdout validation)： 最基本的方法 直接將數據分為訓練集和驗證集 優點：簡單、快速 缺點：結果可能不穩定 K折驗證(k-fold validation)： 將數據分為K個相等的部分 每次使用其中一部分作為驗證集 重複K次，取平均結果 優點：更穩定的評估結果 常用的K值為5或10 K折加洗牌驗證(Iterated K-fold validation with shuffling)： 在K折驗證的基礎上增加多輪重複 每輪都重新打亂數據順序 優點：最穩定的評估結果 缺點：計算成本較高 基準線(Baseline) 在評估模型是否有效時，可以在訓練前先設定一個基準線，而機器學習的目的在於打敗基準線。 以 mnist 來說，假設 0~9 的 labels 數大致相同，則盲猜能達到的準確度應該相當於期望值，也就是 \(\frac{1}{10}\)。換句話說，我們可以將基準線訂為 10%，如果模型做出來的準確度比 10% 還要差，那還不如盲猜呢！ 以 IMDB 為例，準確度至少要差過 50%，因為只有正負評論兩個類別。 在路透社的文章分類目，準確度至少要超過 18~19%，因為有樣本不平衡的問題。 以工廠為例，可能每 10000 張缺陷照片會出現 1 張是有害缺陷，模型若是判斷全部 10000 張都是無害的，那正確率也有 \(\frac{9999}{10000}\)，也就是 99.99%，但這代表抓不到有害缺陷，那就代表這個模型是無效的，所以當資料不平衡時，會產生很大的問題。 評估模型時的注意事項 資料代表性(data representation): 我們希望訓練集和測試集都有一定的代表性，足以反映手邊的資料分佈。 準例來說，我們在做選舉民意調查時，我們都用打市話來進行市調，做為訓練集，然後收集手機民調，做為測試集。由於市話與手機的使用族群不相同，可能導致了訓練集與測試集的代表性都不足。 故我們通常需要對資料進行洗牌(shuffle)，使訓練集與測試集都具備一定的代表性。 時間的方向性(the arrow of time): 如果我們想要從過去資料來預測未來狀態，如天氣、股票走勢，則不該隨意將資料打亂並拆分成訓練集和測試集。 具備時間性的資料被打亂，會造成時間漏失(temporal leak)，而造成時序錯位。 在進行具時間性的預測時，應確保測試資料的發生時間是在訓練資料之後。 資料中的重複現象: 如果資料出現兩次，隨意打亂，可能會造成資料同時出現在訓練集與測試集中，那麼就相當於資料洩漏(information leak)，這樣訓練出來的模型將不可信。</p></div><footer class=entry-footer><span title='2025-01-12 17:54:25 +0800 +0800'>January 12, 2025</span>&nbsp;·&nbsp;1 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 評估模型" href=https://intervalrain.github.io/ai/5_2/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 普適化</h2></header><div class=entry-content><p>機器學習最重要的兩件事是：
準確的模型評估 訓練次數與普適化之間的平衡 普適化(generalization) 普適化是機器學習的終極目標，什麼是普適化呢？首先要先解釋是什麼低度擬合(underfitting) 與 過度擬合(overfitting)。 觀察下方的圖，訓練集與驗證集在訓練初期，損失值都穩定下降，此時稱為 underfitting，代表神經網路尚未學習到資料中的共同特徵。經過一定時間後，驗證指標會開始停滯並開始變差，這代表模型開始發生 overfitting，代表模型已經額外學習了一些只有訓練集中的特徵，進而可能在面對新資料時造成干擾。而 穩健擬合(robust fit) 是 underfitting 與 overfitting 之間的點，代表最佳的 epochs。 下圖的黑線與就是 robust fit 的表現，綠線是 overfitting 的表現。 可以看到綠線在訓練集有很好的表現，但可能會在新的資料點進入時，有錯誤的判斷。 普適化就是找到一個面對所有資料都能有穩定且好的表現的模型。 overfitting overfitting 容易發生在 具有雜訊的資料 具有罕見特徵 標示錯誤的資料 如果訓練過程，模型針對這些離群值(outlier)進行學習，普適化表現自然會下降。 模糊特徵 然後並非所有雜訊都是由不準確性(特徵模糊/標示錯誤)產生的，當處理的問題本身就具備不確定性或模棱兩可時，就算是字跡清晰、標籤正確也可能是雜訊，特別是一些沒有明確界線的特徵。 就像是下面的三杯一樣的水，由不同的人來 label，也會 label 出不一樣的答案。 最有感的就是問卷量表，客戶滿意度 (CSAT) 調查問卷通常分為 1 (非常不滿意) 到 5 分 (非常滿意) 。每個人對於滿意度的給分都不一致，所以就容易產生差異。 穩健的模型會忽略訓練資料中個別的資料點，從眾數著眼。 罕見特徵(rare feature)與虛假關聯(spurious correlation) 罕見特徵: 通常是樣本中出現頻率極低的特徵，可能具有高辨識度或影響力，但也可能是噪音，需要小心解讀。
事故發生的地點是某條高速公路上極少使用的臨時匝道。 事故發生時段是凌晨2點到3點，且伴隨濃霧天氣。 涉事車輛是一輛載有易燃化學品的大型貨車，並與一輛滿載乘客的大巴相撞。 如何影響機器學習： 特徵稀疏性： 這些罕見的特徵組合在訓練數據中可能只出現過1-2次，但其後果卻極為嚴重（多人傷亡），模型可能過度強調這些罕見情況作為高危因素。 過擬合風險： 如果模型只根據這些少量案例進行學習，可能無法有效處理未見過的場景（如不同的道路或車輛組合）。 虛假關聯: 則是數據之間表面上有相關性，但實際上缺乏因果關係，可能由於第三因素驅動或純粹的巧合。
冰淇淋銷量與溺水事件： 描述：夏季冰淇淋銷量與溺水事件呈現高度正相關。 原因：兩者都與溫度升高相關，並非冰淇淋銷量導致溺水事件增加。 接下來來做一個實驗，我們在每一張數字中擴充維度，分別擴充 noise 與 zeros ...</p></div><footer class=entry-footer><span title='2025-01-12 17:52:09 +0800 +0800'>January 12, 2025</span>&nbsp;·&nbsp;4 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 普適化" href=https://intervalrain.github.io/ai/5_1/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 迴歸問題</h2></header><div class=entry-content><p>認識波士頓住房價資料集 透過 tensorflow 引入資料集 波士頓住房價資料集是 1970 年代中期波士頓的郊區資料，包含犯罪率、當地財產稅等。 from tensorflow.keras.datasets import boston_housing (train_data, train_targets), (test_data, test_targets) = boston_housing.load_data() 與先前兩個範例最大的差別是，資料點明顯較少，共 404 + 102 = 506 筆。 print(train_data.shape) print(train_targets.shape) print(test_data.shape) print(test_targets.shape) > (404, 13) (404,) (102, 13) (102,) 需注意，每個特徵都有不同的單位刻度 查看特徵分布 特徵值範圍分析:
特徵 最小值 最大值 平均值 標準差 特徵意義 CRIM 0.006 88.976 3.614 8.602 城鎮人均犯罪率 ZN 0.000 100.000 11.364 23.322 佔地面積超過25000平方呎的住宅用地比例 INDUS 0.460 27.740 11.137 6.860 每個城鎮非零售商業用地的比例 CHAS 0.000 1.000 0.069 0.254 Charles River 虛擬變數 (1 if tract bounds river; 0 otherwise) NOX 0.385 0.871 0.555 0.116 一氧化氮濃度 RM 3.561 8.780 6.285 0.703 每棟住宅的平均房間數 AGE 2.900 100.000 68.575 28.149 1940年之前建成的自用房屋比例 DIS 1.130 12.126 3.795 2.106 到波士頓5個就業中心的加權距離 RAD 1.000 24.000 9.549 8.707 到高速公路的可達性指數 TAX 187.000 711.000 408.237 168.537 每10000美元的房產稅率 PTRATIO 12.600 22.000 18.456 2.165 城鎮師生比例 B 0.320 396.900 356.674 91.295 1000(Bk - 0.63)^2，其中Bk為城鎮中黑人的比例 LSTAT 1.730 37.970 12.653 7.141 人口中地位較低人群的百分比 程式碼：
...</p></div><footer class=entry-footer><span title='2025-01-12 16:39:58 +0800 +0800'>January 12, 2025</span>&nbsp;·&nbsp;3 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 迴歸問題" href=https://intervalrain.github.io/ai/4_3/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 多元分類問題</h2></header><div class=entry-content><p>認識路透社(Reuters)資料集 透過 tensorflow 引入資料集
參數 path: 數據的緩存位置（相對於 ~/.keras/dataset）。 num_words: 整數或 None。單詞按其出現頻率（在訓練集中）進行排名，並且僅保留 num_words 個最常見的單詞。任何較不常見的單詞在序列數據中都將顯示為 oov_char值。如果為 None，則保留所有單詞。默認為 None。 skip_top: 跳過前 N 個最常出現的單詞（這些單詞可能沒有信息量）。這些單詞在數據集中將顯示為 oov_char 值。0 表示不跳過任何單詞。默認為 0。 maxlen: int 或 None。最大序列長度。任何較長的序列都將被截斷。None 表示不截斷。默認為 None。 test_split: 介於 0. 與 1. 之間的浮點數。用作測試資料集的比例。0.2 表示 20% 的資料集用作測試資料。預設值為 0.2。 seed: 整數。用於可重複資料洗牌的種子。 start_char: 整數。序列的開頭將標記為此字元。0 通常是填充字元。預設值為 1。 oov_char: 整數。超出詞彙表的字元。由於 num_words 或 skip_top 限制而被刪除的詞彙將替換為此字元。 index_from: 整數。使用此索引及更高的索引來索引實際詞彙。 回傳值 Numpy 陣列的 tuple: (x_train, y_train), (x_test, y_test)。 相同於 IMDB 資料集，資料集包含了許多相異單字，這數字對訓練而言非常龐大，且對分類任務沒什麼幫助，所以我們只保留 10000 個最常出現的單字 from tensorflow.keras.datasets import reuters (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000) Reuters 路透社資料集是 1986 年由路透社發佈的一組簡短新聞和對應主題的資料集，被廣泛用於文章分類的研究。
...</p></div><footer class=entry-footer><span title='2025-01-06 22:58:45 +0800 +0800'>January 6, 2025</span>&nbsp;·&nbsp;3 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 多元分類問題" href=https://intervalrain.github.io/ai/4_2/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 二元分類問題</h2></header><div class=entry-content><p>1. 認識 IMDB 資料集 透過 tensorflow 引入資料集 參數 path: 資料的快取位置（相對於 ~/.keras/dataset）。 num_words: 整數或 None。單詞按其出現頻率（在訓練集中）進行排序，並且僅保留 num_words 個最常出现的单词。任何較不常出现的单词在序列資料中都將顯示為 oov_char 值。如果為 None，則保留所有單詞。預設值為 None。 skip_top: 跳過出現頻率最高的前 N 個單詞（這些單詞可能沒有參考價值）。這些單詞在資料+ 集中將顯示為 oov_char 值。當為 0 時，則不跳過任何單詞。預設值為 0。 maxlen: 整數或 None。最大序列長度。任何較長的序列都將被截斷。None 表示不進行截斷。預設值為 None。 seed: 整數。用於可重現資料洗牌的種子。 start_char: 整數。序列的開頭將標記為此字元。0 通常是填充字元。預設值為 1。 oov_char: 整數。詞彙表外字元。由於 num_words 或 skip_top 限制而被刪除的詞彙將替換為此字元。 index_from: 整數。使用此索引和更高索引為實際詞彙編制索引。 回傳值 Numpy 陣列的 tuple: (x_train, y_train), (x_test, y_test)。 其中資料集包含了 88585 個相異單字，有很大的單字甚至只有出現一次，這數字對訓練而言非常龐大，且對分類任務沒什麼幫助，所以我們只保留 10000 個最常出現的單字 from tensorflow.keras.datasets import imdb (train_images, train_labels), (test_images, test_labels) = imdb.load_data(num_words=10000) IMDb (Internet Movie Database) 是一個與影視相關的網路資料庫，IMDB 資料集就是從 IMDb 網站收集而來的資料。資料包含了 50000 筆評論，其中包含了 50% 的正面評論與 50% 的負面評論。 print(train_data.shape) print(train_labels.shape) print(test_data.shape) print(test_labels.shape) > (25000,) > (25000,) > (25000,) > (25000,) 資料的組成是由一連串的數字所組成，如：
\( \begin{array}{|c|c|c|c|c|c|c|c|c|c|c|} \hline \text{(保留)} & \text{the} & \text{and} & \text{a} & \text{…} & \text{in} & \text{…} & \text{wonderful} & \text{…} & \text{morning} & \text{…} \\ \hline 0 & 1 & 2 & 3 & … & 8 & … & 386 & … & 1969 & …\\ \hline \end{array} \) 每個數字代表一個單字，編號愈前面代表愈常用。 print(train_data[0]) > [1, 14, 22, 16, 43, 530, 973, 1622, ..., 32] 我們可以用 imdb.get_word_index(path="imdb_word_index.json") 來得到這個字典，並試著還原原始評論。 注意按引 0~2 為留保字，故索引值需位移 3。 dict = imdb.get_word_index(path="imdb_word_index.json") index_to_word = {value: key for key, value in dict.items()} sentence_0 = ' '.join([index_to_word.get(idx, '?') for idx in train_data[0]]) print(sentence_0) > ? this film was just brilliant casting location scenery story direction labels 是由 1, 0 組成的陣列，代表正評(1)或負評(0) print(train_labels[:10]) > array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0]) 2. 準備資料 由於目前的測試資料的長度都不一樣長，我們需要預先做整型，方法有二： 填補資料中每個串列，使長度相同。 做 mutli-hot(k-hot) 轉換。
\( \begin{array}{|c|} \hline \text{column}\\ \hline \text{A}\\ \hline \text{B}\\ \hline \text{C}\\ \hline \text{A}\\ \hline \text{A}\\\hline \end{array} \rightarrow \begin{array}{|c|c|c|} \hline \text{A} & \text{B} & \text{C}\\ \hline 1 & 0 & 0\\ \hline 0 & 1 & 0\\ \hline 0 & 0 & 1\\ \hline 1 & 0 & 0\\ \hline 1 & 0 & 0\\\hline \end{array} \) 將測試資料轉成 multi-hot 型式： import numpy as np def vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results x_train = vectorize_sequences(train_data) x_test = vectorize_sequences(test_data) 將 labels 也轉成向量資料 y_train = np.asarray(train_labels).astype('float32') y_test = np.asarray(test_labels).astype('float32') 3. 建立神經網路 我們的輸入資料是向量、標籤為 0 與 1，我們可以使用一個密集層堆疊架構搭配 relu 函數。
...</p></div><footer class=entry-footer><span title='2025-01-02 12:50:29 +0800 +0800'>January 2, 2025</span>&nbsp;·&nbsp;3 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 二元分類問題" href=https://intervalrain.github.io/ai/4_1/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 3-7. Keras API</h2></header><div class=entry-content><p>1. Layer Layer 是神經網路的基本資料處理模組(data-processing module)，可以接受一個或多個張量輸入，再輸出一個或多個張量。Layer 的權重可視為該層的狀態(state)，在經過隨機梯度下降法(SGD)不斷更新權重(學習)，最終得到損失值最低的權重值。 分類 不同格式的資料需要不同的層來處理。 密集連接層(densely connected layer): 又稱全連接層(fully connected layer)或密集層(dense layer)。如果資料輸入是簡單的 1D 向量資料，則多半是儲存在 2D 張量中，其 shape 為 (樣本 samples, 特徵 features)，通常是用 循環層(recurrent layer): 如果輸入的資料是 2D 序列(sequence) 資料，多半是儲存在 3D 張量中，其 shape 為 (樣本 samples, 時戳 timestamps, 特徵 features)，如 LSTM 層。 2D 卷積層: 如果是 3D 影像資料，多半是儲存在 4D 張量中，通常使用 2D 卷積層(Conv2D Layer) Keras 中的基礎 Layer 類別 Layer 類別是 Keras 的核心，每個 Keras 元件都是一個 Layer 物件並與 Layer 有密切互動。Layer 是將一些狀態(權重)和運算(正向傳播)包在一起的物件。 雖然權重可以在建構子 __init__() 中建立，但我們通常會使用 build() 來建立，然後用 call() 來執行正向傳播。 from tensorflow import tf class SimpleDemo(keras.layers.Layer): def __init__(self, units, activation=None): super().__init__() self.units = units self.activation = activation def build(self, input_shape): input_dim = input_shape[-1] self.W = self.add_weight(shape=(input_dim, self.units),initializer="random_normal") self.b = self.add_weight(shape=(self.units,),initializer="zeros") def call(self, inputs): y = tf.matmul(inputs, self.W) + self.b if self.activation is not None: y = self.activation(y) return y 我們可以像使用函式一樣來實例化 Layer 物件，其輸入為一個張量 sample_layer = SimpleDemo(units=32, activation=tf.nn.relu) 建立 build() 方法的意義在於，我們希望在第一次呼叫 layer 物件時才即時創建權重張量。 keras 會幫我們做好自動推論權重的 shape，我們可以把關注給放在如何定義 build()。 model = keras.Sequential([ SimpleDense(32, activation="relu"), SimpleDense(64, activation="relu"), SimpleDense(32, activation="relu"), SimpleDense(10, activation="softmax") ]) 事實上 __call__() 做的是遠不只推論 shape，還有 eager 執行模式 和 graph 執行模式 之的路徑選擇等等。 2. Model 深度學習模型是由多個層所組成的結構，在 Keras 是以 Model 類別來建立模型物件。
...</p></div><footer class=entry-footer><span title='2024-12-20 15:49:27 +0800 +0800'>December 20, 2024</span>&nbsp;·&nbsp;2 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 3-7. Keras API" href=https://intervalrain.github.io/ai/3_7/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[AI] 3-6. 實作線性分類器</h2></header><div class=entry-content><p>命題 我們先嘗試製作一個 sample data，考慮某一個台獨國內的房價與坪數的分布，標記出「城市」與「鄉下」兩個標籤：
紫色代表城市的房子、黃色代表鄉下的房子 城市的房子房價較高且坪數較小、鄉下的房子房價較低且坪數較大。 隱藏的特徵為單位坪數的價格，城市會高於鄉下。 以下是產生的 data，seed 設為 42 import numpy as np def load_data(n1=1000,n2=300,seed=42): np.random.seed(seed) n=n1+n2 # 城市房屋 city = np.random.multivariate_normal( mean=[25.9, 1503.7], # [坪數, 總價(萬)] cov=[[33.64, 400], # 坪數標準差 5.8 [400, 64112.24]], # 總價標準差 253.2 size=n ) # 鄉村房屋 rural = np.random.multivariate_normal( mean=[31.8, 834.3], # [坪數, 總價(萬)] cov=[[62.41, 200], # 坪數標準差 7.9 [200, 9623.61]], # 總價標準差 98.1 size=n ) # 合併資料 data = np.vstack((city, rural)).astype(np.float32) labels = np.vstack((np.zeros((n,1), dtype="float32"), np.ones((n,1), dtype="float32"))) # 打散順序 shuffle_idx = np.random.permutation(len(data)) data = data[shuffle_idx] labels = labels[shuffle_idx] # 分割訓練集和測試集 train_data = data[:2*n1] test_data = data[2*n1:] train_labels = labels[:2*n1] test_labels = labels[2*n1:] return (train_data, train_labels), (test_data, test_labels) (train_data, train_labels), (test_data, test_labels) = load_data() 通用函式 因為 price 與 size 之間的值相差較大，所以我定義了正規化的函式、繪圖的函式、training 函式。 import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler import tensorflow as tf from tensorflow.keras import layers, models, callbacks import pandas as pd # 共用參數 LEARNING_RATE = 0.01 BATCH_SIZE = 32 EPOCHS = 100 EARLY_STOPPING_PATIENCE = 10 def normalize_data(train_data, test_data): """標準化資料""" scaler = StandardScaler() train_normalized = scaler.fit_transform(train_data) test_normalized = scaler.transform(test_data) return train_normalized, test_normalized, scaler def plot_decision_boundary(model, X, y, title, scaler=None, is_extended=False): """繪製決策邊界 Parameters: ----------- model : 訓練好的模型 X : array-like, shape (n_samples, 2) or (n_samples, 3) 輸入特徵 y : array-like 目標變數 title : str 圖表標題 scaler : StandardScaler, optional 用於還原正規化的scaler is_extended : bool 是否為擴展特徵模型 """ if is_extended: # 對於擴展特徵模型，只使用前兩個特徵繪圖 X_plot = X[:, :2] else: X_plot = X # 設定決策邊界的範圍 x_min, x_max = X_plot[:, 0].min() - 0.5, X_plot[:, 0].max() + 0.5 y_min, y_max = X_plot[:, 1].min() - 0.5, X_plot[:, 1].max() + 0.5 # 創建網格點 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02)) # 預測網格點的類別 grid_points = np.c_[xx.ravel(), yy.ravel()] if is_extended: # 為擴展特徵模型添加 price/size ratio ratio = grid_points[:, 1:2] / grid_points[:, 0:1] grid_points = np.hstack((grid_points, ratio)) Z = model.predict(grid_points) Z = Z.reshape(xx.shape) # 如果提供了scaler，將數據轉換回原始尺度 if scaler is not None: # 轉換網格點 grid_points_original = scaler.inverse_transform(grid_points[:, :2]) xx_original = grid_points_original[:, 0].reshape(xx.shape) yy_original = grid_points_original[:, 1].reshape(yy.shape) # 轉換特徵點 X_original = scaler.inverse_transform(X_plot) x_plot = X_original[:, 0] y_plot = X_original[:, 1] xlabel = 'Size' ylabel = 'Price' else: x_plot = X_plot[:, 0] y_plot = X_plot[:, 1] xx_original = xx yy_original = yy xlabel = 'Normalized Size' ylabel = 'Normalized Price' # 繪製圖形 plt.figure(figsize=(10, 8)) # 繪製預測機率的漸層 plt.contourf(xx_original, yy_original, Z, alpha=0.4, levels=np.linspace(0, 1, 11)) # 添加決策邊界（機率=0.5的等高線） plt.contour(xx_original, yy_original, Z, levels=[0.5], colors='red', linestyles='-', linewidths=2) # 繪製資料點 plt.scatter(x_plot, y_plot, c=y.ravel(), alpha=0.8) plt.title(title) plt.xlabel(xlabel) plt.ylabel(ylabel) # 添加顏色條 plt.colorbar(label='Prediction Probability') plt.show() def train_and_evaluate(model, train_data, train_labels, test_data, test_labels, epochs=EPOCHS, batch_size=BATCH_SIZE, use_early_stopping=False): """訓練模型並評估""" callbacks_list = [] if use_early_stopping: early_stopping = callbacks.EarlyStopping( monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True ) callbacks_list.append(early_stopping) history = model.fit( train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=callbacks_list, verbose=0 ) test_loss, test_accuracy = model.evaluate(test_data, test_labels, verbose=0) return history, test_loss, test_accuracy 定義模型 本篇設定了不同的 model, optimizer, loss，不代表哪一種 pattern 的優劣，純粹是示範 library 使用。 由於我想看不同的 optimizer/activation/loss function的差異，所以我共用了部分參數 LEARNING_RATE = 0.01 BATCH_SIZE = 32 EPOCHS = 100 EARLY_STOPPING_PATIENCE = 10 我列了幾個不同的策略，來看看結果的差異 SGD + 線性模型 (wx+b) + MSE SGD + Sigmoid 模型 + MSE SGD + ReLU 模型 + MSE SGD + Sigmoid 模型 + BCE Adam + Sigmoid 模型 + BCE Adam + Sigmoid 模型 + BCE (加入新特徵 price/size) Adam + 雜複模型 + BCD Adam + 複雜模型 + BCD (加入新特徵 price/size) 1. SGD + 線性模型 (wx+b) + MSE # 1. 線性模型 (wx+b) def create_linear_model(): model = models.Sequential([ layers.Dense(1, input_shape=(2,), activation='linear') ]) model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE), loss='mse', metrics=['accuracy']) return model ...</p></div><footer class=entry-footer><span title='2024-12-19 16:55:12 +0800 +0800'>December 19, 2024</span>&nbsp;·&nbsp;5 分鐘&nbsp;·&nbsp;Rain Hu</footer><a class=entry-link aria-label="post link to [AI] 3-6. 實作線性分類器" href=https://intervalrain.github.io/ai/3_6/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://intervalrain.github.io/tags/ai/page/2/>下一頁&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://intervalrain.github.io/>Rain Hu's Workspace</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>